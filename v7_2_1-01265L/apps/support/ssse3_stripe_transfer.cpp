/*****************************************************************************/
// File: ssse3_stripe_transfer.cpp [scope = APPS/SUPPORT]
// Version: Kakadu, V7.2.1
// Author: David Taubman
// Last Revised: 28 March, 2013
/*****************************************************************************/
// Copyright 2001, David Taubman, The University of New South Wales (UNSW)
// The copyright owner is Unisearch Ltd, Australia (commercial arm of UNSW)
// Neither this copyright statement, nor the licensing details below
// may be removed from this file or dissociated from its contents.
/*****************************************************************************/
// Licensee: International Centre For Radio Astronomy Research, Uni of WA
// License number: 01265
// The licensee has been granted a UNIVERSITY LIBRARY license to the
// contents of this source file.  A brief summary of this license appears
// below.  This summary is not to be relied upon in preference to the full
// text of the license agreement, accepted at purchase of the license.
// 1. The License is for University libraries which already own a copy of
//    the book, "JPEG2000: Image compression fundamentals, standards and
//    practice," (Taubman and Marcellin) published by Kluwer Academic
//    Publishers.
// 2. The Licensee has the right to distribute copies of the Kakadu software
//    to currently enrolled students and employed staff members of the
//    University, subject to their agreement not to further distribute the
//    software or make it available to unlicensed parties.
// 3. Subject to Clause 2, the enrolled students and employed staff members
//    of the University have the right to install and use the Kakadu software
//    and to develop Applications for their own use, in their capacity as
//    students or staff members of the University.  This right continues
//    only for the duration of enrollment or employment of the students or
//    staff members, as appropriate.
// 4. The enrolled students and employed staff members of the University have the
//    right to Deploy Applications built using the Kakadu software, provided
//    that such Deployment does not result in any direct or indirect financial
//    return to the students and staff members, the Licensee or any other
//    Third Party which further supplies or otherwise uses such Applications.
// 5. The Licensee, its students and staff members have the right to distribute
//    Reusable Code (including source code and dynamically or statically linked
//    libraries) to a Third Party, provided the Third Party possesses a license
//    to use the Kakadu software, and provided such distribution does not
//    result in any direct or indirect financial return to the Licensee,
//    students or staff members.  This right continues only for the
//    duration of enrollment or employment of the students or staff members,
//    as appropriate.
/******************************************************************************
Description:
   Provides SIMD implementations to accelerate the conversion and transfer of
data between the line buffers generated by `kdu_multi_synthesis' or
`kdu_multi_analysis' and the (possibly interleaved) application-supplied
sample buffers supplied to `kdu_stripe_decompressor::pull_stripe' or
`kdu_stripe_compressor::push_stripe'.  This file provides conversion
functions that require at most SSSE3 support.  Other files might provide
implementations that require AVX, AVX2 or perhaps other processor
instruction support.
******************************************************************************/
#include "kdu_arch.h"

#if (defined KDU_PENTIUM_MSVC)
#  undef KDU_PENTIUM_MSVC
#  ifndef KDU_X86_INTRINSICS
#    define KDU_X86_INTRINSICS // Use portable intrinsics instead
#  endif
#endif // KDU_PENTIUM_MSVC

#if ((!defined KDU_NO_SSSE3) && (defined KDU_X86_INTRINSICS))

#include <tmmintrin.h>
#include <assert.h>

// Shuffle control for converting interleaved colour components to
// contiguous groups of components, within a 16 byte group.  The suffix
// "_556" means that the interleaved sequence (starting from the MSB position)
// [R B G R B G ... B G R] is converted to 5 blues in the MSBs followed
// by 5 greens and then 6 reds in the LSBs.  Similarly, the suffix "_565"
// means that we are converting the interleaved sequence [G R B G R ... R B G],
// while the "_655" suffix converts [B G R B G ... G R B] to 6 blues in the
// MSB positions, followed by 5 greens and then 5 reds in the LSB positions.
static kdu_byte kd_shuffle_r556[16] =
  { 0, 3, 6, 9, 12, 15, 1, 4, 7, 10, 13, 2, 5, 8, 11, 14 };
static kdu_byte kd_shuffle_r565[16] =
  { 2, 5, 8, 11, 14, 0, 3, 6, 9, 12, 15, 1, 4, 7, 10, 13 };
static kdu_byte kd_shuffle_r655[16] =
  { 1, 4, 7, 10, 13, 2, 5, 8, 11, 14, 0, 3, 6, 9, 12, 15 };

// Shuffle control for converting contiguous groups of colour components
// to fully interleaved components within a 16 byte group.  The suffix
// "_556" means that the contiguous group starts out with 5 blues in the
// MSBs, then 5 greens and then 6 reds in the LSBs -- the interleaved
// pattern, starting from the MSB position, will be [R B G R B G ... B G R].
// Similarly, the suffix "_565" produces [G R B G R ... R B G], while the
// "_655" suffix yields [B G R B G ... G R B].
static kdu_byte kd_shuffle_556[16] =
  { 0, 6, 11, 1, 7, 12, 2, 8, 13, 3, 9, 14, 4, 10, 15, 5 };
static kdu_byte kd_shuffle_565[16] =
  { 5, 11, 0, 6, 12, 1, 7, 13, 2, 8, 14, 3, 9, 15, 4, 10 };
static kdu_byte kd_shuffle_655[16] =
  { 10, 0, 5, 11, 1, 6, 12, 2, 7, 13, 3, 8, 14, 4, 9, 15 };

// Shuffle control for converting 4-way interleaved data to 4 groups of
// samples from the same channel, within a 16 byte group.
static kdu_byte kd_shuffle_r4444[16] =
  { 0, 4, 8, 12, 1, 5, 9, 13, 2, 6, 10, 14, 3, 7, 11, 15 };
    
#include <stdio.h>

/* ========================================================================= */
/*               SIMD functions used by `kdu_stripe_compressor'              */
/* ========================================================================= */

/*****************************************************************************/
/* EXTERN               ssse3_int16_from_uint8_ilv1                           */
/*****************************************************************************/

void
  ssse3_int16_from_uint8_ilv1(kdu_int16 **dst, kdu_byte *src, int width,
                              int src_precision, int tgt_precision,
                              bool is_absolute, bool src_signed)
{
  assert(!src_signed); // Function prototype generic; we don't do signed bytes
  if (!is_absolute)
    tgt_precision = KDU_FIX_POINT;
  int offset = (src_signed)?0:(1<<(src_precision-1));
  int pre_upshift = 16 - src_precision;
  int post_downshift = 16 - tgt_precision;
  
  __m128i vec_off =   _mm_set1_epi8((kdu_byte) offset);
  __m128i vec_left =  _mm_cvtsi32_si128(pre_upshift);
  __m128i vec_right = _mm_cvtsi32_si128(post_downshift);
  __m128i *dp = (__m128i *)(dst[0]);
  __m128i v1, v2, v3, v4;
  
  kdu_byte *sp = src;
  for (; width >= 32; width-=32, sp+=32, dp+=4)
    { 
      // Load 2 input dqwords
      v2 = _mm_loadu_si128((__m128i *) sp);
      v4 = _mm_loadu_si128((__m128i *)(sp+16));
      
      // Convert to four output dqwords
      v2 = _mm_sub_epi8(v2,vec_off);     v4 = _mm_sub_epi8(v4,vec_off);
      v1 = _mm_unpacklo_epi8(v2,v2);     v2 = _mm_unpackhi_epi8(v2,v2);
      v3 = _mm_unpacklo_epi8(v4,v4);     v4 = _mm_unpackhi_epi8(v4,v4);
      v1 = _mm_sll_epi16(v1,vec_left);   v2 = _mm_sll_epi16(v2,vec_left); 
      v3 = _mm_sll_epi16(v3,vec_left);   v4 = _mm_sll_epi16(v4,vec_left); 
      v1 = _mm_sra_epi16(v1,vec_right);  v2 = _mm_sra_epi16(v2,vec_right);
      v3 = _mm_sra_epi16(v3,vec_right);  v4 = _mm_sra_epi16(v4,vec_right);
      dp[0] = v1;  dp[1] = v2;           dp[2] = v3;  dp[3] = v4;
    }
 if (width > 0)
   { // Handle the last 1-31 input bytes and associated 1-4 output dqwords
     // carefully so as to avoid buffer overrun.
     union { __m128i dqwords[2]; kdu_byte bytes[32]; };
     for (int c=0; c < width; c++) bytes[c] = sp[c];
     v2 = dqwords[0]; v4 = dqwords[1];  
     v2 = _mm_sub_epi8(v2,vec_off);     v4 = _mm_sub_epi8(v4,vec_off);
     v1 = _mm_unpacklo_epi8(v2,v2);     v2 = _mm_unpackhi_epi8(v2,v2);
     v3 = _mm_unpacklo_epi8(v4,v4);     v4 = _mm_unpackhi_epi8(v4,v4);
     v1 = _mm_sll_epi16(v1,vec_left);   v2 = _mm_sll_epi16(v2,vec_left); 
     v3 = _mm_sll_epi16(v3,vec_left);   v4 = _mm_sll_epi16(v4,vec_left); 
     v1 = _mm_sra_epi16(v1,vec_right);  v2 = _mm_sra_epi16(v2,vec_right);
     v3 = _mm_sra_epi16(v3,vec_right);  v4 = _mm_sra_epi16(v4,vec_right);
     dp[0] = v1;
     if (width < 8) return;
     dp[1] = v2;
     if (width < 16) return;
     dp[2] = v3;
     if (width < 24) return;
     dp[3] = v4;
   }
}

/*****************************************************************************/
/* EXTERN              ssse3_int16_from_uint8_ilv3                           */
/*****************************************************************************/

void
  ssse3_int16_from_uint8_ilv3(kdu_int16 **dst, kdu_byte *src, int width,
                              int src_precision, int tgt_precision,
                              bool is_absolute, bool src_signed)
{
  assert(!src_signed); // Function prototype generic; we don't do signed bytes
  if (!is_absolute)
    tgt_precision = KDU_FIX_POINT;
  int offset = (src_signed)?0:(1<<(src_precision-1));
  int pre_upshift = 16 - src_precision;
  int post_downshift = 16 - tgt_precision;
  
  __m128i vec_shuffle_556 = _mm_loadu_si128((__m128i *) kd_shuffle_r556);
  __m128i vec_shuffle_565 = _mm_loadu_si128((__m128i *) kd_shuffle_r565);
  __m128i vec_shuffle_655 = _mm_loadu_si128((__m128i *) kd_shuffle_r655);
  __m128i vec_off =   _mm_set1_epi8((kdu_byte) offset);
  __m128i vec_left =  _mm_cvtsi32_si128(pre_upshift);
  __m128i vec_right = _mm_cvtsi32_si128(post_downshift);
  __m128i *dp1 = (__m128i *)(dst[0]);
  __m128i *dp2 = (__m128i *)(dst[1]);
  __m128i *dp3 = (__m128i *)(dst[2]);
  __m128i v1, v2, v3, vt, vb;

  kdu_byte *sp = src;
  for (; width >= 16; width-=16, sp+=48, dp1+=2, dp2+=2, dp3+=2)
    { 
      // Load and de-interleave 3 dqwords of bytes
      v1 = _mm_loadu_si128((__m128i *) sp);
      v2 = _mm_loadu_si128((__m128i *)(sp+16));
      v3 = _mm_loadu_si128((__m128i *)(sp+32));
      v1 = _mm_shuffle_epi8(v1,vec_shuffle_556); // MSBs-LSBs = [5B | 5G | 6R]
      v2 = _mm_shuffle_epi8(v2,vec_shuffle_565); // MSBs-LSBs = [5B | 6G | 5R]
      v3 = _mm_shuffle_epi8(v3,vec_shuffle_655); // MSBs-LSBs = [6B | 5G | 5R]
      
      // Move 16R to vt, 16G to vb and 16B to v3
      vt = _mm_slli_si128(v1,10);    // Move 6 reds to to MSBs
      vt = _mm_alignr_epi8(v2,vt,5);     v2 = _mm_srli_si128(v2,5);
      vt = _mm_alignr_epi8(v3,vt,5);     v3 = _mm_srli_si128(v3,5);
      vt = _mm_sub_epi8(vt,vec_off); // Level adjused reds in vt
      vb = _mm_slli_si128(v1,5);     // Move 5 greens to MSBs
      vb = _mm_alignr_epi8(v2,vb,6);     v2 = _mm_srli_si128(v2,6);
      vb = _mm_alignr_epi8(v3,vb,5);     v3 = _mm_srli_si128(v3,5);
      vb = _mm_sub_epi8(vb,vec_off); // Level adjusted greens in vb      
      v2 = _mm_alignr_epi8(v2,v1,5);     v3 = _mm_alignr_epi8(v3,v2,6);      
      v3 = _mm_sub_epi8(v3,vec_off); // Level adjusted blues in v3      
      
      // Expand, shift and write the channel data
      v1 = _mm_unpacklo_epi8(vt,vt);     vt = _mm_unpackhi_epi8(vt,vt);
      v1 = _mm_sll_epi16(v1,vec_left);   vt = _mm_sll_epi16(vt,vec_left); 
      v1 = _mm_sra_epi16(v1,vec_right);  vt = _mm_sra_epi16(vt,vec_right);
      dp1[0] = v1;  dp1[1] = vt;
      v2 = _mm_unpacklo_epi8(vb,vb);     vb = _mm_unpackhi_epi8(vb,vb);
      v2 = _mm_sll_epi16(v2,vec_left);   vb = _mm_sll_epi16(vb,vec_left); 
      v2 = _mm_sra_epi16(v2,vec_right);  vb = _mm_sra_epi16(vb,vec_right);
      dp2[0] = v2;  dp2[1] = vb;
      v1 = _mm_unpacklo_epi8(v3,v3);     v3 = _mm_unpackhi_epi8(v3,v3);
      v1 = _mm_sll_epi16(v1,vec_left);   v3 = _mm_sll_epi16(v3,vec_left); 
      v1 = _mm_sra_epi16(v1,vec_right);  v3 = _mm_sra_epi16(v3,vec_right);
      dp3[0] = v1;  dp3[1] = v3;
    }
  if (width > 0)
    { // Handle the last 3-45 input bytes and associated output dqwords
      // carefully so as to avoid buffer overrun.
      union { __m128i dqwords[3]; kdu_byte bytes[48]; };
      width += 2*width;
      for (int c=0; c < width; c++) bytes[c] = sp[c];
      v1 = dqwords[0];  v2 = dqwords[1];  v3 = dqwords[2];
      v1 = _mm_shuffle_epi8(v1,vec_shuffle_556); // MSBs-LSBs = [5B | 5G | 6R]
      v2 = _mm_shuffle_epi8(v2,vec_shuffle_565); // MSBs-LSBs = [5B | 6G | 5R]
      v3 = _mm_shuffle_epi8(v3,vec_shuffle_655); // MSBs-LSBs = [6B | 5G | 5R]

      vt = _mm_slli_si128(v1,10);    // Move 6 reds to to MSBs
      vt = _mm_alignr_epi8(v2,vt,5);     v2 = _mm_srli_si128(v2,5);
      vt = _mm_alignr_epi8(v3,vt,5);     v3 = _mm_srli_si128(v3,5);
      vt = _mm_sub_epi8(vt,vec_off); // Level adjused reds in vt
      vb = _mm_slli_si128(v1,5);     // Move 5 greens to MSBs
      vb = _mm_alignr_epi8(v2,vb,6);     v2 = _mm_srli_si128(v2,6);
      vb = _mm_alignr_epi8(v3,vb,5);     v3 = _mm_srli_si128(v3,5);
      vb = _mm_sub_epi8(vb,vec_off); // Level adjusted greens in vb      
      v2 = _mm_alignr_epi8(v2,v1,5);     v3 = _mm_alignr_epi8(v3,v2,6);      
      v3 = _mm_sub_epi8(v3,vec_off); // Level adjusted blues in vr      
      
      // Expand, shift and write the channel data
      v1 = _mm_unpacklo_epi8(vt,vt);     vt = _mm_unpackhi_epi8(vt,vt);
      v1 = _mm_sll_epi16(v1,vec_left);   vt = _mm_sll_epi16(vt,vec_left); 
      v1 = _mm_sra_epi16(v1,vec_right);  vt = _mm_sra_epi16(vt,vec_right);
      dp1[0] = v1;
      v2 = _mm_unpacklo_epi8(vb,vb);     vb = _mm_unpackhi_epi8(vb,vb);
      v2 = _mm_sll_epi16(v2,vec_left);   vb = _mm_sll_epi16(vb,vec_left); 
      v2 = _mm_sra_epi16(v2,vec_right);  vb = _mm_sra_epi16(vb,vec_right);
      dp2[0] = v2;
      v1 = _mm_unpacklo_epi8(v3,v3);     v3 = _mm_unpackhi_epi8(v3,v3);
      v1 = _mm_sll_epi16(v1,vec_left);   v3 = _mm_sll_epi16(v3,vec_left); 
      v1 = _mm_sra_epi16(v1,vec_right);  v3 = _mm_sra_epi16(v3,vec_right);
      dp3[0] = v1;
      if (width >= 8)
        { 
          dp1[1] = vt;
          dp2[1] = vb;
          dp3[1] = v3;
        }
    }
}

/*****************************************************************************/
/* EXTERN               ssse3_int16_from_uint8_ilv4                          */
/*****************************************************************************/

void
  ssse3_int16_from_uint8_ilv4(kdu_int16 **dst, kdu_byte *src, int width,
                              int src_precision, int tgt_precision,
                              bool is_absolute, bool src_signed)
{
  assert(!src_signed); // Function prototype generic; we don't do signed bytes
  if (!is_absolute)
    tgt_precision = KDU_FIX_POINT;
  int offset = (src_signed)?0:(1<<(src_precision-1));
  int pre_upshift = 16 - src_precision;
  int post_downshift = 16 - tgt_precision;
  
  __m128i vec_shuffle = _mm_loadu_si128((__m128i *) kd_shuffle_r4444);
  __m128i vec_off =   _mm_set1_epi8((kdu_byte) offset);
  __m128i vec_left =  _mm_cvtsi32_si128(pre_upshift);
  __m128i vec_right = _mm_cvtsi32_si128(post_downshift);
  __m128i *dp1 = (__m128i *)(dst[0]);
  __m128i *dp2 = (__m128i *)(dst[1]);
  __m128i *dp3 = (__m128i *)(dst[2]);
  __m128i *dp4 = (__m128i *)(dst[3]);
  __m128i v1, v2, v3;
  
  kdu_byte *sp = src;
  for (; width >= 8; width-=8, sp+=32, dp1++, dp2++, dp3++, dp4++)
    { 
      v1 = _mm_loadu_si128((__m128i *) sp);
      v2 = _mm_loadu_si128((__m128i *)(sp+16));
      v1 = _mm_shuffle_epi8(v1,vec_shuffle); // [4A | 4B | 4G | 4R]
      v2 = _mm_shuffle_epi8(v2,vec_shuffle); // [4A | 4B | 4G | 4R]
      v1 = _mm_sub_epi8(v1,vec_off);    v2 = _mm_sub_epi8(v2,vec_off);
      v3 = _mm_unpacklo_epi32(v1,v2); // v3 = [8G | 8R]
      v1 = _mm_unpackhi_epi32(v1,v2); // v1 = [8A | 8B]
      v2 = _mm_unpacklo_epi8(v3,v3);     v3 = _mm_unpackhi_epi8(v3,v3);
      v2 = _mm_sll_epi16(v2,vec_left);   v3 = _mm_sll_epi16(v3,vec_left); 
      v2 = _mm_sra_epi16(v2,vec_right);  v3 = _mm_sra_epi16(v3,vec_right);
      dp1[0] = v2;  dp2[0] = v3;
      v2 = _mm_unpacklo_epi8(v1,v1);     v1 = _mm_unpackhi_epi8(v1,v1);
      v2 = _mm_sll_epi16(v2,vec_left);   v1 = _mm_sll_epi16(v1,vec_left); 
      v2 = _mm_sra_epi16(v2,vec_right);  v1 = _mm_sra_epi16(v1,vec_right);
      dp3[0] = v2;  dp4[0] = v1;
    }
  if (width > 0)
    { // Handle the last 4-28 input bytes and associated output dqwords
      // carefully so as to avoid buffer overrun.
      union { __m128i dqwords[2]; kdu_byte bytes[32]; };
      width <<= 2;
      for (int c=0; c < width; c++) bytes[c] = sp[c];
      v1 = dqwords[0];  v2 = dqwords[1];
      v1 = _mm_shuffle_epi8(v1,vec_shuffle); // [4A | 4B | 4G | 4R]
      v2 = _mm_shuffle_epi8(v2,vec_shuffle); // [4A | 4B | 4G | 4R]
      v1 = _mm_sub_epi8(v1,vec_off);    v2 = _mm_sub_epi8(v2,vec_off);
      v3 = _mm_unpacklo_epi32(v1,v2); // v3 = [8G | 8R]
      v1 = _mm_unpackhi_epi32(v1,v2); // v1 = [8A | 8B]
      v2 = _mm_unpacklo_epi8(v3,v3);     v3 = _mm_unpackhi_epi8(v3,v3);
      v2 = _mm_sll_epi16(v2,vec_left);   v3 = _mm_sll_epi16(v3,vec_left); 
      v2 = _mm_sra_epi16(v2,vec_right);  v3 = _mm_sra_epi16(v3,vec_right);
      dp1[0] = v2;  dp2[0] = v3;
      v2 = _mm_unpacklo_epi8(v1,v1);     v1 = _mm_unpackhi_epi8(v1,v1);
      v2 = _mm_sll_epi16(v2,vec_left);   v1 = _mm_sll_epi16(v1,vec_left); 
      v2 = _mm_sra_epi16(v2,vec_right);  v1 = _mm_sra_epi16(v1,vec_right);
      dp3[0] = v2;  dp4[0] = v1;
    }
}

/*****************************************************************************/
/* EXTERN               ssse3_floats_from_uint8_ilv1                         */
/*****************************************************************************/

void
  ssse3_floats_from_uint8_ilv1(float **dst, kdu_byte *src, int width,
                               int src_precision, int tgt_precision,
                               bool is_absolute, bool src_signed)
{
  assert(!src_signed); // Function prototype generic; we don't do signed bytes
  int offset = (src_signed)?0:(1<<(src_precision-1));
  int upshift = 32 - src_precision;
  float scale = 1.0F  / (((float)(1<<16)) * ((float)(1<<16)));
  
  __m128i vec_off   = _mm_set1_epi8((kdu_byte) offset);
  __m128i vec_shift = _mm_cvtsi32_si128(upshift);
  __m128  vec_scale = _mm_set1_ps(scale);
  __m128 *dp = (__m128 *)(dst[0]);
  __m128i v1, v2, v3, v4;
  __m128 v1f, v2f, v3f, v4f;
  
  kdu_byte *sp = src;
  for (; width >= 16; width-=16, sp+=16, dp+=4)
    { 
      v4 = _mm_loadu_si128((__m128i *) sp);
      v4 = _mm_sub_epi8(v4,vec_off);
      v2 = _mm_unpacklo_epi8(v4,v4);     v4 = _mm_unpackhi_epi8(v4,v4);
      v1 = _mm_unpacklo_epi16(v2,v2);    v2 = _mm_unpackhi_epi16(v2,v2);
      v3 = _mm_unpacklo_epi16(v4,v4);    v4 = _mm_unpackhi_epi16(v4,v4);
      v1 = _mm_sll_epi32(v1,vec_shift);  v2 = _mm_sll_epi32(v2,vec_shift);
      v3 = _mm_sll_epi32(v3,vec_shift);  v4 = _mm_sll_epi32(v4,vec_shift);
      v1f = _mm_cvtepi32_ps(v1);         v2f = _mm_cvtepi32_ps(v2);
      v3f = _mm_cvtepi32_ps(v3);         v4f = _mm_cvtepi32_ps(v4);
      v1f = _mm_mul_ps(v1f,vec_scale);   v2f = _mm_mul_ps(v2f,vec_scale);
      v3f = _mm_mul_ps(v3f,vec_scale);   v4f = _mm_mul_ps(v4f,vec_scale);
      dp[0] = v1f;  dp[1] = v2f;  dp[2] = v3f;  dp[3] = v4f;
    }
  if (width > 0)
    { // Handle the last 1-15 input bytes and associated output dqwords
      // carefully so as to avoid buffer overrun.
      union { __m128i dqword; kdu_byte bytes[16]; };
      for (int c=0; c < width; c++) bytes[c] = sp[c];
      v4 = dqword;
      v4 = _mm_sub_epi8(v4,vec_off);
      v2 = _mm_unpacklo_epi8(v4,v4);     v4 = _mm_unpackhi_epi8(v4,v4);
      v1 = _mm_unpacklo_epi16(v2,v2);    v2 = _mm_unpackhi_epi16(v2,v2);
      v3 = _mm_unpacklo_epi16(v4,v4);    v4 = _mm_unpackhi_epi16(v4,v4);
      v1 = _mm_sll_epi32(v1,vec_shift);  v2 = _mm_sll_epi32(v2,vec_shift);
      v3 = _mm_sll_epi32(v3,vec_shift);  v4 = _mm_sll_epi32(v4,vec_shift);
      v1f = _mm_cvtepi32_ps(v1);         v2f = _mm_cvtepi32_ps(v2);
      v3f = _mm_cvtepi32_ps(v3);         v4f = _mm_cvtepi32_ps(v4);
      v1f = _mm_mul_ps(v1f,vec_scale);   v2f = _mm_mul_ps(v2f,vec_scale);
      v3f = _mm_mul_ps(v3f,vec_scale);   v4f = _mm_mul_ps(v4f,vec_scale);
      dp[0] = v1f;
      if (width <= 4) return;
      dp[1] = v2f;
      if (width <= 8) return;
      dp[2] = v3f;
      if (width <= 12) return;
      dp[3] = v4f;
    }
}

/*****************************************************************************/
/* EXTERN               ssse3_floats_from_uint8_ilv3                         */
/*****************************************************************************/

void
  ssse3_floats_from_uint8_ilv3(float **dst, kdu_byte *src, int width,
                               int src_precision, int tgt_precision,
                               bool is_absolute, bool src_signed)
{
  assert(!src_signed); // Function prototype generic; we don't do signed bytes
  int offset = (src_signed)?0:(1<<(src_precision-1));
  int upshift = 16 - src_precision;
  float scale = 1.0F  / (((float)(1<<16)) * ((float)(1<<16)));
  
  __m128i vec_shuffle_556 = _mm_loadu_si128((__m128i *) kd_shuffle_r556);
  __m128i vec_shuffle_565 = _mm_loadu_si128((__m128i *) kd_shuffle_r565);
  __m128i vec_shuffle_655 = _mm_loadu_si128((__m128i *) kd_shuffle_r655);
  __m128i vec_off   = _mm_set1_epi8((kdu_byte) offset);
  __m128i vec_left = _mm_cvtsi32_si128(upshift);
  __m128  vec_scale = _mm_set1_ps(scale);
  __m128 *dp1 = (__m128 *)(dst[0]);
  __m128 *dp2 = (__m128 *)(dst[1]);
  __m128 *dp3 = (__m128 *)(dst[2]);
  __m128i v1, v2, v3, va, vb, vc, vz;
  __m128 v1f, v2f, v3f, vaf, vbf, vcf, vzf;
  
  kdu_byte *sp = src;
  for (; width >= 16; width-=16, sp+=48, dp1+=4, dp2+=4, dp3+=4)
    { 
      // Load and de-interleave 3 dqwords of bytes
      v1 = _mm_loadu_si128((__m128i *) sp);
      v2 = _mm_loadu_si128((__m128i *)(sp+16));
      v3 = _mm_loadu_si128((__m128i *)(sp+32));
      v1 = _mm_shuffle_epi8(v1,vec_shuffle_556); // MSBs-LSBs = [5B | 5G | 6R]
      v2 = _mm_shuffle_epi8(v2,vec_shuffle_565); // MSBs-LSBs = [5B | 6G | 5R]
      v3 = _mm_shuffle_epi8(v3,vec_shuffle_655); // MSBs-LSBs = [6B | 5G | 5R]
      
      // Move 16R to va, 16G to vb and 16B to v3
      va = _mm_slli_si128(v1,10);    // Move 6 reds to to MSBs
      va = _mm_alignr_epi8(v2,va,5);     v2 = _mm_srli_si128(v2,5);
      va = _mm_alignr_epi8(v3,va,5);     v3 = _mm_srli_si128(v3,5);
      va = _mm_sub_epi8(va,vec_off); // Level adjused reds in va
      vb = _mm_slli_si128(v1,5);     // Move 5 greens to MSBs
      vb = _mm_alignr_epi8(v2,vb,6);     v2 = _mm_srli_si128(v2,6);
      vb = _mm_alignr_epi8(v3,vb,5);     v3 = _mm_srli_si128(v3,5);
      vb = _mm_sub_epi8(vb,vec_off); // Level adjusted greens in vb      
      v2 = _mm_alignr_epi8(v2,v1,5);     v3 = _mm_alignr_epi8(v3,v2,6);      
      v3 = _mm_sub_epi8(v3,vec_off); // Level adjusted blues in v3     

      // Expand all channels to 16 bits and upshift to the MSBs
      v1 = _mm_unpacklo_epi8(va,va);     va = _mm_unpackhi_epi8(va,va);
      v1 = _mm_sll_epi16(v1,vec_left);   va = _mm_sll_epi16(va,vec_left); 
      v2 = _mm_unpacklo_epi8(vb,vb);     vb = _mm_unpackhi_epi8(vb,vb);
      v2 = _mm_sll_epi16(v2,vec_left);   vb = _mm_sll_epi16(vb,vec_left); 
      vc = _mm_unpackhi_epi8(v3,v3);     v3 = _mm_unpacklo_epi8(v3,v3);
      vc = _mm_sll_epi16(vc,vec_left);   v3 = _mm_sll_epi16(v3,vec_left);

      // At this stage, the 3 channels are (v1,va), (v2,vb) and (v3,vc) 
      // Expand to 32 bits, convert, multiply and write the first 8
      // floats for each channel.
      vz = _mm_setzero_si128();          vz = _mm_unpacklo_epi16(vz,v1);
      v1 = _mm_unpackhi_epi16(v1,v1);    v1 = _mm_slli_epi32(v1,16);
      vzf = _mm_cvtepi32_ps(vz);         v1f = _mm_cvtepi32_ps(v1);
      vzf = _mm_mul_ps(vzf,vec_scale);   v1f = _mm_mul_ps(v1f,vec_scale);
      dp1[0] = vzf;   dp1[1] = v1f;
      vz = _mm_setzero_si128();          vz = _mm_unpacklo_epi16(vz,v2);
      v2 = _mm_unpackhi_epi16(v2,v2);    v2 = _mm_slli_epi32(v2,16);
      vzf = _mm_cvtepi32_ps(vz);         v2f = _mm_cvtepi32_ps(v2);
      vzf = _mm_mul_ps(vzf,vec_scale);   v2f = _mm_mul_ps(v2f,vec_scale);
      dp2[0] = vzf;   dp2[1] = v2f;
      vz = _mm_setzero_si128();          vz = _mm_unpacklo_epi16(vz,v3);
      v3 = _mm_unpackhi_epi16(v3,v3);    v3 = _mm_slli_epi32(v3,16);
      vzf = _mm_cvtepi32_ps(vz);         v3f = _mm_cvtepi32_ps(v3);
      vzf = _mm_mul_ps(vzf,vec_scale);   v3f = _mm_mul_ps(v3f,vec_scale);
      dp3[0] = vzf;   dp3[1] = v3f;
      
      // Expand to 32 bits, convert, multiply and write the second 8
      // floats for each channel.
      v1 = _mm_setzero_si128();          v1 = _mm_unpacklo_epi16(v1,va);
      va = _mm_unpackhi_epi16(va,va);    va = _mm_slli_epi32(va,16);
      v2 = _mm_setzero_si128();          v2 = _mm_unpacklo_epi16(v2,vb);
      vb = _mm_unpackhi_epi16(vb,vb);    vb = _mm_slli_epi32(vb,16);
      v3 = _mm_setzero_si128();          v3 = _mm_unpacklo_epi16(v3,vc);
      vc = _mm_unpackhi_epi16(vc,vc);    vc = _mm_slli_epi32(vc,16);
      v1f = _mm_cvtepi32_ps(v1);         v1f = _mm_mul_ps(v1f,vec_scale);
      vaf = _mm_cvtepi32_ps(va);         vaf = _mm_mul_ps(vaf,vec_scale);
      dp1[2] = v1f;  dp1[3] = vaf;
      v2f = _mm_cvtepi32_ps(v2);         v2f = _mm_mul_ps(v2f,vec_scale);
      vbf = _mm_cvtepi32_ps(vb);         vbf = _mm_mul_ps(vbf,vec_scale);
      dp2[2] = v2f;  dp2[3] = vbf;
      v3f = _mm_cvtepi32_ps(v3);         v3f = _mm_mul_ps(v3f,vec_scale);
      vcf = _mm_cvtepi32_ps(vc);         vcf = _mm_mul_ps(vcf,vec_scale);
      dp3[2] = v3f;  dp3[3] = vcf;
    }
  if (width > 0)
    { // Handle the last 3-45 input bytes and associated output dqwords
      // carefully so as to avoid buffer overrun.
      width += 2*width; // Convert to the number of remaining input bytes
      union { __m128i dqwords[3]; kdu_byte bytes[48]; };
      for (int c=0; c < width; c++) bytes[c] = sp[c];
      v1 = dqwords[0];  v2 = dqwords[1];  v3 = dqwords[2];
      v1 = _mm_shuffle_epi8(v1,vec_shuffle_556); // MSBs-LSBs = [5B | 5G | 6R]
      v2 = _mm_shuffle_epi8(v2,vec_shuffle_565); // MSBs-LSBs = [5B | 6G | 5R]
      v3 = _mm_shuffle_epi8(v3,vec_shuffle_655); // MSBs-LSBs = [6B | 5G | 5R]

      va = _mm_slli_si128(v1,10);    // Move 6 reds to to MSBs
      va = _mm_alignr_epi8(v2,va,5);     v2 = _mm_srli_si128(v2,5);
      va = _mm_alignr_epi8(v3,va,5);     v3 = _mm_srli_si128(v3,5);
      va = _mm_sub_epi8(va,vec_off); // Level adjused reds in va
      vb = _mm_slli_si128(v1,5);     // Move 5 greens to MSBs
      vb = _mm_alignr_epi8(v2,vb,6);     v2 = _mm_srli_si128(v2,6);
      vb = _mm_alignr_epi8(v3,vb,5);     v3 = _mm_srli_si128(v3,5);
      vb = _mm_sub_epi8(vb,vec_off); // Level adjusted greens in vb      
      v2 = _mm_alignr_epi8(v2,v1,5);     v3 = _mm_alignr_epi8(v3,v2,6);      
      v3 = _mm_sub_epi8(v3,vec_off); // Level adjusted blues in v3     
      
      v1 = _mm_unpacklo_epi8(va,va);     va = _mm_unpackhi_epi8(va,va);
      v1 = _mm_sll_epi16(v1,vec_left);   va = _mm_sll_epi16(va,vec_left); 
      v2 = _mm_unpacklo_epi8(vb,vb);     vb = _mm_unpackhi_epi8(vb,vb);
      v2 = _mm_sll_epi16(v2,vec_left);   vb = _mm_sll_epi16(vb,vec_left); 
      vc = _mm_unpackhi_epi8(v3,v3);     v3 = _mm_unpacklo_epi8(v3,v3);
      vc = _mm_sll_epi16(vc,vec_left);   v3 = _mm_sll_epi16(v3,vec_left);

      vz = _mm_setzero_si128();          vz = _mm_unpacklo_epi16(vz,v1);
      vzf = _mm_cvtepi32_ps(vz);         vzf = _mm_mul_ps(vzf,vec_scale);
      dp1[0] = vzf;
      vz = _mm_setzero_si128();          vz = _mm_unpacklo_epi16(vz,v2);
      vzf = _mm_cvtepi32_ps(vz);         vzf = _mm_mul_ps(vzf,vec_scale);
      dp2[0] = vzf;
      vz = _mm_setzero_si128();          vz = _mm_unpacklo_epi16(vz,v3);
      vzf = _mm_cvtepi32_ps(vz);         vzf = _mm_mul_ps(vzf,vec_scale);
      dp3[0] = vzf;
      
      if (width <= 12) return;
      v1 = _mm_unpackhi_epi16(v1,v1);    v1 = _mm_slli_epi32(v1,16);
      v1f = _mm_cvtepi32_ps(v1);         v1f = _mm_mul_ps(v1f,vec_scale);
      dp1[1] = v1f;
      v2 = _mm_unpackhi_epi16(v2,v2);    v2 = _mm_slli_epi32(v2,16);      
      v2f = _mm_cvtepi32_ps(v2);         v2f = _mm_mul_ps(v2f,vec_scale);
      dp2[1] = v2f;
      v3 = _mm_unpackhi_epi16(v3,v3);    v3 = _mm_slli_epi32(v3,16);      
      v3f = _mm_cvtepi32_ps(v3); v3f = _mm_mul_ps(v3f,vec_scale);
      dp3[1] = v3f;

      if (width <= 24) return;
      v1 = _mm_setzero_si128();          v1 = _mm_unpacklo_epi16(v1,va);
      va = _mm_unpackhi_epi16(va,va);    va = _mm_slli_epi32(va,16);
      v2 = _mm_setzero_si128();          v2 = _mm_unpacklo_epi16(v2,vb);
      vb = _mm_unpackhi_epi16(vb,vb);    vb = _mm_slli_epi32(vb,16);
      v3 = _mm_setzero_si128();          v3 = _mm_unpacklo_epi16(v3,vc);
      vc = _mm_unpackhi_epi16(vc,vc);    vc = _mm_slli_epi32(vc,16);
      v1f = _mm_cvtepi32_ps(v1);         v1f = _mm_mul_ps(v1f,vec_scale);
      dp1[2] = v1f;
      v2f = _mm_cvtepi32_ps(v2);         v2f = _mm_mul_ps(v2f,vec_scale);
      dp2[2] = v2f;
      v3f = _mm_cvtepi32_ps(v3);         v3f = _mm_mul_ps(v3f,vec_scale);
      dp3[2] = v3f;

      if (width <= 36) return;
      vaf = _mm_cvtepi32_ps(va);         vaf = _mm_mul_ps(vaf,vec_scale);
      dp1[3] = vaf;
      vbf = _mm_cvtepi32_ps(vb);         vbf = _mm_mul_ps(vbf,vec_scale);
      dp2[3] = vbf;
      vcf = _mm_cvtepi32_ps(vc);         vcf = _mm_mul_ps(vcf,vec_scale);
      dp3[3] = vcf;
    }
}

/*****************************************************************************/
/* EXTERN               ssse3_floats_from_uint8_ilv4                         */
/*****************************************************************************/

void
  ssse3_floats_from_uint8_ilv4(float **dst, kdu_byte *src, int width,
                               int src_precision, int tgt_precision,
                               bool is_absolute, bool src_signed)
{
  assert(!src_signed); // Function prototype generic; we don't do signed bytes
  int offset = (src_signed)?0:(1<<(src_precision-1));
  int upshift = 16 - src_precision;
  float scale = 1.0F  / (((float)(1<<16)) * ((float)(1<<16)));

  __m128 vec_scale = _mm_set1_ps(scale);
  __m128i vec_shuffle = _mm_loadu_si128((__m128i *) kd_shuffle_r4444);
  __m128i vec_off = _mm_set1_epi8((kdu_byte) offset);
  __m128i vec_left = _mm_cvtsi32_si128(upshift);
  __m128 *dp1 = (__m128 *)(dst[0]);
  __m128 *dp2 = (__m128 *)(dst[1]);
  __m128 *dp3 = (__m128 *)(dst[2]);
  __m128 *dp4 = (__m128 *)(dst[3]);
  __m128i v1, v2, v3, v4;
  __m128 v1f, v2f, v3f, v4f;
  
  kdu_byte *sp = src;
  for (; width >= 4; width-=4, sp+=16, dp1++, dp2++, dp3++, dp4++)
    { 
      v4 = _mm_loadu_si128((__m128i *) sp);
      v4 = _mm_shuffle_epi8(v4,vec_shuffle); // [4A | 4B | 4G | 4R]
      v4 = _mm_sub_epi8(v4,vec_off);
      v2 = _mm_unpacklo_epi8(v4,v4);    v4 = _mm_unpackhi_epi8(v4,v4);      
      v2 = _mm_sll_epi16(v2,vec_left);  v4 = _mm_sll_epi16(v4,vec_left);
     
      v1 = _mm_setzero_si128();         v1 = _mm_unpacklo_epi16(v1,v2);
      v2 = _mm_unpackhi_epi16(v2,v2);   v2 = _mm_slli_epi32(v2,16);
      v3 = _mm_setzero_si128();         v3 = _mm_unpacklo_epi16(v3,v4);
      v4 = _mm_unpackhi_epi16(v4,v4);   v4 = _mm_slli_epi32(v4,16);
      v1f = _mm_cvtepi32_ps(v1);        v2f = _mm_cvtepi32_ps(v2);
      v3f = _mm_cvtepi32_ps(v3);        v4f = _mm_cvtepi32_ps(v4);
      v1f = _mm_mul_ps(v1f,vec_scale);  v2f = _mm_mul_ps(v2f,vec_scale);
      v3f = _mm_mul_ps(v3f,vec_scale);  v4f = _mm_mul_ps(v4f,vec_scale);
      dp1[0] = v1f;  dp2[0] = v2f;  dp3[0] = v3f;  dp4[0] = v4f;
    }
  if (width > 0)
    { // Handle the last 1-15 input bytes carefully so as to avoid
      // buffer overrun.
      width <<= 2; // Convert to the number of remaining input bytes
      union { __m128i dqword; kdu_byte bytes[16]; };
      for (int c=0; c < width; c++) bytes[c] = sp[c];
      v4 = dqword;
      v4 = _mm_shuffle_epi8(v4,vec_shuffle); // [4A | 4B | 4G | 4R]
      v4 = _mm_sub_epi8(v4,vec_off);
      v2 = _mm_unpacklo_epi8(v4,v4);    v4 = _mm_unpackhi_epi8(v4,v4);      
      v2 = _mm_sll_epi16(v2,vec_left);  v4 = _mm_sll_epi16(v4,vec_left);
      
      v1 = _mm_setzero_si128();         v1 = _mm_unpacklo_epi16(v1,v2);
      v2 = _mm_unpackhi_epi16(v2,v2);   v2 = _mm_slli_epi32(v2,16);
      v3 = _mm_setzero_si128();         v3 = _mm_unpacklo_epi16(v3,v4);
      v4 = _mm_unpackhi_epi16(v4,v4);   v4 = _mm_slli_epi32(v4,16);
      v1f = _mm_cvtepi32_ps(v1);        v2f = _mm_cvtepi32_ps(v2);
      v3f = _mm_cvtepi32_ps(v3);        v4f = _mm_cvtepi32_ps(v4);
      v1f = _mm_mul_ps(v1f,vec_scale);  v2f = _mm_mul_ps(v2f,vec_scale);
      v3f = _mm_mul_ps(v3f,vec_scale);  v4f = _mm_mul_ps(v4f,vec_scale);
      dp1[0] = v1f;  dp2[0] = v2f;  dp3[0] = v3f;  dp4[0] = v4f;      
    }
}

/*****************************************************************************/
/* EXTERN               ssse3_int16_from_int16_ilv1                          */
/*****************************************************************************/

void
  ssse3_int16_from_int16_ilv1(kdu_int16 **dst, kdu_int16 *src, int width,
                              int src_precision, int tgt_precision,
                              bool is_absolute, bool src_signed)
{
  if (!is_absolute)
    tgt_precision = KDU_FIX_POINT;
  int offset = (src_signed)?0:(1<<(src_precision-1));
  int pre_upshift = 16 - src_precision;
  int post_downshift = 16 - tgt_precision;
  
  __m128i vec_off =   _mm_set1_epi16((kdu_int16) offset);
  __m128i vec_left =  _mm_cvtsi32_si128(pre_upshift);
  __m128i vec_right = _mm_cvtsi32_si128(post_downshift);
  __m128i *dp = (__m128i *)(dst[0]);
  __m128i v1, v2;
  
  kdu_int16 *sp = src;
  for (; width >= 16; width-=16, sp+=16, dp+=2)
    { 
      v1 = _mm_loadu_si128((__m128i *) sp);
      v2 = _mm_loadu_si128((__m128i *)(sp+8));
      v1 = _mm_add_epi16(v1,vec_off);    v2 = _mm_add_epi16(v2,vec_off);
      v1 = _mm_sll_epi16(v1,vec_left);   v2 = _mm_sll_epi16(v2,vec_left);
      v1 = _mm_sra_epi16(v1,vec_right);  v2 = _mm_sra_epi16(v2,vec_right);
      dp[0] = v1;  dp[1] = v2;
    }
  if (width > 0)
    { // Handle the last 1-15 input words and associated 1-2 output dqwords
      // carefully so as to avoid buffer overrun.
      union { __m128i dqwords[2]; kdu_int16 words[16]; };
      for (int c=0; c < width; c++) words[c] = sp[c];
      v1 = dqwords[0];  v2 = dqwords[1];
      v1 = _mm_add_epi16(v1,vec_off);    v2 = _mm_add_epi16(v2,vec_off);
      v1 = _mm_sll_epi16(v1,vec_left);   v2 = _mm_sll_epi16(v2,vec_left);
      v1 = _mm_sra_epi16(v1,vec_right);  v2 = _mm_sra_epi16(v2,vec_right);
      dp[0] = v1;
      if (width > 8)
        dp[1] = v2;
    }
}

/*****************************************************************************/
/* EXTERN               ssse3_int32_from_int16_ilv1                          */
/*****************************************************************************/

void
  ssse3_int32_from_int16_ilv1(kdu_int32 **dst, kdu_int16 *src, int width,
                              int src_precision, int tgt_precision,
                              bool is_absolute, bool src_signed)
{
  assert(is_absolute);
  int offset = (src_signed)?0:(1<<(src_precision-1));
  int pre_upshift = 16 - src_precision;
  int post_downshift = 32 - tgt_precision;
  
  __m128i vec_off =   _mm_set1_epi16((kdu_int16) offset);
  __m128i vec_left =  _mm_cvtsi32_si128(pre_upshift);
  __m128i vec_right = _mm_cvtsi32_si128(post_downshift);
  __m128i *dp = (__m128i *)(dst[0]);
  __m128i v1, v2, v3, v4;
  
  kdu_int16 *sp = src;
  for (; width >= 16; width-=16, sp+=16, dp+=4)
    { 
      v2 = _mm_loadu_si128((__m128i *) sp);
      v4 = _mm_loadu_si128((__m128i *)(sp+8));
      v2 = _mm_add_epi16(v2,vec_off);    v4 = _mm_add_epi16(v4,vec_off);
      v2 = _mm_sll_epi16(v2,vec_left);   v4 = _mm_sll_epi16(v4,vec_left);
      v1 = _mm_setzero_si128();          v1 = _mm_unpacklo_epi16(v1,v2);
      v2 = _mm_unpackhi_epi16(v2,v2);    v2 = _mm_slli_epi32(v2,16);
      v3 = _mm_setzero_si128();          v3 = _mm_unpacklo_epi16(v3,v4);
      v4 = _mm_unpackhi_epi16(v4,v4);    v4 = _mm_slli_epi32(v4,16);
      v1 = _mm_sra_epi32(v1,vec_right);  v2 = _mm_sra_epi32(v2,vec_right);
      v3 = _mm_sra_epi32(v3,vec_right);  v4 = _mm_sra_epi32(v4,vec_right);
      dp[0] = v1;  dp[1] = v2;  dp[2] = v3;  dp[3] = v4;
    }
  if (width > 0)
    { // Handle the last 1-15 input words and associated 1-4 output dqwords
      // carefully so as to avoid buffer overrun.
      union { __m128i dqwords[2]; kdu_int16 words[16]; };
      for (int c=0; c < width; c++) words[c] = sp[c];
      v2 = dqwords[0];  v4 = dqwords[1];
      v2 = _mm_add_epi16(v2,vec_off);    v4 = _mm_add_epi16(v4,vec_off);
      v2 = _mm_sll_epi16(v2,vec_left);   v4 = _mm_sll_epi16(v4,vec_left);
      v1 = _mm_setzero_si128();          v1 = _mm_unpacklo_epi16(v1,v2);
      v2 = _mm_unpackhi_epi16(v2,v2);    v2 = _mm_slli_epi32(v2,16);
      v3 = _mm_setzero_si128();          v3 = _mm_unpacklo_epi16(v3,v4);
      v4 = _mm_unpackhi_epi16(v4,v4);    v4 = _mm_slli_epi32(v4,16);
      v1 = _mm_sra_epi32(v1,vec_right);  v2 = _mm_sra_epi32(v2,vec_right);
      v3 = _mm_sra_epi32(v3,vec_right);  v4 = _mm_sra_epi32(v4,vec_right);
      dp[0] = v1;
      if (width <= 4) return;
      dp[1] = v2;
      if (width <= 8) return;
      dp[2] = v3;
      if (width <= 12) return;
      dp[3] = v4;
    }
}

/*****************************************************************************/
/* EXTERN               ssse3_floats_from_int16_ilv1                         */
/*****************************************************************************/

void
  ssse3_floats_from_int16_ilv1(float **dst, kdu_int16 *src, int width,
                               int src_precision, int tgt_precision,
                               bool is_absolute, bool src_signed)
{
  assert(!is_absolute);
  int offset = (src_signed)?0:(1<<(src_precision-1));
  int upshift = 16 - src_precision;
  float scale = 1.0F  / (((float)(1<<16)) * ((float)(1<<16)));
  
  __m128 vec_scale = _mm_set1_ps(scale);
  __m128i vec_off =   _mm_set1_epi16((kdu_int16) offset);
  __m128i vec_left =  _mm_cvtsi32_si128(upshift);
  __m128 *dp = (__m128 *)(dst[0]);
  __m128i v1, v2, v3, v4;
  __m128 v1f, v2f, v3f, v4f;
  
  kdu_int16 *sp = src;
  for (; width >= 16; width-=16, sp+=16, dp+=4)
    { 
      v2 = _mm_loadu_si128((__m128i *) sp);
      v4 = _mm_loadu_si128((__m128i *)(sp+8));
      v2 = _mm_add_epi16(v2,vec_off);    v4 = _mm_add_epi16(v4,vec_off);
      v2 = _mm_sll_epi16(v2,vec_left);   v4 = _mm_sll_epi16(v4,vec_left);
      v1 = _mm_setzero_si128();          v1 = _mm_unpacklo_epi16(v1,v2);
      v2 = _mm_unpackhi_epi16(v2,v2);    v2 = _mm_slli_epi32(v2,16);
      v3 = _mm_setzero_si128();          v3 = _mm_unpacklo_epi16(v3,v4);
      v4 = _mm_unpackhi_epi16(v4,v4);    v4 = _mm_slli_epi32(v4,16);
      v1f = _mm_cvtepi32_ps(v1);         v2f = _mm_cvtepi32_ps(v2);
      v3f = _mm_cvtepi32_ps(v3);         v4f = _mm_cvtepi32_ps(v4);
      v1f = _mm_mul_ps(v1f,vec_scale);   v2f = _mm_mul_ps(v2f,vec_scale);
      v3f = _mm_mul_ps(v3f,vec_scale);   v4f = _mm_mul_ps(v4f,vec_scale);
      dp[0] = v1f;  dp[1] = v2f;  dp[2] = v3f;  dp[3] = v4f;
    }
  if (width > 0)
    { // Handle the last 1-15 input words and associated 1-4 output dqwords
      // carefully so as to avoid buffer overrun.
      union { __m128i dqwords[2]; kdu_int16 words[16]; };
      for (int c=0; c < width; c++) words[c] = sp[c];
      v2 = dqwords[0];  v4 = dqwords[1];
      v2 = _mm_add_epi16(v2,vec_off);    v4 = _mm_add_epi16(v4,vec_off);
      v2 = _mm_sll_epi16(v2,vec_left);   v4 = _mm_sll_epi16(v4,vec_left);
      v1 = _mm_setzero_si128();          v1 = _mm_unpacklo_epi16(v1,v2);
      v2 = _mm_unpackhi_epi16(v2,v2);    v2 = _mm_slli_epi32(v2,16);
      v3 = _mm_setzero_si128();          v3 = _mm_unpacklo_epi16(v3,v4);
      v4 = _mm_unpackhi_epi16(v4,v4);    v4 = _mm_slli_epi32(v4,16);
      v1f = _mm_cvtepi32_ps(v1);         v2f = _mm_cvtepi32_ps(v2);
      v3f = _mm_cvtepi32_ps(v3);         v4f = _mm_cvtepi32_ps(v4);
      v1f = _mm_mul_ps(v1f,vec_scale);   v2f = _mm_mul_ps(v2f,vec_scale);
      v3f = _mm_mul_ps(v3f,vec_scale);   v4f = _mm_mul_ps(v4f,vec_scale);
      dp[0] = v1f;
      if (width <= 4) return;
      dp[1] = v2f;
      if (width <= 8) return;
      dp[2] = v3f;
      if (width <= 12) return;
      dp[3] = v4f;
    }
}

/*****************************************************************************/
/* EXTERN               ssse3_floats_from_floats_ilv1                        */
/*****************************************************************************/

void
  ssse3_floats_from_floats_ilv1(float **dst, float *src, int width,
                                int src_precision, int tgt_precision,
                                bool is_absolute, bool src_signed)
{
  assert(!is_absolute);
  float scale = 1.0F; // Amount to scale from src to unit range
  while (src_precision < 0)
    { src_precision += 16; scale *= (float)(1<<16); }
  while (src_precision > 16)
    { src_precision -= 16; scale *= 1.0F / (float)(1<<16); }
  scale *= 1.0F / (float)(1<<src_precision);
  float offset = (src_signed)?0.0F:0.5F;
  
  __m128 vec_scale = _mm_set1_ps(scale);
  __m128 vec_off = _mm_set1_ps(offset);
  __m128 v1f, v2f, v3f, v4f;
  __m128 *dp = (__m128 *)(dst[0]);
  
  float *sp = src;
  for (; width >= 16; width-=16, dp+=4, sp+=16)
    { 
      v1f = _mm_loadu_ps(sp);             v2f = _mm_loadu_ps(sp+4);
      v3f = _mm_loadu_ps(sp+8);           v4f = _mm_loadu_ps(sp+12);
      v1f = _mm_mul_ps(v1f,vec_scale);    v2f = _mm_mul_ps(v2f,vec_scale);
      v3f = _mm_mul_ps(v3f,vec_scale);    v4f = _mm_mul_ps(v4f,vec_scale);
      v1f = _mm_sub_ps(v1f,vec_off);      v2f = _mm_sub_ps(v2f,vec_off);
      v3f = _mm_sub_ps(v3f,vec_off);      v4f = _mm_sub_ps(v4f,vec_off);
      dp[0] = v1f;  dp[1] = v2f;  dp[2] = v3f;  dp[3] = v4f;
    }
  if (width > 0)
    { // Handle the last 1-15 input floats and associated 1-4 output dqwords
      // carefully so as to avoid buffer overrun.
      union { __m128 dqwords[4]; float floats[16]; };
      int c;
      for (c=0; c < width; c++) floats[c] = sp[c];
      for (; c < 16; c++) floats[c] = 0.0F;
      v1f=dqwords[0];  v2f=dqwords[1];  v3f=dqwords[2];  v4f=dqwords[3];
      v1f = _mm_mul_ps(v1f,vec_scale);    v2f = _mm_mul_ps(v2f,vec_scale);
      v3f = _mm_mul_ps(v3f,vec_scale);    v4f = _mm_mul_ps(v4f,vec_scale);
      v1f = _mm_sub_ps(v1f,vec_off);      v2f = _mm_sub_ps(v2f,vec_off);
      v3f = _mm_sub_ps(v3f,vec_off);      v4f = _mm_sub_ps(v4f,vec_off);
      dp[0] = v1f;
      if (width <= 4) return;
      dp[1] = v2f;
      if (width <= 8) return;
      dp[2] = v3f;
      if (width <= 12) return;
      dp[3] = v4f;
    }
}


/* ========================================================================= */
/*             SIMD functions used by `kdu_stripe_decompressor'              */
/* ========================================================================= */

/*****************************************************************************/
/* EXTERN              ssse3_int16_to_uint8_rs_ilv1                          */
/*****************************************************************************/

void ssse3_int16_to_uint8_rs_ilv1(kdu_byte *dst, kdu_int16 **src, int width,
                                  int precision, int orig_precision,
                                  bool is_absolute, bool dst_signed)
{
  assert(!dst_signed); // Function prototype generic; we don't do signed bytes
  if (!is_absolute)
    orig_precision = KDU_FIX_POINT;
  assert((orig_precision >= precision) && (orig_precision < 16));
    // NB: orig_precision cannot be 16 or the saturating shift below will
    // cause truncation of valid values.
  int downshift = orig_precision - precision;
  int offset_val = (1 << (orig_precision-1)) + ((1<<downshift) >> 1);
  int post_max_val = (1<<precision) - 1; // Max value after conversion

  __m128i vec_off = _mm_set1_epi16((kdu_int16) offset_val);
  __m128i vec_shift = _mm_cvtsi32_si128(downshift);
  __m128i vec_max = _mm_set1_epi8((kdu_byte) post_max_val);
  __m128i *sp = (__m128i *)(src[0]);
  __m128i v1, v2;

  kdu_byte *dp = dst;
  for (; width >= 16; width-=16, dp+=16, sp+=2)
    { 
      v1=sp[0];                          v2=sp[1];
      v1 = _mm_adds_epi16(v1,vec_off);   v2 = _mm_adds_epi16(v2,vec_off);
      v1 = _mm_sra_epi16(v1,vec_shift);  v2 = _mm_sra_epi16(v2,vec_shift);
      v1 = _mm_packus_epi16(v1,v2);      v1 = _mm_min_epu8(v1,vec_max);  
      _mm_storeu_si128((__m128i *) dp,v1);
    }
  if (width > 0)
    { // Take care to avoid buffer overrun while writing last 1 to 15 bytes
      union { __m128i dqword; kdu_byte bytes[16]; };
      v1=sp[0];                          v2=sp[1];
      v1 = _mm_adds_epi16(v1,vec_off);   v2 = _mm_adds_epi16(v2,vec_off);
      v1 = _mm_sra_epi16(v1,vec_shift);  v2 = _mm_sra_epi16(v2,vec_shift);
      v1 = _mm_packus_epi16(v1,v2);      dqword = _mm_min_epu8(v1,vec_max);  
      for (int c=0; c < width; c++) dp[c] = bytes[c];
    }
}

/*****************************************************************************/
/* EXTERN              ssse3_int16_to_uint8_rs_ilv3                          */
/*****************************************************************************/

void ssse3_int16_to_uint8_rs_ilv3(kdu_byte *dst, kdu_int16 **src, int width,
                                  int precision, int orig_precision,
                                  bool is_absolute, bool dst_signed)
{
  assert(!dst_signed); // Function prototype generic; we don't do signed bytes
  if (!is_absolute)
    orig_precision = KDU_FIX_POINT;
  assert((orig_precision >= precision) && (orig_precision < 16));
    // NB: orig_precision cannot be 16 or the saturating shift below will
    // cause truncation of valid values.
  int downshift = orig_precision - precision;
  int offset_val = (1 << (orig_precision-1)) + ((1<<downshift) >> 1);
  int post_max_val = (1<<precision) - 1; // Max value after conversion
  
  __m128i vec_shuffle_556 = _mm_loadu_si128((__m128i *) kd_shuffle_556);
  __m128i vec_shuffle_565 = _mm_loadu_si128((__m128i *) kd_shuffle_565);
  __m128i vec_shuffle_655 = _mm_loadu_si128((__m128i *) kd_shuffle_655);
  __m128i vec_off = _mm_set1_epi16((kdu_int16) offset_val);
  __m128i vec_shift = _mm_cvtsi32_si128(downshift);
  __m128i vec_max = _mm_set1_epi8((kdu_byte) post_max_val);
  __m128i *sp1=(__m128i *)(src[0]);
  __m128i *sp2=(__m128i *)(src[1]);
  __m128i *sp3=(__m128i *)(src[2]);
  __m128i v1, v2, v3, vt;
  
  kdu_byte *dp = dst;
  for (; width >= 16; width-=16, sp1+=2, sp2+=2, sp3+=2, dp+=48)
    { 
      // Prepare dqwords with: 16xR, 16xG, 16xB
      v1=sp1[0];                         vt=sp1[1];
      v1 = _mm_adds_epi16(v1,vec_off);   vt = _mm_adds_epi16(vt,vec_off);
      v1 = _mm_sra_epi16(v1,vec_shift);  vt = _mm_sra_epi16(vt,vec_shift);
      v1 = _mm_packus_epi16(v1,vt);      v1 = _mm_min_epu8(v1,vec_max);  
      v2=sp2[0];                         vt=sp2[1];
      v2 = _mm_adds_epi16(v2,vec_off);   vt = _mm_adds_epi16(vt,vec_off);
      v2 = _mm_sra_epi16(v2,vec_shift);  vt = _mm_sra_epi16(vt,vec_shift);
      v2 = _mm_packus_epi16(v2,vt);      v2 = _mm_min_epu8(v2,vec_max);  
      v3=sp3[0];                         vt=sp3[1];
      v3 = _mm_adds_epi16(v3,vec_off);   vt = _mm_adds_epi16(vt,vec_off);
      v3 = _mm_sra_epi16(v3,vec_shift);  vt = _mm_sra_epi16(vt,vec_shift);
      v3 = _mm_packus_epi16(v3,vt);      v3 = _mm_min_epu8(v3,vec_max);
      
      // Generate 3 output dqwords
      vt = _mm_slli_si128(v1,10); // Keep first 6 red bytes
      vt = _mm_alignr_epi8(v2,vt,5);     v2 = _mm_srli_si128(v2,5);
      vt = _mm_alignr_epi8(v3,vt,5);     v3 = _mm_srli_si128(v3,5);
      vt = _mm_shuffle_epi8(vt,vec_shuffle_556); // [5B | 5G | 6R]
      _mm_storeu_si128((__m128i *) dp,vt);
      vt = _mm_slli_si128(v1,5); // Keep next 5 red bytes
      vt = _mm_alignr_epi8(v2,vt,6);     v2 = _mm_srli_si128(v2,6);
      vt = _mm_alignr_epi8(v3,vt,5);     v3 = _mm_srli_si128(v3,5);
      vt = _mm_shuffle_epi8(vt,vec_shuffle_565); // [5B | 6G | 5R]
      _mm_storeu_si128((__m128i *)(dp+16),vt);
      vt = _mm_alignr_epi8(v2,v1,5);
      vt = _mm_alignr_epi8(v3,vt,6);
      vt = _mm_shuffle_epi8(vt,vec_shuffle_655); // [6B | 5G | 5R]
      _mm_storeu_si128((__m128i *)(dp+32),vt);
    }
  if (width > 0)
    { // Take care to avoid buffer overrun while writing last 3 to 45 bytes
      union { __m128i dqwords[3]; kdu_byte bytes[48]; };
      width += 2*width; // Convert to number of remaining output bytes
      v1=sp1[0];                         vt=sp1[1];
      v1 = _mm_adds_epi16(v1,vec_off);   vt = _mm_adds_epi16(vt,vec_off);
      v1 = _mm_sra_epi16(v1,vec_shift);  vt = _mm_sra_epi16(vt,vec_shift);
      v1 = _mm_packus_epi16(v1,vt);      v1 = _mm_min_epu8(v1,vec_max);  
      v2=sp2[0];                         vt=sp2[1];
      v2 = _mm_adds_epi16(v2,vec_off);   vt = _mm_adds_epi16(vt,vec_off);
      v2 = _mm_sra_epi16(v2,vec_shift);  vt = _mm_sra_epi16(vt,vec_shift);
      v2 = _mm_packus_epi16(v2,vt);      v2 = _mm_min_epu8(v2,vec_max);  
      v3=sp3[0];                         vt=sp3[1];
      v3 = _mm_adds_epi16(v3,vec_off);   vt = _mm_adds_epi16(vt,vec_off);
      v3 = _mm_sra_epi16(v3,vec_shift);  vt = _mm_sra_epi16(vt,vec_shift);
      v3 = _mm_packus_epi16(v3,vt);      v3 = _mm_min_epu8(v3,vec_max);  

      vt = _mm_slli_si128(v1,10); // Keep first 6 red bytes
      vt = _mm_alignr_epi8(v2,vt,5);     v2 = _mm_srli_si128(v2,5);
      vt = _mm_alignr_epi8(v3,vt,5);     v3 = _mm_srli_si128(v3,5);
      dqwords[0] = _mm_shuffle_epi8(vt,vec_shuffle_556); // [5B | 5G | 6R]
      vt = _mm_slli_si128(v1,5); // Keep next 5 red bytes
      vt = _mm_alignr_epi8(v2,vt,6);     v2 = _mm_srli_si128(v2,6);
      vt = _mm_alignr_epi8(v3,vt,5);     v3 = _mm_srli_si128(v3,5);
      dqwords[1] = _mm_shuffle_epi8(vt,vec_shuffle_565); // [5B | 6G | 5R]
      vt = _mm_alignr_epi8(v2,v1,5);
      vt = _mm_alignr_epi8(v3,vt,6);
      dqwords[2] = _mm_shuffle_epi8(vt,vec_shuffle_655); // [6B | 5G | 5R]
      for (int c=0; c < width; c++) dp[c] = bytes[c];
    }
}

/*****************************************************************************/
/* EXTERN              ssse3_int16_to_uint8_rs_ilv4                          */
/*****************************************************************************/

void ssse3_int16_to_uint8_rs_ilv4(kdu_byte *dst, kdu_int16 **src, int width,
                                  int precision, int orig_precision,
                                  bool is_absolute, bool dst_signed)
{
  assert(!dst_signed); // Function prototype generic; we don't do signed bytes
  if (!is_absolute)
    orig_precision = KDU_FIX_POINT;
  assert((orig_precision >= precision) && (orig_precision < 16));
    // NB: orig_precision cannot be 16 or the saturating shift below will
    // cause truncation of valid values.
  int downshift = orig_precision - precision;
  int offset_val = (1 << (orig_precision-1)) + ((1<<downshift) >> 1);
  int post_max_val = (1<<precision) - 1; // Max value after conversion
  
  __m128i vec_off = _mm_set1_epi16((kdu_int16) offset_val);
  __m128i vec_shift = _mm_cvtsi32_si128(downshift);
  __m128i vec_max = _mm_set1_epi8((kdu_byte) post_max_val);
  __m128i *sp1=(__m128i *)(src[0]);
  __m128i *sp2=(__m128i *)(src[1]);
  __m128i *sp3=(__m128i *)(src[2]);
  __m128i *sp4=(__m128i *)(src[3]);
  __m128i v1, v2, v3, v4, vt, vu;

  kdu_byte *dp = dst;
  for (; width >= 16; width-=16, sp1+=2, sp2+=2, sp3+=2, sp4+=2, dp+=64)
    { 
      // Prepare four output dqwords
      v1=sp1[0];                         vt=sp1[1];
      v1 = _mm_adds_epi16(v1,vec_off);   vt = _mm_adds_epi16(vt,vec_off);
      v1 = _mm_sra_epi16(v1,vec_shift);  vt = _mm_sra_epi16(vt,vec_shift);
      v1 = _mm_packus_epi16(v1,vt);      v1 = _mm_min_epu8(v1,vec_max);  
      v2=sp2[0];                         vt=sp2[1];
      v2 = _mm_adds_epi16(v2,vec_off);   vt = _mm_adds_epi16(vt,vec_off);
      v2 = _mm_sra_epi16(v2,vec_shift);  vt = _mm_sra_epi16(vt,vec_shift);
      v2 = _mm_packus_epi16(v2,vt);      v2 = _mm_min_epu8(v2,vec_max);  
      v3=sp3[0];                         vt=sp3[1];
      v3 = _mm_adds_epi16(v3,vec_off);   vt = _mm_adds_epi16(vt,vec_off);
      v3 = _mm_sra_epi16(v3,vec_shift);  vt = _mm_sra_epi16(vt,vec_shift);
      v3 = _mm_packus_epi16(v3,vt);      v3 = _mm_min_epu8(v3,vec_max);  
      v4=sp4[0];                         vt=sp4[1];
      v4 = _mm_adds_epi16(v4,vec_off);   vt = _mm_adds_epi16(vt,vec_off);
      v4 = _mm_sra_epi16(v4,vec_shift);  vt = _mm_sra_epi16(vt,vec_shift);
      v4 = _mm_packus_epi16(v4,vt);      v4 = _mm_min_epu8(v4,vec_max);  

      // Generate 4 output dqwords
      vu = _mm_unpackhi_epi8(v1,v2);     v1 = _mm_unpacklo_epi8(v1,v2);
      vt = _mm_unpackhi_epi8(v3,v4);     v3 = _mm_unpacklo_epi8(v3,v4);
      v2 = _mm_unpacklo_epi16(v1,v3);    v1 = _mm_unpackhi_epi16(v1,v3);
      _mm_storeu_si128((__m128i *) dp,v2);
      _mm_storeu_si128((__m128i *)(dp+16),v1);
      v4 = _mm_unpacklo_epi16(vu,vt);    vu = _mm_unpackhi_epi16(vu,vt);
      _mm_storeu_si128((__m128i *)(dp+32),v4);
      _mm_storeu_si128((__m128i *)(dp+48),vu);
    }
  if (width > 0)
    { // Take care writing final 4-60 bytes to avoid buffer overrun
      union { __m128i dqwords[4]; kdu_byte bytes[64]; };
      width <<= 2; // Convert to number of remaining output bytes
      v1=sp1[0];                         vt=sp1[1];
      v1 = _mm_adds_epi16(v1,vec_off);   vt = _mm_adds_epi16(vt,vec_off);
      v1 = _mm_sra_epi16(v1,vec_shift);  vt = _mm_sra_epi16(vt,vec_shift);
      v1 = _mm_packus_epi16(v1,vt);      v1 = _mm_min_epu8(v1,vec_max);  
      v2=sp2[0];                         vt=sp2[1];
      v2 = _mm_adds_epi16(v2,vec_off);   vt = _mm_adds_epi16(vt,vec_off);
      v2 = _mm_sra_epi16(v2,vec_shift);  vt = _mm_sra_epi16(vt,vec_shift);
      v2 = _mm_packus_epi16(v2,vt);      v2 = _mm_min_epu8(v2,vec_max);  
      v3=sp3[0];                         vt=sp3[1];
      v3 = _mm_adds_epi16(v3,vec_off);   vt = _mm_adds_epi16(vt,vec_off);
      v3 = _mm_sra_epi16(v3,vec_shift);  vt = _mm_sra_epi16(vt,vec_shift);
      v3 = _mm_packus_epi16(v3,vt);      v3 = _mm_min_epu8(v3,vec_max);  
      v4=sp4[0];                         vt=sp4[1];
      v4 = _mm_adds_epi16(v4,vec_off);   vt = _mm_adds_epi16(vt,vec_off);
      v4 = _mm_sra_epi16(v4,vec_shift);  vt = _mm_sra_epi16(vt,vec_shift);
      v4 = _mm_packus_epi16(v4,vt);      v4 = _mm_min_epu8(v4,vec_max);  
      
      vu = _mm_unpackhi_epi8(v1,v2);     v1 = _mm_unpacklo_epi8(v1,v2);
      vt = _mm_unpackhi_epi8(v3,v4);     v3 = _mm_unpacklo_epi8(v3,v4);
      dqwords[0] = _mm_unpacklo_epi16(v1,v3);
      dqwords[1] = _mm_unpackhi_epi16(v1,v3);
      dqwords[2] = _mm_unpacklo_epi16(vu,vt);
      dqwords[3] = _mm_unpackhi_epi16(vu,vt);
      for (int c=0; c < width; c++) dp[c] = bytes[c];
    }
}

/*****************************************************************************/
/* EXTERN               ssse3_floats_to_uint8_ilv1                           */
/*****************************************************************************/

void ssse3_floats_to_uint8_ilv1(kdu_byte *dst, float **src, int width,
                                int precision, int orig_precision,
                                bool is_absolute, bool dst_signed)
{
  assert(!dst_signed); // Function prototype generic; we don't do signed bytes
  float scale = (float)(1<<precision);
  int offset_val = (1 << (precision-1));
  int post_max_val = (1<<precision) - 1; // Max value after conversion
  
  int mxcsr_orig = _mm_getcsr();
  int mxcsr_cur = mxcsr_orig & ~(3<<13); // Reset rounding control bits
  _mm_setcsr(mxcsr_cur);

  __m128 vec_scale = _mm_set1_ps(scale);
  __m128i vec_off = _mm_set1_epi16((kdu_int16) offset_val);
  __m128i vec_max = _mm_set1_epi8((kdu_byte) post_max_val);
  __m128 *sp = (__m128 *)(src[0]);
  __m128 v1f, v2f, v3f, v4f;
  __m128i v1, v2, v3, v4;
  
  kdu_byte *dp=dst;
  for (; width >= 16; width-=16, sp+=4, dp+=16)
    { 
      v1f=sp[0];                         v2f=sp[1];
      v3f=sp[2];                         v4f=sp[3];
      v1f = _mm_mul_ps(v1f,vec_scale);   v2f = _mm_mul_ps(v2f,vec_scale);
      v3f = _mm_mul_ps(v3f,vec_scale);   v4f = _mm_mul_ps(v4f,vec_scale);
      v1 = _mm_cvtps_epi32(v1f);         v2 = _mm_cvtps_epi32(v2f);
      v3 = _mm_cvtps_epi32(v3f);         v4 = _mm_cvtps_epi32(v4f);
      v1 = _mm_packs_epi32(v1,v2);       v2 = _mm_packs_epi32(v3,v4);
      v1 = _mm_adds_epi16(v1,vec_off);   v2 = _mm_adds_epi16(v2,vec_off);
      v1 = _mm_packus_epi16(v1,v2);      v1 = _mm_min_epu8(v1,vec_max);  
      _mm_storeu_si128((__m128i *) dp,v1);
    }
  if (width > 0)
    { // Take care writing final 1-15 output bytes to avoid buffer overrun
      union { __m128i dqword; kdu_byte bytes[16]; };
      v1f=sp[0];                         v2f=sp[1];
      v3f=sp[2];                         v4f=sp[3];
      v1f = _mm_mul_ps(v1f,vec_scale);   v2f = _mm_mul_ps(v2f,vec_scale);
      v3f = _mm_mul_ps(v3f,vec_scale);   v4f = _mm_mul_ps(v4f,vec_scale);
      v1 = _mm_cvtps_epi32(v1f);         v2 = _mm_cvtps_epi32(v2f);
      v3 = _mm_cvtps_epi32(v3f);         v4 = _mm_cvtps_epi32(v4f);
      v1 = _mm_packs_epi32(v1,v2);       v2 = _mm_packs_epi32(v3,v4);
      v1 = _mm_adds_epi16(v1,vec_off);   v2 = _mm_adds_epi16(v2,vec_off);
      v1 = _mm_packus_epi16(v1,v2);      v1 = _mm_min_epu8(v1,vec_max);
      for (int c=0; c < width; c++) dp[c] = bytes[c];
    }
  
  _mm_setcsr(mxcsr_orig); // Restore rounding control bits
}

/*****************************************************************************/
/* EXTERN               ssse3_floats_to_uint8_ilv3                           */
/*****************************************************************************/

void ssse3_floats_to_uint8_ilv3(kdu_byte *dst, float **src, int width,
                                int precision, int orig_precision,
                                bool is_absolute, bool dst_signed)
{
  assert(!dst_signed); // Function prototype generic; we don't do signed bytes
  float scale = (float)(1<<precision);
  int offset_val = (1 << (precision-1));
  int post_max_val = (1<<precision) - 1; // Max value after conversion
  
  int mxcsr_orig = _mm_getcsr();
  int mxcsr_cur = mxcsr_orig & ~(3<<13); // Reset rounding control bits
  _mm_setcsr(mxcsr_cur);
  
  __m128 vec_scale = _mm_set1_ps(scale);
  __m128i vec_off = _mm_set1_epi16((kdu_int16) offset_val);
  __m128i vec_max = _mm_set1_epi8((kdu_byte) post_max_val);
  __m128i vec_shuffle_556 = _mm_loadu_si128((__m128i *) kd_shuffle_556);
  __m128i vec_shuffle_565 = _mm_loadu_si128((__m128i *) kd_shuffle_565);
  __m128i vec_shuffle_655 = _mm_loadu_si128((__m128i *) kd_shuffle_655);
  __m128 *sp1=(__m128 *)(src[0]);
  __m128 *sp2=(__m128 *)(src[1]);
  __m128 *sp3=(__m128 *)(src[2]);
  __m128 v1f, v2f, v3f, v4f;
  __m128i v1, v2, v3, v4, v5, v6, vt;
  
  kdu_byte *dp = dst;
  for (; width >= 16; width-=16, sp1+=2, sp2+=2, sp3+=2, dp+=48)
    { 
      // Prepare three pairs of dqwords, each with 8 packed 16-bit words
      v1f=sp1[0];                        v2f=sp1[1];
      v1f = _mm_mul_ps(v1f,vec_scale);   v2f = _mm_mul_ps(v2f,vec_scale);
      v1 = _mm_cvtps_epi32(v1f);         v2 = _mm_cvtps_epi32(v2f);
      v1 = _mm_packs_epi32(v1,v2);
      v3f=sp1[2];                        v4f=sp1[3];  
      v3f = _mm_mul_ps(v3f,vec_scale);   v4f = _mm_mul_ps(v4f,vec_scale);
      v2 = _mm_cvtps_epi32(v3f);         v3 = _mm_cvtps_epi32(v4f);
      v2 = _mm_packs_epi32(v2,v3);
      v1f=sp2[0];                        v2f=sp2[1];  
      v1f = _mm_mul_ps(v1f,vec_scale);   v2f = _mm_mul_ps(v2f,vec_scale);
      v3 = _mm_cvtps_epi32(v1f);         v4 = _mm_cvtps_epi32(v2f);
      v3 = _mm_packs_epi32(v3,v4);
      v3f=sp2[2];                        v4f=sp2[3];  
      v3f = _mm_mul_ps(v3f,vec_scale);   v4f = _mm_mul_ps(v4f,vec_scale);
      v4 = _mm_cvtps_epi32(v3f);         v5 = _mm_cvtps_epi32(v4f);
      v4 = _mm_packs_epi32(v4,v5);
      v1f=sp3[0];                        v2f=sp3[1];  
      v1f = _mm_mul_ps(v1f,vec_scale);   v2f = _mm_mul_ps(v2f,vec_scale);
      v5 = _mm_cvtps_epi32(v1f);         v6 = _mm_cvtps_epi32(v2f);
      v5 = _mm_packs_epi32(v5,v6);
      v3f=sp3[2];                        v4f=sp3[3];  
      v3f = _mm_mul_ps(v3f,vec_scale);   v4f = _mm_mul_ps(v4f,vec_scale);
      v6 = _mm_cvtps_epi32(v3f);         vt = _mm_cvtps_epi32(v4f);
      v6 = _mm_packs_epi32(v6,vt);
      
      // Convert to 3 dqwords with: 16xR, 16xG, 16xB
      v1 = _mm_adds_epi16(v1,vec_off);   v2 = _mm_adds_epi16(v2,vec_off);
      v1 = _mm_packus_epi16(v1,v2);
      v3 = _mm_adds_epi16(v3,vec_off);   v4 = _mm_adds_epi16(v4,vec_off);
      v2 = _mm_packus_epi16(v3,v4);
      v5 = _mm_adds_epi16(v5,vec_off);   v6 = _mm_adds_epi16(v6,vec_off);
      v3 = _mm_packus_epi16(v5,v6);
      v1 = _mm_min_epu8(v1,vec_max);     v2 = _mm_min_epu8(v2,vec_max);
      v3 = _mm_min_epu8(v3,vec_max);
      
      // Generate 3 output dqwords
      vt = _mm_slli_si128(v1,10); // Keep first 6 red bits
      vt = _mm_alignr_epi8(v2,vt,5);     v2 = _mm_srli_si128(v2,5);
      vt = _mm_alignr_epi8(v3,vt,5);     v3 = _mm_srli_si128(v3,5);
      vt = _mm_shuffle_epi8(vt,vec_shuffle_556); // [5B | 5G | 6R]
      _mm_storeu_si128((__m128i *) dp,vt);
      vt = _mm_slli_si128(v1,5); // Keep next 5 red bits
      vt = _mm_alignr_epi8(v2,vt,6);     v2 = _mm_srli_si128(v2,6);
      vt = _mm_alignr_epi8(v3,vt,5);     v3 = _mm_srli_si128(v3,5);
      vt = _mm_shuffle_epi8(vt,vec_shuffle_565); // [5B | 6G | 5R]
      _mm_storeu_si128((__m128i *)(dp+16),vt);
      vt = _mm_alignr_epi8(v2,v1,5);
      vt = _mm_alignr_epi8(v3,vt,6);
      vt = _mm_shuffle_epi8(vt,vec_shuffle_655); // [6B | 5G | 5R]
      _mm_storeu_si128((__m128i *)(dp+32),vt);
    }
  if (width > 0)
    { // Take care writing final 3-45 output bytes to avoid buffer overrun
      union { __m128i dqwords[3]; kdu_byte bytes[48]; };
      width += 2*width; // Convert to number of remaining output bytes
      v1f=sp1[0];                        v2f=sp1[1];  
      v1f = _mm_mul_ps(v1f,vec_scale);   v2f = _mm_mul_ps(v2f,vec_scale);
      v1 = _mm_cvtps_epi32(v1f);         v2 = _mm_cvtps_epi32(v2f);
      v1 = _mm_packs_epi32(v1,v2);
      v3f=sp1[2];                        v4f=sp1[3];  
      v3f = _mm_mul_ps(v3f,vec_scale);   v4f = _mm_mul_ps(v4f,vec_scale);
      v2 = _mm_cvtps_epi32(v3f);         v3 = _mm_cvtps_epi32(v4f);
      v2 = _mm_packs_epi32(v2,v3);
      v1f=sp2[0];                        v2f=sp2[1];  
      v1f = _mm_mul_ps(v1f,vec_scale);   v2f = _mm_mul_ps(v2f,vec_scale);
      v3 = _mm_cvtps_epi32(v1f);         v4 = _mm_cvtps_epi32(v2f);
      v3 = _mm_packs_epi32(v3,v4);
      v3f=sp2[2];                        v4f=sp2[3];  
      v3f = _mm_mul_ps(v3f,vec_scale);   v4f = _mm_mul_ps(v4f,vec_scale);
      v4 = _mm_cvtps_epi32(v3f);         v5 = _mm_cvtps_epi32(v4f);
      v4 = _mm_packs_epi32(v4,v5);
      v1f=sp3[0];                        v2f=sp3[1];  
      v1f = _mm_mul_ps(v1f,vec_scale);   v2f = _mm_mul_ps(v2f,vec_scale);
      v5 = _mm_cvtps_epi32(v1f);         v6 = _mm_cvtps_epi32(v2f);
      v5 = _mm_packs_epi32(v5,v6);
      v3f=sp3[2];                        v4f=sp3[3];  
      v3f = _mm_mul_ps(v3f,vec_scale);   v4f = _mm_mul_ps(v4f,vec_scale);
      v6 = _mm_cvtps_epi32(v3f);         vt = _mm_cvtps_epi32(v4f);
      v6 = _mm_packs_epi32(v6,vt);
      
      v1 = _mm_adds_epi16(v1,vec_off);   v2 = _mm_adds_epi16(v2,vec_off);
      v1 = _mm_packus_epi16(v1,v2);
      v3 = _mm_adds_epi16(v3,vec_off);   v4 = _mm_adds_epi16(v4,vec_off);
      v2 = _mm_packus_epi16(v3,v4);
      v5 = _mm_adds_epi16(v5,vec_off);   v6 = _mm_adds_epi16(v6,vec_off);
      v3 = _mm_packus_epi16(v5,v6);
      v1 = _mm_min_epu8(v1,vec_max);     v2 = _mm_min_epu8(v2,vec_max);
      v3 = _mm_min_epu8(v3,vec_max);

      vt = _mm_slli_si128(v1,10); // Keep first 6 red bits
      vt = _mm_alignr_epi8(v2,vt,5);     v2 = _mm_srli_si128(v2,5);
      vt = _mm_alignr_epi8(v3,vt,5);     v3 = _mm_srli_si128(v3,5);
      dqwords[0] = _mm_shuffle_epi8(vt,vec_shuffle_556); // [5B | 5G | 6R]
      vt = _mm_slli_si128(v1,5); // Keep next 5 red bits
      vt = _mm_alignr_epi8(v2,vt,6);     v2 = _mm_srli_si128(v2,6);
      vt = _mm_alignr_epi8(v3,vt,5);     v3 = _mm_srli_si128(v3,5);
      dqwords[1] = _mm_shuffle_epi8(vt,vec_shuffle_565); // [5B | 6G | 5R]
      vt = _mm_alignr_epi8(v2,v1,5);
      vt = _mm_alignr_epi8(v3,vt,6);
      dqwords[2] = _mm_shuffle_epi8(vt,vec_shuffle_655); // [6B | 5G | 5R]
      for (int c=0; c < width; c++) dp[c] = bytes[c];
    }
  
  _mm_setcsr(mxcsr_orig); // Restore rounding control bits
}

/*****************************************************************************/
/* EXTERN               ssse3_floats_to_uint8_ilv4                           */
/*****************************************************************************/

void ssse3_floats_to_uint8_ilv4(kdu_byte *dst, float **src, int width,
                                int precision, int orig_precision,
                                bool is_absolute, bool dst_signed)
{
  assert(!dst_signed); // Function prototype generic; we don't do signed bytes
  float scale = (float)(1<<precision);
  int offset_val = (1 << (precision-1));
  int post_max_val = (1<<precision) - 1; // Max value after conversion
  
  int mxcsr_orig = _mm_getcsr();
  int mxcsr_cur = mxcsr_orig & ~(3<<13); // Reset rounding control bits
  _mm_setcsr(mxcsr_cur);

  __m128 vec_scale = _mm_set1_ps(scale);
  __m128i vec_off = _mm_set1_epi16((kdu_int16) offset_val);
  __m128i vec_max = _mm_set1_epi8((kdu_byte) post_max_val);
  __m128 *sp1=(__m128 *)(src[0]);
  __m128 *sp2=(__m128 *)(src[1]);
  __m128 *sp3=(__m128 *)(src[2]);
  __m128 *sp4=(__m128 *)(src[3]);
  __m128 v1f, v2f, v3f, v4f;
  __m128i v1, v2, v3, v4, v5;
  
  kdu_byte *dp = dst;
  for (; width >= 8; width-=8, sp1+=2, sp2+=2, sp3+=2, sp4+=2, dp+=32)
    { 
      // Prepare four dqwords, each with 8 packed 16-bit words
      v1f=sp1[0];                        v2f=sp1[1];
      v1f = _mm_mul_ps(v1f,vec_scale);   v2f = _mm_mul_ps(v2f,vec_scale);
      v1 = _mm_cvtps_epi32(v1f);         v2 = _mm_cvtps_epi32(v2f);
      v1 = _mm_packs_epi32(v1,v2);
      v3f=sp2[0];                        v4f=sp2[1];  
      v3f = _mm_mul_ps(v3f,vec_scale);   v4f = _mm_mul_ps(v4f,vec_scale);
      v2 = _mm_cvtps_epi32(v3f);         v3 = _mm_cvtps_epi32(v4f);
      v2 = _mm_packs_epi32(v2,v3);
      v1f=sp3[0];                        v2f=sp3[1];  
      v1f = _mm_mul_ps(v1f,vec_scale);   v2f = _mm_mul_ps(v2f,vec_scale);
      v3 = _mm_cvtps_epi32(v1f);         v4 = _mm_cvtps_epi32(v2f);
      v3 = _mm_packs_epi32(v3,v4);
      v3f=sp4[0];                        v4f=sp4[1];  
      v3f = _mm_mul_ps(v3f,vec_scale);   v4f = _mm_mul_ps(v4f,vec_scale);
      v4 = _mm_cvtps_epi32(v3f);         v5 = _mm_cvtps_epi32(v4f);
      v4 = _mm_packs_epi32(v4,v5);
  
      // Convert to 2 dqwords: comps 1,2 in lo qwords; comps 3,4 in hi qwords
      v1 = _mm_adds_epi16(v1,vec_off);   v2 = _mm_adds_epi16(v2,vec_off);
      v3 = _mm_adds_epi16(v3,vec_off);   v4 = _mm_adds_epi16(v4,vec_off);
      v1 = _mm_packus_epi16(v1,v3);      v2 = _mm_packus_epi16(v2,v4);
      v1 = _mm_min_epu8(v1,vec_max);     v2 = _mm_min_epu8(v2,vec_max);
  
      // Generate 2 interleaved output dqwords
      v5 = _mm_unpacklo_epi8(v1,v2);  // Interleaves components 1 and 2
      v1 = _mm_unpackhi_epi8(v1,v2);  // Interleaves components 3 and 4
      v2 = _mm_unpacklo_epi16(v5,v1);
      _mm_storeu_si128((__m128i *) dp,v2);
      v5 = _mm_unpackhi_epi16(v5,v1);
      _mm_storeu_si128((__m128i *)(dp+16),v5);
    }
  if (width > 0)
    { // Take care writing final 4 to 28 output bytes to avoid buffer overrun
      union { __m128i dqwords[2]; kdu_byte bytes[32]; };
      width <<= 2; // Convert to number of remaining output bytes
      v1f=sp1[0];                        v2f=sp1[1];
      v1f = _mm_mul_ps(v1f,vec_scale);   v2f = _mm_mul_ps(v2f,vec_scale);
      v1 = _mm_cvtps_epi32(v1f);         v2 = _mm_cvtps_epi32(v2f);
      v1 = _mm_packs_epi32(v1,v2);
      v3f=sp2[0];                        v4f=sp2[1];
      v3f = _mm_mul_ps(v3f,vec_scale);   v4f = _mm_mul_ps(v4f,vec_scale);
      v2 = _mm_cvtps_epi32(v3f);         v3 = _mm_cvtps_epi32(v4f);
      v2 = _mm_packs_epi32(v2,v3);
      v1f=sp3[0];                        v2f=sp3[1];
      v1f = _mm_mul_ps(v1f,vec_scale);   v2f = _mm_mul_ps(v2f,vec_scale);
      v3 = _mm_cvtps_epi32(v1f);         v4 = _mm_cvtps_epi32(v2f);
      v3 = _mm_packs_epi32(v3,v4);
      v3f=sp4[0];                        v4f=sp4[1];
      v3f = _mm_mul_ps(v3f,vec_scale);   v4f = _mm_mul_ps(v4f,vec_scale);
      v4 = _mm_cvtps_epi32(v3f);         v5 = _mm_cvtps_epi32(v4f);
      v4 = _mm_packs_epi32(v4,v5);
      
      v1 = _mm_adds_epi16(v1,vec_off);   v2 = _mm_adds_epi16(v2,vec_off);
      v3 = _mm_adds_epi16(v3,vec_off);   v4 = _mm_adds_epi16(v4,vec_off);
      v1 = _mm_packus_epi16(v1,v3);      v2 = _mm_packus_epi16(v2,v4);
      v1 = _mm_min_epu8(v1,vec_max);     v2 = _mm_min_epu8(v2,vec_max);

      v5 = _mm_unpacklo_epi8(v1,v2);  // Interleaves components 1 and 2
      v1 = _mm_unpackhi_epi8(v1,v2);  // Interleaves components 3 and 4
      dqwords[0] = _mm_unpacklo_epi16(v5,v1);
      dqwords[1] = _mm_unpackhi_epi16(v5,v1);
      for (int c=0; c < width; c++) dp[c] = bytes[c];
    }
  
  _mm_setcsr(mxcsr_orig); // Restore rounding control bits
}

/*****************************************************************************/
/* EXTERN                ssse3_int16_to_int16_ilv1                           */
/*****************************************************************************/

void ssse3_int16_to_int16_ilv1(kdu_int16 *dst, kdu_int16 **src, int width,
                               int precision, int orig_precision,
                               bool is_absolute, bool dst_signed)
{
  assert(orig_precision < 16);
    // NB: orig_precision cannot be 16 or the saturating downshift below may
    // cause truncation of valid values.
  if (!is_absolute)
    orig_precision = KDU_FIX_POINT;
  int upshift=0, downshift=orig_precision-precision;
  int offset_val=0; // Offset is applied first, then downshift
  int pre_shift_min = -(1<<(orig_precision-1));
  int pre_shift_max = -(1+pre_shift_min);
  if (downshift <= 0)
    { 
      upshift = -downshift;
      downshift = 0;
    }
  else
    { 
      offset_val = 1 << (downshift-1); // Pre-shift rounding offset
      pre_shift_max -= offset_val;
    }
  if (!dst_signed)
    offset_val += 1 << (orig_precision-1); // Can saturate without overflow

  __m128i vec_off = _mm_set1_epi16((kdu_int16) offset_val);
  __m128i vec_right = _mm_cvtsi32_si128(downshift);
  __m128i vec_left = _mm_cvtsi32_si128(upshift);
  __m128i vec_min = _mm_set1_epi16((kdu_int16) pre_shift_min);
  __m128i vec_max = _mm_set1_epi16((kdu_int16) pre_shift_max);
  __m128i *sp = (__m128i *)(src[0]);
  __m128i v1, v2;
  
  kdu_int16 *dp = dst;
  for (; width >= 16; width-=16, sp+=2, dp+=16)
    { 
      v1=sp[0];                          v2=sp[1];
      v1 = _mm_max_epi16(v1,vec_min);    v2 = _mm_max_epi16(v2,vec_min);
      v1 = _mm_min_epi16(v1,vec_max);    v2 = _mm_min_epi16(v2,vec_max);
      v1 = _mm_adds_epi16(v1,vec_off);   v2 = _mm_adds_epi16(v2,vec_off);
      v1 = _mm_sra_epi16(v1,vec_right);  v2 = _mm_sra_epi16(v2,vec_right);
      v1 = _mm_sll_epi16(v1,vec_left);     v2 = _mm_sll_epi16(v2,vec_left);
      _mm_storeu_si128((__m128i *) dp,v1);
      _mm_storeu_si128((__m128i *)(dp+8),v2);
    }
  if (width > 0)
    { // Take care writing final 1 to 15 output words to avoid buffer overrun
      union { __m128i dqwords[2]; kdu_int16 words[16]; };
      v1=sp[0];                          v2=sp[1];
      v1 = _mm_max_epi16(v1,vec_min);    v2 = _mm_max_epi16(v2,vec_min);
      v1 = _mm_min_epi16(v1,vec_max);    v2 = _mm_min_epi16(v2,vec_max);
      v1 = _mm_adds_epi16(v1,vec_off);   v2 = _mm_adds_epi16(v2,vec_off);
      v1 = _mm_sra_epi16(v1,vec_right);  v2 = _mm_sra_epi16(v2,vec_right);
      dqwords[0] = _mm_sll_epi16(v1,vec_left);
      dqwords[1] = _mm_sll_epi16(v2,vec_left);      
      for (int c=0; c < width; c++) dp[c] = words[c];
    }
}

/*****************************************************************************/
/* EXTERN              ssse3_int32_to_int16_rs_ilv1                          */
/*****************************************************************************/

void ssse3_int32_to_int16_rs_ilv1(kdu_int16 *dst, kdu_int32 **src, int width,
                                  int precision, int orig_precision,
                                  bool is_absolute, bool dst_signed)
{
  assert(is_absolute);
  assert(orig_precision >= precision);
  int downshift = orig_precision - precision;
  int pre_offset_val = (1 << downshift) >> 1;
  int min_val = -(1 << (precision-1));
  int max_val = -(1+min_val);
  int post_offset_val = (dst_signed)?0:(1<<(precision-1));
  
  __m128i vec_preoff = _mm_set1_epi32(pre_offset_val);
  __m128i vec_shift = _mm_cvtsi32_si128(downshift);
  __m128i vec_postoff = _mm_set1_epi16((kdu_int16) post_offset_val);;
  __m128i vec_min = _mm_set1_epi16((kdu_int16) min_val);
  __m128i vec_max = _mm_set1_epi16((kdu_int16) max_val);
  __m128i *sp = (__m128i *)(src[0]);
  __m128i v1, v2, v3, v4;
  
  kdu_int16 *dp = dst;
  for (; width >= 16; width-=16, sp+=4, dp+=16)
    { 
      v1=sp[0];                           v2=sp[1];
      v3=sp[2];                           v4=sp[3];
      v1 = _mm_add_epi32(v1,vec_preoff);  v2 = _mm_add_epi32(v2,vec_preoff);
      v3 = _mm_add_epi32(v3,vec_preoff);  v4 = _mm_add_epi32(v4,vec_preoff);
      v1 = _mm_sra_epi32(v1,vec_shift);   v2 = _mm_sra_epi32(v2,vec_shift);
      v3 = _mm_sra_epi32(v3,vec_shift);   v4 = _mm_sra_epi32(v4,vec_shift);
      v1 = _mm_packs_epi32(v1,v2);        v2 = _mm_packs_epi32(v3,v4);
      v1 = _mm_max_epi16(v1,vec_min);     v2 = _mm_max_epi16(v2,vec_min);
      v1 = _mm_min_epi16(v1,vec_max);     v2 = _mm_min_epi16(v2,vec_max);
      v1 = _mm_add_epi16(v1,vec_postoff); v2 = _mm_add_epi16(v2,vec_postoff);
      _mm_storeu_si128((__m128i *) dp,v1);
      _mm_storeu_si128((__m128i *)(dp+8),v2);
    }
  if (width > 0)
    { // Take care writing final 1 to 15 output words to avoid buffer overrun
      union { __m128i dqwords[2]; kdu_int16 words[16]; };
      v1=sp[0];                           v2=sp[1];
      v3=sp[2];                           v4=sp[3];
      v1 = _mm_add_epi32(v1,vec_preoff);  v2 = _mm_add_epi32(v2,vec_preoff);
      v3 = _mm_add_epi32(v3,vec_preoff);  v4 = _mm_add_epi32(v4,vec_preoff);
      v1 = _mm_sra_epi32(v1,vec_shift);   v2 = _mm_sra_epi32(v2,vec_shift);
      v3 = _mm_sra_epi32(v3,vec_shift);   v4 = _mm_sra_epi32(v4,vec_shift);
      v1 = _mm_packs_epi32(v1,v2);        v2 = _mm_packs_epi32(v3,v4);
      v1 = _mm_max_epi16(v1,vec_min);     v2 = _mm_max_epi16(v2,vec_min);
      v1 = _mm_min_epi16(v1,vec_max);     v2 = _mm_min_epi16(v2,vec_max);
      dqwords[0] = _mm_add_epi16(v1,vec_postoff);
      dqwords[1] = _mm_add_epi16(v2,vec_postoff);
      for (int c=0; c < width; c++) dp[c] = words[c];
    }
}

/*****************************************************************************/
/* EXTERN                ssse3_floats_to_int16_ilv1                          */
/*****************************************************************************/

void ssse3_floats_to_int16_ilv1(kdu_int16 *dst, float **src, int width,
                                int precision, int orig_precision,
                                bool is_absolute, bool dst_signed)
{
  assert(!is_absolute);
  float scale = (float)(1<<precision);
  int min_val = -(1 << (precision-1));
  int max_val = -(1+min_val);
  int post_offset_val = (dst_signed)?0:(1<<(precision-1));
  int mxcsr_orig = _mm_getcsr();
  int mxcsr_cur = mxcsr_orig & ~(3<<13); // Reset rounding control bits
  _mm_setcsr(mxcsr_cur);
  
  __m128 vec_scale = _mm_set1_ps(scale);
  __m128i vec_postoff = _mm_set1_epi16((kdu_int16) post_offset_val);;
  __m128i vec_min = _mm_set1_epi16((kdu_int16) min_val);
  __m128i vec_max = _mm_set1_epi16((kdu_int16) max_val);
  __m128 *sp = (__m128 *)(src[0]);
  __m128 v1f, v2f, v3f, v4f;
  __m128i v1, v2, v3, v4;
  
  kdu_int16 *dp = dst;
  for (; width >= 0; width-=16, sp+=4, dp+=16)
    { 
      v1f=sp[0];                          v2f=sp[1];
      v3f=sp[2];                          v4f=sp[3];
      v1f = _mm_mul_ps(v1f,vec_scale);    v2f = _mm_mul_ps(v2f,vec_scale);
      v3f = _mm_mul_ps(v3f,vec_scale);    v4f = _mm_mul_ps(v4f,vec_scale);
      v1 = _mm_cvtps_epi32(v1f);          v2 = _mm_cvtps_epi32(v2f);
      v3 = _mm_cvtps_epi32(v3f);          v4 = _mm_cvtps_epi32(v4f);
      v1 = _mm_packs_epi32(v1,v2);        v2 = _mm_packs_epi32(v3,v4);
      v1 = _mm_max_epi16(v1,vec_min);     v2 = _mm_max_epi16(v2,vec_min);
      v1 = _mm_min_epi16(v1,vec_max);     v2 = _mm_min_epi16(v2,vec_max);
      v1 = _mm_add_epi16(v1,vec_postoff); v2 = _mm_add_epi16(v2,vec_postoff);
      _mm_storeu_si128((__m128i *) dp,v1);
      _mm_storeu_si128((__m128i *)(dp+8),v2);
    }
  if (width > 0)
    { // Take care writing final 1 to 15 output words to avoid buffer overrun
      union { __m128i dqwords[2]; kdu_int16 words[16]; };
      v1f=sp[0];                          v2f=sp[1];
      v3f=sp[2];                          v4f=sp[3];
      v1f = _mm_mul_ps(v1f,vec_scale);    v2f = _mm_mul_ps(v2f,vec_scale);
      v3f = _mm_mul_ps(v3f,vec_scale);    v4f = _mm_mul_ps(v4f,vec_scale);
      v1 = _mm_cvtps_epi32(v1f);          v2 = _mm_cvtps_epi32(v2f);
      v3 = _mm_cvtps_epi32(v3f);          v4 = _mm_cvtps_epi32(v4f);
      v1 = _mm_packs_epi32(v1,v2);        v2 = _mm_packs_epi32(v3,v4);
      v1 = _mm_max_epi16(v1,vec_min);     v2 = _mm_max_epi16(v2,vec_min);
      v1 = _mm_min_epi16(v1,vec_max);     v2 = _mm_min_epi16(v2,vec_max);
      dqwords[0] = _mm_add_epi16(v1,vec_postoff);
      dqwords[1] = _mm_add_epi16(v2,vec_postoff);
      for (int c=0; c < width; c++) dp[c] = words[c];
    }
  
  _mm_setcsr(mxcsr_orig); // Restore rounding control bits
}

/*****************************************************************************/
/* EXTERN                ssse3_floats_to_floats_ilv1                         */
/*****************************************************************************/

void ssse3_floats_to_floats_ilv1(float *dst, float **src, int width,
                                 int precision, int orig_precision,
                                 bool is_absolute, bool dst_signed)
{
  assert(!is_absolute);
  float scale = 1.0F; // Amount to scale from unit range to dst
  while (precision < 0)
    { precision += 16; scale *= 1.0F/(float)(1<<16); }
  while (precision > 16)
    { precision -= 16; scale *= (float)(1<<16); }
  scale *= (float)(1<<precision);
  float offset = (dst_signed)?0.0F:(0.5F*scale);

  __m128 vec_scale = _mm_set1_ps(scale);
  __m128 vec_off = _mm_set1_ps(offset);
  __m128 v1f, v2f, v3f, v4f;
  __m128 *sp = (__m128 *)(src[0]);
  
  float *dp = dst;
  for (; width >= 16; width-=16, sp+=4, dp+=16)
    { 
      v1f=sp[0];                          v2f=sp[1];
      v3f=sp[2];                          v4f=sp[3];
      v1f = _mm_mul_ps(v1f,vec_scale);    v2f = _mm_mul_ps(v2f,vec_scale);
      v3f = _mm_mul_ps(v3f,vec_scale);    v4f = _mm_mul_ps(v4f,vec_scale);
      v1f = _mm_add_ps(v1f,vec_off);      v2f = _mm_add_ps(v2f,vec_off);
      v3f = _mm_add_ps(v3f,vec_off);      v4f = _mm_add_ps(v4f,vec_off);
      _mm_storeu_ps(dp,v1f);          _mm_storeu_ps((dp+4),v2f);
      _mm_storeu_ps((dp+8),v3f);      _mm_storeu_ps((dp+12),v4f);
    }
  if (width > 0)
    { // Take care writing final 1 to 15 output floats to avoid buffer overrun
      union { __m128 dqwords[4]; float floats[16]; };
      v1f=sp[0];                          v2f=sp[1];
      v3f=sp[2];                          v4f=sp[3];
      v1f = _mm_mul_ps(v1f,vec_scale);    v2f = _mm_mul_ps(v2f,vec_scale);
      v3f = _mm_mul_ps(v3f,vec_scale);    v4f = _mm_mul_ps(v4f,vec_scale);
      dqwords[0] = _mm_add_ps(v1f,vec_off);
      dqwords[1] = _mm_add_ps(v2f,vec_off);
      dqwords[2] = _mm_add_ps(v3f,vec_off);
      dqwords[3] = _mm_add_ps(v4f,vec_off);
      for (int c=0; c < width; c++) dp[c] = floats[c];
    }
}

#endif // !KDU_NO_SSSE3

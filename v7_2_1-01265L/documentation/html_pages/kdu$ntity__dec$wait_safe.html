<HTML>
<HEAD>
<TITLE> Kakadu Hyper-Doc (kdu_thread_entity::declare_first_owner_wait_safe) </TITLE>
<STYLE TYPE="text/css">
<!--
.indented-text { padding-left: 20pt; }
.function-text { padding-left: 80pt;
                 text-indent: -60pt; }
-->
</STYLE>
</HEAD>
<BODY TEXT="#000000" LINK="#0000ff" BGCOLOR="#B4DCB4">
<P ALIGN="RIGHT">|<A HREF="#SynopsiS"> synopsis </A>||<A HREF="#ReturnS"> return value </A>||<A HREF="#ArgS"> arguments </A>||<A HREF="kdu$ntity__add_thread.html"> prev </A>||<A HREF="kdu$ntity__set$frequency.html"> next </A>|</P>
<H1><A NAME="ToP">kdu_thread_entity::declare_first_owner_wait_safe</A></H1>
<DIR><DIR><H3><EM><U>Java:</U> Kdu_thread_entity.Declare_first_owner_wait_safe</EM></H3></DIR></DIR>
<P CLASS="function-text"><B>bool</B> declare_first_owner_wait_safe(
<B>bool</B>&nbsp;is_safe)</P>
<DIR><DIR><P CLASS="function-text"><EM><U>Java:</U> public native boolean Declare_first_owner_wait_safe(
boolean&nbsp;is_safe)</EM></P></DIR></DIR>
<P>[Declared in <A HREF="++++coresy$+kdu_threads+h.html">"../coresys/common/kdu_threads.h"</A>]</P><P><A HREF="kdu$ntity.html">Go to class description.</A></P>

<P ALIGN="CENTER"><HR></P><H2><A NAME="SynopsiS">Synopsis</A></H2>
<DIV CLASS="indented-text">
<P>
This function needs to be understood in conjunction
with the 
<B>queue_flags</B> argument to 
<A HREF="kdu$ntity__attach_queue.html">attach_queue</A>. In brief, Kakadu's multi-threading sub-system has
the notion a "working wait" state, where a thread calls
into 
<A HREF="kdu$ntity__wai$condition.html">wait_for_condition</A>, 
<A HREF="kdu$ntity__join.html">join</A> or 
<A HREF="kdu$ntity__terminate.html">terminate</A> to wait for a condition, but while it is waiting the
thread actually executes outstanding jobs that have
been scheduled through any of the attached 
<A HREF="kdu$2queue.html">kdu_thread_queue</A>s. When a queue is attached, the supplied 
<B>queue_flags</B> argument can be used to constrain the contexts in
which its jobs are run. In particular, some jobs should
not be run by threads that are in a working wait state,
because these jobs might wind up waiting on a condition
that is blocked by the working wait state.
</P><P>
 This function determines how the thread group owner
(see 
<A HREF="kdu$ntity__is_$oup_owner.html">is_group_owner</A>) thread should be treated when it first enters a working
wait state. Unlike worker threads, the group owner
can only execute jobs from within the working wait
state, so its resources may be underutilized if it
cannot perform certain types of jobs. Moreover, unlike
worker threads, when the group owner first enters a
working wait state, the point at which it does so is
well defined by the program flow. In many cases, therefore,
it can be determined that the group owner's first entry
to a working wait state is a safe context for running
jobs that must otherwise be run only from non-waiting
worker threads.
</P><P>
 Specifically, passing 
<A HREF="kdu$ntity__dec$wait_safe.html#is_safe">is_safe</A>=true to this function will cause the group owner to
be elligible for running jobs that have been scheduled
from within a queue attached with the 
<A HREF="globals.html#KDU_THREAD_QUEUE_SAFE_CONTEXT">KDU_THREAD_QUEUE_SAFE_CONTEXT</A> flag, so long as the group owner has not recursively
entered the working wait state. That is, to be elligible
to run jobs that require a safe context, the group
owner may be inside 
<A HREF="kdu$ntity__join.html">join</A>, 
<A HREF="kdu$ntity__terminate.html">terminate</A> or 
<A HREF="kdu$ntity__wai$condition.html">wait_for_condition</A>, so long as it has not entered these functions from
within a job that it is already executing in a working
wait state.
</P><P>
 Before concluding this description, it is worth providing
some concrete examples of where this function should
or should not be used. Kakadu's multi-threading sub-system
is used extensively for parallel realization of block
coding and DWT analysis/synthesis operations. Usually,
this is done by instantiating 
<A HREF="kdu$1lysis.html">kdu_multi_analysis</A> or 
<A HREF="kdu$hesis.html">kdu_multi_synthesis</A> objects and then pushing (analysis) or pulling (synthesis)
image data to/from these objects from a single thread
&mdash; almost always the group owner of a multi-threaded
processing group whose workers participate in the underlying
processing. This model is very nice because the push/pull
management thread sees a single-threaded API and does
not have to worry about synchronization issues, except
that it must 
<A HREF="kdu$ntity__join.html">join</A> or 
<A HREF="kdu$ntity__terminate.html">terminate</A> on instantiated thread queues before cleaning them
up. What actually happens is that the push/pull thread
encounters conditions which might block its progress
internally and enters the working wait state, participating
as a processor of scheduled jobs, until it can proceed
with what appears to be a single threaded processing
model. In this case, it is NOT GENERALLY SAFE for the
group owner to be considered elligible to run custom
jobs that the require a safe context (these are jobs
that the application is deploying separately). The
reasons for this are explained with the 
<A HREF="kdu$ntity__attach_queue.html">attach_queue</A> function.
</P><P>
 However, Kakadu's multi-threading sub-system provides
an alternate, potentially more efficient approach for
using the multi-threaded data processing machinery
that is accessible using objects like 
<A HREF="kdu$1lysis.html">kdu_multi_analysis</A>, 
<A HREF="kdu$hesis.html">kdu_multi_synthesis</A> and the lower level objects 
<A HREF="kdu$lysis.html">kdu_analysis</A>, 
<A HREF="kdu$1hesis.html">kdu_synthesis</A>, 
<A HREF="kdu$3coder.html">kdu_encoder</A> and 
<A HREF="kdu$2coder.html">kdu_decoder</A>. In this alternate approach, the push/pull data processing
calls are themselves executed from a suitable scheduled
job, that runs only until the data processing objects
(typically 
<A HREF="kdu$1lysis.html">kdu_multi_analysis</A> or 
<A HREF="kdu$hesis.html">kdu_multi_synthesis</A>) report that further push/pull calls may encounter
blocking conditions, forcing the caller to enter a
working wait. At that point the running push/pull job
finishes and another one is automatically scheduled
once the potentially blocking conditions have been
cleared. This execution model has the potential to
be more efficient for platforms with a large number
of CPU's because it allows the push/pull functionality
to resume as soon as the blocking condition is cleared
and a thread (any thread) is available to do work.
By contrast, the simpler approach described in the
previous paragraph requires a specific thread (typically
the group owner) to perform all push/pull calls, so
if it is temporarily blocked and goes away to do other
work, it cannot resume the push/pull operations until
this other work is finished. This may cause the push/pull
activity to be suspended so long that the complete
multi-processing system stalls, causing a large number
of thread context switches and under-utilization of
available processing resources. In this more efficient
processing paradigm, the group owner typically spends
all of its time in a working wait state, waiting for
the actual processing to be performed by other jobs
&mdash; usually, the group owner sets things up and
then invokes 
<A HREF="kdu$ntity__join.html">join</A> to wait until everything is done. In this case, the
group owner's working wait does not present a dependency
for any other job in the system, so it is always safe
for the group owner to participate in any kind of work
from within its outer-most working wait. For this reason,
applications that are based on this paradigm, should
generally use the present function first to declare
the group owner as elligible to run any kind of job
from its outer-most working wait state.
</P><P>
 For further guidance on how to implement the second
processing paradigm described in the above paragraph,
the reader is referred to the 
<A HREF="kdu$1queue.html">kdu_run_queue</A> object, which wraps up most of the functionality required
to run what would normally be a single-threaded push/pull
processing pipeline from within a scheduled job that
runs until it would potentially block and is automatically
rescheduled once the blocking conditions go away. Whether
or not this processing paradigm actually improves processing
throughput depends on many things, and one should note
that frequently migrating the top-level push/pull processing
calls between threads could actually have a negative
impact on cache utilization.
</P><P>
 Note that this present function may only be invoked
from the group owner thread itself.
</P>
</DIV>

<P ALIGN="CENTER"><HR></P><H2><A NAME="ReturnS">Return Value</A></H2><DIV CLASS="indented-text">
<P>
The value passed for 
<A HREF="kdu$ntity__dec$wait_safe.html#is_safe">is_safe</A> within a previous call to this function, if any, or
else false. The default condition for a thread group
on which this function has never been called, is that
the group owner thread does not offer a safe context
for running sensitive jobs.
</P>
</DIV>
<H2><A NAME="ArgS">Arguments</A></H2><DIV CLASS="indented-text">
<H4><A NAME="is_safe">is_safe</A> [<B>bool</B>]</H4><DIV CLASS="indented-text">
<P>
If true, the group owner is declared to be elligible
to run jobs that require a safe context, so long as
it is within an outer-most call to 
<A HREF="kdu$ntity__join.html">join</A>, 
<A HREF="kdu$ntity__terminate.html">terminate</A> or 
<A HREF="kdu$ntity__wai$condition.html">wait_for_condition</A>, not having re-entered a working wait state from within
a scheduled job.
</P><P>
 If false, the group owner's elligibility to run such
jobs is revoked. This is the default condition, in
case this function is never called.
</P>
</DIV>
</DIV>

<P ALIGN="CENTER"><HR></P><P ALIGN="RIGHT">|<A HREF="#ToP"> top </A>||<A HREF="#SynopsiS"> synopsis </A>||<A HREF="#ReturnS"> return value </A>||<A HREF="#ArgS"> arguments </A>||<A HREF="kdu$ntity__add_thread.html"> prev </A>||<A HREF="kdu$ntity__set$frequency.html"> next </A>|</P>
</BODY>
</HTML>

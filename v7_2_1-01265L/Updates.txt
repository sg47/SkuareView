Changes from KDU-7.2 to KDU-7.2.1
---------------------------------
-- Rationalized the "makefiles" for non-MSVC/non-Xcode builds, adding
   a complete set of makefiles for 64-bit Windows builds under MINGW64
   (Minimal GNU for Windows) toolchain.
-- Added SIMD processing speedups to the front-end data format conversion
   logic found in `kdu_stripe_compressor' and `kdu_stripe_decompressor'
   so that conversion between Kakadu's four native formats and the
   application's preferred format does not represent a bottleneck for
   performance, where developers choose to use these simplified API's.
   The documentation for these API's has also been expanded somewhat to
   identify what buffer configurations are likely to be handled most
   efficiently.
-- Extended the "kdu_buffered_compress" and "kdu_buffered_expand" demo
   apps to work with PPM and BMP files (1, 3 or 4 bytes/pixel) as well as
   PGM and raw files, adding other features such as the ability to
   control internal processing precision via "-precise" and "-fastest"
   options.
-- Extended the "kdu_buffered_compress" application to allow an image to
   be buffered in memory and automatically replicated during compression,
   so that the performance of Kakadu on very large images can be tested
   without dependence on the file system; on powerful platforms, Kakadu can
   compress data at much higher rates than can be delivered by a typical
   disk or SSD-based file system.  Also added the option for
   "kdu_buffered_compress" to send its output to a null "structured cache"
   data target if the "-o" option is omitted (as done by "kdu_compress").
-- "kdu_buffered_compress" has also been augmented with a "-stats"
   option to report compression stats (working memory, achieved bit-rate,
   distortion-length slope threshold).  Moreover, it has been modified
   to use the same set of visual weights as "kdu_compress" by default, with
   a "-no_weights" option to turn this default weighting policy off.
   This ensures that both compression applications behave the same way,
   to avoid any confusion that might otherwise be caused.
-- All of the compression and decompression demo apps (kdu_compress/expand,
   kdu_buffered_compress/expand, kdu_v_compress/expand and kdu_vex_fast)
   have had the default policy associated with their "-double_buffering"
   argument modified so that the default is now to pass the value -1 to
   `kdu_multi_analysis::create'/`kdu_multi_synthesis::create' (possibly
   via higher level API's) for its `buffer_rows' argument.  The
   `kdu_multi_analysis::create' and `kdu_multi_synthesis::create' functions
   now interpret a -ve `buffer_rows' argument as a request for the
   stripe buffers that interface the multi-component transform machinery
   with the spatial DWT machinery to be automatically allocated a suitable
   height, taking into account the width of the stripes to be processed --
   this usually leads to a better trade-off between cache demands and
   thread scheduling flexibility than can be achieved by assigning a fixed
   buffer height for all applications.  Of course, you can still explicitly
   dimension the buffers, as before.
-- Fixed a problem with the "kdu_client" implementation that caused
   requests for a large number of new JPIP channels to be unduly
   delayed; moreover, in some circumstances, this problem may have
   manifested itself in slow transport over HTTP-TCP and HTTP-UDP
   transports, because the creation of new JPIP channels sometimes
   left existing channels with an artificially enlarged understanding
   of their number of outstanding requests.
-- Split the "kdu_elementary.h" core system file into two parts, with the
   most elementary part, "kdu_ubiquitous.h", free from class and in-line
   function definitions.  This has no effect upon applications, which should
   continue to include "kdu_elementary.h" as and when they did before; however
   it does improve the structure of some of the Kakadu internals, especially
   where AVX optimizations are incorporated.
-- Modified the XCODE project files and MAC-xxx makefiles so as to use
   the "clang++" compiler in place of "g++" and to build AVX optimizations
   where appropriate.  This was previously not possible on MAC platforms,
   for which the highest GCC version was 4.2.
-- Fixed a minor error in "kdu_winshow" that caused it to open MJ2 files
   initially in single component mode, as opposed to full colour composited
   frame mode.
-- Fixed a subtle yet important problem in the distortion estimation
   algorithm used by the block encoder that could, under extremely rare
   conditions, cause code-block bit-streams to be excessively truncated,
   resulting in unexpectedly large distortion reconstructed image distortion.
   As it turns out, this problem has existed in all previous versions of
   Kakadu, but not in any of the speed-pack releases of Kakadu.  The
   problem affects lossy compression only (not lossless compression).
-- Made a very minor change to the one of the atomic intrinsics in
   "kdu_elementary.h" for Win64 builds so that 64-bit versions of the
   Managed library ("kdu_mni.dll") can be built despite a latent
   bug in the VS2008 and VS2010 compilers -- but was apparently fixed
   in VS2012.
-- Fixed a minor problem in "kdu_macshow" that could result in tiny
   cracks appearing during image navigation when rendering to a retina
   display: solution was simply to extend the region invalidated by
   new imagery rendered in `on_idle' by one pixel all around so that
   the GPU scaling machinery has sufficient data to work with.
-- Fixed a subtle bug in the UDP transport logic within	 `kdu_client'.
-- Fixed a minor bug in the auxiliary rate calculation in "kdu_server" and
   also modified the `estimated_network_usecs_per_byte' member to use
   a floating point representation, rather than an integral type.
-- Removed a typo from the core system, where "KDU_NO_SSSE3" was
   accidentally entered as "KDU_NO_SSSE3n" in one place -- relevant only
   if building in environments where SSSE3 instructions are not available.
-- Made minor corrections to the documentation in "Usage_Examples.txt".

Changes from KDU-7.1 to KDU-7.2
-------------------------------
1) Introduced a new job queue design at the heart of the core multi-threaded
   sub-system that involves an absolute minimum of bus locking operations
   to enqueue and dequeue processing jobs.  There are absolutely no
   spin locks or critical section locks associated with the distributed
   job enqueueing and dequeueing processes -- mutexes are employed only
   for macroscopic operations such as the creation and destruction of
   job queues.
   -- NB: There has been a small change in the `kdu_thread_queue::bind_jobs'
      and `kdu_thread_queue::schedule_jobs' functions in that these functions
      now take arrays of `kdu_thread_job' pointers, whereas previously they
      took arrays of `kdu_thread_job_ref' pointers.  The additional level
      of indirection through opaque pointers has been removed, for a variety
      of reasons.  This is unlikely to impact any third party applications,
      but any required changes should be readily apparent if existing apps
      fail to compile.
2) Added comprehensive support for what we call "JPX containers"
   (these are the new Compositing Layer Extensions boxes introduced by
   IS15444-2/AMD3), as well as JPX files with aggregated codestreams
   (Multiple Codestream boxes).  The new `jpx_source' and `jpx_target'
   implementations correctly generate and interpret metadata embedded
   within JPX containers, which is by no means simple but can be used
   in very powerful ways.  As a result, Kakadu and its demo applications
   now support multiple JPX presentation threads (or tracks), each with
   its own animation stream.  It means that you can effectively build
   metadata that references whole presentation tracks or portions of them
   in an efficient way.  It also means that Kakadu can very efficiently
   encapsulate video (including live video -- i.e. where the sequence
   length is unknown a priori or unbounded) within a JPX file and access
   it in a random or streaming fashion via JPIP, without the linearly
   growing random access overhead that was previously incurred by using
   JPX files to store video -- prior to these innovations, all codestreams
   and their header boxes needed to live at the top level of a JPX file so
   that a massive number of placeholder boxes (each ~28 bytes) might need
   to be sent to a remote client as minimal representation for the codestreams
   preceding one that might be of interest.
      These innovations make JPX files an excellent choice for embedding
   video, especially where auxiliary navigation or annotation
   metadata is to be embedded as well, or for advanced applications such as
   hyperspectral video using multi-component transforms for both efficient
   compression and to enable a diversity of presentation modes (via
   presentation threads).  For example, a multi-component transform may
   provide numerous output components that are synthesized from the source
   content through custom transformation steps.  JPX compositing layers may
   be defined to present these output components in interesting ways and
   these rather complex descriptions can be embedded within JPX containers
   so that they self-replicate to describe video content with any number
   of distinct presentation threads.
3) Greatly enhanced the JPX metadata and animation management/discovery
   features offered by Kakadu.  This has been done largely in response to
   the new features implied by JPX containers (Compositing Layer Extensions
   boxes).  You can now: count tracks; count frames in a track; count
   temporal duration of tracks or of the frames in a track that precede
   one that is of interest; and search for composited frames that match
   number list descriptions, search forwards and backwards.
   These features are very efficient and are designed to work with content
   that may contain millions of frames and numerous presentation tracks.
4) Extended the JPX file writing capabilities of Kakadu.  Specifically,
   it is no longer necessary to write all of the auxiliary metadata via
   a single call to `jpx_target::write_metadata'.  You can now interleave
   metadata and imagery writing and you can add metadata on the fly as
   content becomes available.  Moreover, all of this is done while being
   careful to avoid polluting the top-level of the file with a huge number
   of boxes -- that would interfere with efficient random access and
   interactive JPIP browsing.
5) Extended the "kdu_server" demo application to correctly serve JPX
   files that use containers (Compositing Layer Extensions boxes) and
   Multiple Codestream boxes.
   -- Note 1: this means that JPIP works correctly with such files, which
      can greatly reduce the amount of file format metadata that needs to
      be communicated when interactively browsing a source with a large
      number of compositing layers and codestreams.
6) Upgraded the "kdu_merge" demo app to support generation of JPX files
   with containers -- this was done largely for testing purposes, but it
   allows some very interesting configurations to be synthesized using
   command-line options (admittedly not trivial to get your head around).
7) Upgraded the "kdu_show" (windows and Mac versions) demo app to support
   editing, interpretation and navigation for metadata embedded within
   JPX containers.  The "kdu_show" application now also supports the
   playback of muliple presentation tracks (a.k.a. threads) induced by
   JPX containers and the synthesizing of metadata-driven animations that
   take advantage of all of the features of JPX containers.  For example,
   holding the shift key down while double-clicking on a metadata entry
   that is embedded within a JPX container will synthesize a metadata-driven
   animation that plays out all the associated content from the container,
   taking repetitions of the container into account.  These capabilities are
   all realized using the underlying powerful API's, rather than being
   specific to the "kdu_show" demo app itself.
8) Upgraded the "kdu_v_compress" demo app to support writing of video directly
   to a JPX file, along with metadata, taking advantage of JPX containers.
   This demo app now supports four compressed video formats: MJ2, JPX, JPB
   (broadcast streams) and MJC (raw) video streams.  This demo is the best
   platform on which to build live video compression applications.  If you
   want to use Part-2 codestream features for your video or embed rich
   auxiliary metadata, or facilitate interactive access via JPIP, JPX is by
   far the best choice, except that there is not currently any support for
   audio in the JPX file format.
9) Upgraded the "kdu_v_expand" demo app to support JPX input files,
   processing the first codestream in each animation frame -- of course JPX
   animations are not as simple as regular video, potentially involving many
   different codestreams, composited in interesting ways, but this upgrade
   allows "kdu_v_expand" to decompress content written to a JPX file by
   "kdu_v_compress", which also means that video constructed using advanced
   Part-2 codestream features, such as multi-component transforms, or
   arbitrary wavelet kernels, can be passed through "kdu_v_expand".
10) Upgraded the "kdu_vex_fast" demo app to support JPX input files, handling
    them in exactly the same way as "kdu_v_expand".
11) Substantially reduced the number of file handles and synchronization
    objects required by the "kdu_server" application, especially when serving
    source files that contain a large number of embedded codestreams.
12) Augmented `kdu_region_animator' with the ability to calculate and
    issue its own window-of-interest requests to a `kdu_client' interface
    and to analyze the current status of those requests.  This is achieved
    primarily through two functions, `generate_imagery_requests' and
    `notify_client_progress', that are used by an application interested
    in presenting animated content that resides on a remote server.  The
    `kdu_region_animator' object provides all the state information and
    machinery to manage the timing and request/response management for
    JPIP video and other more ellaborate animations, including
    complex composited animations and metadata-driven animations.  These
    new capabilities are demonstrated by the "kdu_macshow" and "kdu_winshow"
    demo applications, whose animation and JPIP-related auto-refresh logic
    has also been rationalized and simplified.  In other words, JPIP video
    is now here!
13) To support JPIP video/animation, the `kdu_client' interface has been
    augmented slightly to support application-defined `custom_id' values
    that can be used to tag and keep track of the status of posted
    windows of interest, along with a `get_window_info' function that
    provides enhanced status information about posted windows of interest,
    including an indication of the amount of "service time" that a given
    window of interest may have received from the server.  The
    `post_window' function has also been augmented with the ability to
    associate a `service_time' with a window of interest, allowing the
    application to post a sequence of so-called "timed requests".
       Three new member functions, `sync_timing', `get_timed_request_horizon'
    and `trim_timed_requests', have been added to simplify the process of
    determining when and how timed requests should be issued during an
    interactive video/animation, so as to maximize the responsiveness of
    the experience.
       Exactly how timed requests are handled by the `kdu_client'
    implementation should be irrelevant to the application, but the current
    approach is based on the issuing of requests that specify a byte limit
    ("len" request field), that is based on estimates of the channel
    statistics and subject to a control loop that aims to achieve actual
    cumulative service times that match the cumulative requested service
    times within a sequence of timed requests.  Normally, the "len" field
    is used only for requests over HTTP-only transports, but for timed
    requests, the client uses the "len" field also with the HTTP-TCP and
    HTTP-UDP transports, although the division of responsibilities between
    client and server for the maintainance of channel responsiveness is
    different in these transports.  Timed requests also work in the
    stateless communication mode, although it is fundamentally less efficient
    than the stateful modes.
       In the next release, the `kdu_client' implementation will also be able
    to use the "twait" (timed-wait) request field that is part of
    Amendment 5 of the JPIP standard -- this allows the server to manage
    service time distribution for timed requests, which is fundamentally
    more flexible and efficient than the byte limit approach.
       In the next release, the `kdu_client' implementation will embody
    several other important efficiency improvements for JPIP video,
    including "jpxf" context requests (JPX-frame requests), the "mset"
    (model-set) request field for cache model management with indefinite
    content streams, and most likely some new features which are proposed
    for Amendment 6 of the JPIP standard.
14) Extended the core `kdu_compressed_target' class to support the writing
    of structured codestream elements to a cache, as opposed to sequentially
    writing a conventional codestream.  This is part of a long standing plan
    to realize fully integrated live encoding, rendering and
    JPIP services, all brokered by a real-time compressed data cache.
    Prior to this point, compressed data caches were used only by `kdu_client'
    to allow live rendering of data sent asynchronously by a JPIP server.
    Currently, we do not provide any serious implementations of the
    caching compressed data target paradigm, but the "kdu_compress" demo app
    offers the option to send all of its compressed data to a "null target" --
    a caching compressed data target that discards everything.  Other, more
    interesting compressed data targets that support structured writing can
    readily be built by third parties, on top of the stable interfaces that
    have now been established.
       At present, the most important benefit that can be derived from
    `kdu_compressed_target' targets that offer structured writing to a cache
    is that incremental codestream flushing is much more flexible and efficient
    for such targets than it is for linear codestream targets.  For a
    structured cache target, incremental flushing (see, e.g., the
    `-flush_period' argument to "kdu_compress") does not require the lower
    resolution components of the image to have very small precincts.  This
    means that huge untiled images can be compressed and incrementally
    flushed to a structured cache target, regardless of how many resolution
    levels (`Clevels') are defined, so long as you define modest sized
    precincts (e.g., `Cprecincts={128,128}').  See the extensive documentation
    of the `kdu_codestream::flush' function for more on the merits of
    incremental flushing with structured cache targets.
15) Considerable effort has been invested in reducing the number of simulation
    passes required by Kakadu's rate control machinery, since these
    simulation passes are inherently single-threaded so that they can
    become a bottleneck on machines with many CPU cores that can be
    effectively exploited by the rest of the compression machinery.  These
    enhancements include:
    A) better prediction of the distortion-length slope thresholds that are
       likely to be most suitable;
    B) a new  algorithm is now used to synthesize quality layers whose target
       lengths are not explicitly provided (when the target lengths array
       passed to `kdu_codestream::flush' or `kdu_codestream::auto_flush' has
       some zero-valued entries);
    C) when working with slope-based rate-control, where distortion-length
       slope thresholds are provided explicitly, redundant simulation passes
       are removed and in fact no simulation passes are required at all if
       the `kdu_compressed_target'-derived compressed data target supports
       structured writes to a cache; and
    D) an additional option has been added to the `flags' argument to
       `kdu_codestream::flush' and `kdu_codestream::auto_flush', allowing
       the caller to identify supplied distortion-length thresholds as
       "hints" to be used to accelerate the rate control algorithm used
       to achieve layer target sizes.
    All of the Kakadu demo apps that perform compression ("kdu_compress",
    "kdu_buffered_compress" and "kdu_v_compress") now provide a "-tolerance"
    argument that allows the accuracy of the rate control process to be
    specified and the default tolerance is now set to 2% (as opposed to 0%).
    Moreover, the "kdu_v_compress" algorithm now uses the new "threshold
    hinting" option (item D above), to pass the distortion-length slope
    thresholds determined for one frame as hints to the rate control
    algorithm for a subsequent frame.
       Together, these changes mean that the overall complexity of the rate
    control process is substantially reduced (perhaps by 10x or more, but
    typically by at least 3-5x, depending on the number of quality layers of
    interest and how they are specified).
       The modified algorithm for determining quality layers whose sizes are
    not specified in the `layer_bytes' array passed to `kdu_codestream::flush'
    and `kdu_codestream::auto_flush' (item B above) does, however, mean that
    those quality layers are not affected by any `Creslengths' attribute --
    this is unlikely to be a problem in most applications, because
    `Creslengths' is not normally used with the target size driven rate
    control mode, except where layer sizes are all specified explicitly.
16) To support the enforcing of new High Frame Rate (HFR) Digital Cinema
    profile specifications, the `Creslengths' attribute has been supplemented
    with a companion `Cagglengths' attribute that can be used to specify
    components whose compressed data tallies should be aggregated,
    subjecting the aggregated totals to specific `Creslengths' constraints.
    The primary intent is to allow constraints to be specified for the
    combined bit-rate of the chrominance channels, rather than applying
    separate constraints to each individual chrominance channel.  However,
    the `Cagglengths' feature provides much more general support for
    setting up aggregated component sets, on a resolution-specific
    (even tile-resolution specific) basis.  See "Usage_Examples.txt" for
    one example of how to set up aggregated component length constraints.
17) Introduced a very tiny change to the way in which distortion is
    calculated during block encoding, which serves two purposes:
    1) the new calculation is potentially slightly more accurate; and
    2) the new calculation avoids the possibility that a truncated
       code-block bit-stream can appear to have zero distortion.
    Previously, this latter possibility existed in certain highly
    pathalogical cases, which could encourage agressive truncation of
    code-block bit-streams to the point where third party JPEG2000 decoders
    that might use a different dequantization rule to Kakadu (dequantization
    is not formally specified by the standard) could potentially render an
    image with much larger distortion.  This change means that the
    distortion-length slope thresholds associated with a given compressed
    bit-rate are very slightly different between KDU7.1 and KDU7.2, but
    not enough to warrant any significant attention.  For a fixed
    distortion-length slope threshold, the compressed bit-rate associated
    with KDU7.2 may differ by perhaps 1% or less from that generated with
    the same fixed distortion-length slope threshold in KDU7.1.
18) Augmented the `mj2_video_target' class to implement the `start_rewrite'
    and `end_rewrite' interfaces advertised by `kdu_compressed_target'.
    Without this, attempts to compress to a DCINEMA profile or any codestream
    that requires TLM markers will fail to write the TLM data, generating a
    warning that the rewrite functionality is missing.
19) Augmented the TIFF file reading and writing logic associated with the
    classes defined by "image_in.h" and "image_out.h" (used only by the
    demo apps "kdu_compress" and "kdu_expand") with the ability to preserve
    ICC profiles within TIFF files -- a simple feature that has always been
    supported by Kakadu's JP2/JPX file format reading/writing API's, but
    whose omission from the demo apps was a cause of concern for some
    Kakadu users.
20) Added a new function, `kdu_input_box::close_without_checking' that is
    now called in place of `kdu_input_box::close' by the `kdu_input_box'
    destructor.  The new function itself calls `close', ensuring that
    derived objects continue to behave as expected; however, the new function
    avoids a potentially regrettable call to `jp2_input_box::is_complete'
    that might access an underlying `jp2_family_src' object whose existence
    cannot be relied upon, especially during exception handling.

Bug Fixes:
A) Fixed a bug in the generation of Java and C#/VB language bindings by
   "kdu_hyperdoc" regarding the order in which temporary marshalling
   arrays are allocated and deleted; the main (perhaps only) current API
   function affected is `kdu_client::check_compatible_url'.
B) Corrected a bug in `kdu_stripe_compressor' in the conversion of unsigned
   16-bit words to internal 16-bit samples for `kdu_multi_analysis'.
C) Corrected an accidental premature "return" statement in the encode-time
   ROI logic that caused a segmentation fault to appear if the "kdu_compress"
   demo app is used with an "-roi" argument but no other parameters
   (such as Rshift) to specify what should be done to the region of interest.
D) Corrected two other bugs in the offsets computed for interfacing with
   ROI line buffers in the block encoding logic -- only for region-of-interest
   encoding.
E) Corrected a very subtle bug in the "kdcs_comms" machinery (network
   comms for the client and server components) which could lead to
   a race condition between acceptance and serving of an incoming
   socket connection.
F) Corrected another subtle race condition in the `kdu_cache' object that
   could cause an attempt to access invalid memory contents when browsing
   remote content with multiple codestreams.
G) Found an error in the interaction between `kdu_codestream::restart'
   (for input codestreams) and the way in which multi-component transform
   parameters are translated and finalized (does not apply to Part-1
   codestreams).  This led to a revision of the way in which parameter
   changes are deteced when using the efficient `kdu_codestream::restart'
   feature, so as to robustly detect all changes between codestreams.
H) Corrected a minor oversight in the "kdu_v_expand" demo app, where the
   `io_queue' object was passed to `kdu_thread_entity::terminate' before
   wrapping up the application, rather than `kdu_thread_entity::join'.  In
   some cases, using `terminate' resulted in premature termination of the
   background multi-threaded processing jobs so that the final decompressed
   video frame might not be written to the file.
I) Fixed some subtle race conditions in "kdu_server" that could occur under
   unusual network conditions or when an application runs out of file handles.
J) Corrected an oversight in `kdu_client::get_window_in_progress' and
   `kdu_client::get_oob_window_in_progress' with respect to the issuing of
   the `KDU_CLIENT_WINDOW_IS_COMPLETE' status flag.  That flag is now set
   only if the server has indicated in an EOR message that it has sent the
   complete response to the window of interest AND the response data from
   all potentially concurrent requests on all other JPIP channels has also
   been received -- this is a subtle correction that is relevant only when
   using multiple request queues, and then only if multiple JPIP channels
   are actually created.
K) Introduced a very minor correction to the way the "kdu_merge" tool
   rounds the relative locations and cropping dimensions provided via its
   `-composit' argument.  Previously, all parameters were rounded
   down while converting to integer quantities, but this can make it
   impossible to specify certain precise cropping or placement configurations
   via the "kdu_merge" command-line; they are now rounded to the nearest
   integer.
L) Slightly modified the way the "kdu_show" application posts JPIP requests
   when there is insufficient information in the cache to begin image
   rendering yet.  Previously, under this case the application always asked
   for the entire composition surface, unless a focus box was manually
   defined, accidentally ignoring the location and dimensions of the
   user's view port.
M) Minor change in `kdu_region_compositor::get_max_available_quality_layers'
   to take the maximum of the number of quality layers used by all
   opened tiles in relevant codestreams, together with the `Clayers'
   attribute in such codestreams' main header.  This helps ensure the
   reliability of the value returned, when no (or few) tiles have yet been
   opened.
N) Fixed some subtle race conditions inside the implementation of `kdu_cache'
   that never appeared until some of our customers started building
   applications that involve heavy multi-threading, very high bandwidths
   and continuous streaming over days/weeks.
O) Fixed a race condition in the `kdu_client' object during primary channel
   address resolution, where a call to `signal_status' was accidentally
   invoked outside the critical section within which such calls should
   originate.
P) Corrected an error in `kdu_window_model::add_instruction' (both versions)
   that is used by the "kdu_server" application and any third party apps
   that build on top of the `kdu_serve' interface.  Specifically, the
   test for an additive cache model statement with 0 bytes (a no-op) was
   accidentally testing for a subtractive model statement with 0 bytes, as
   pointed out by a licensee.
Q) Made sure that all demo apps that might involve multi-threaded processing
   install a thread-safe core message handler for error and warning messages.
   In most cases, this simply means that an error/warning message handler that
   was previously derived from `kdu_message' is now derived from
   `kdu_thread_safe_message' -- this avoids the possibility of garbled
   error messages arriving from multiple threads when the core
   system encounters something wrong with the content or commands supplied.

Changes from KDU-7.0 to KDU-7.1
-------------------------------
This is an important upgrade that incorporates major improvements to
the multi-threaded core processing sub-system introduced in KDU-7, along
with numerous enhancements to other aspects of Kakadu and some new
features.  The KDU-7.1 release is the most stable and comprehensive
version of the regular Kakadu SDK on which to build applications.  This
release incorporates quite a few fixes for bugs that have been exposed
with the help of our licensees who are building applications on top of
some of Kakadu's most advanced features.  Changes from KDU-7.0 are as
follows:
1) Multi-threaded processing sub-system improvements
   -- This version completes the implementation of background codestream
      processing machinery, a feature that was introduced but hardly used
      in KDU-7.0.  The background processing machinery accomplishes tasks
      such as incremental flushing of compressed content (during compression),
      background reading and pre-parsing of codestream content (during
      decompression), background cleanup of compressed data and resources
      that are no longer needed, and background allocation and configuration
      of resources that will be needed in the near future (e.g., precinct
      and code-block structures to accept encoded data during compression).
      Where these background tasks have dependencies (block encoding or
      decoding operations that need the task to be performed), the completion
      of relevant background work contributes to the scheduling of dependent
      tasks so that the dependencies are always satisfied.  As a result,
      block encoding and decoding operations where most threads spend most
      of their time need NEVER BLOCK on the availability of codestream
      resources.
   -- This release greatly reduces the number of points at which threads
      might need to take out locks on critical sections.  Already this was
      greatly reduced with the transition to KDU-7 from previous versions
      of Kakadu, but additional improvements have been made that help
      further improve throughput in heavily multi-threaded platforms.
   -- A number of changes have been introduced to reduce the frequency with
      which the system heap management functions need to be invoked, so as to
      minimize or totally avoid contention for the heap.
   -- A few possible race conditions that we identified in KDU-7.0
      following its release, have been eliminated.
   -- This release includes some special templates for derived
      `kdu_thread_queue' and `kdu_thread_job' objects that can be used in
      heavily multi-threaded environments to ensure that the work typically
      done by a master processing thread (pushing image data into a compressor
      or pulling it out of a decompressor) can be done in an automatically
      rescheduled job that always runs on the first available thread.  This
      can reduce the appearance of delays on the master processing thread as
      bottlenecks to the overall processing throughput of an application.
      To learn more about this feature, consult the documentation for
      `kdu_run_queue'.
   -- This release introduces new "preferred" versions of the
      `kdu_multi_analysis::create' and `kdu_multi_synthesis::create' functions
      that accept control flags rather than a multitude of discrete options.
      The `kdu_multi_synthesis' object has also been augmented with a `start'
      function that can be used (optionally, if the relevant flag is supplied
      to its `create' function) to achieve optimal interleaving of
      multi-threaded processing jobs for cases where multiple tiles are being
      processed concurrently using separate `kdu_multi_synthesis' objects.
   -- Fixed a bug in the dependency propagation logic in
      `kdu_multi_analysis' and `kdu_multi_synthesis' which could occur
      when the multi-threaded (asynchronous) DWT implementation was not
      selected (i.e., when `KDU_MULTI_XFORM_MT_DWT' or `KDU_MULTI_XFORM_DBUF'
      was passed in the `flags' argument to `create').
   -- This release slightly modifies the definition of
      `kdu_thread_queue::update_dependencies' to incorporate a meaningful
      return value.  The return value is used to discover whether dependency
      propagation is of interest to super-queues (or dependency monitors),
      allowing multi-threaded processing machinery to avoid the overhead of
      careful synchronized dependency propagation beyond the point in the
      queue heirarchy where dependencies are of interest.  This feature is
      exploited within `kdu_multi_analysis' and `kdu_multi_synthesis' to
      reduce the overhead incurred within the synchronous DWT path (this is
      used where the `KDU_MULTI_XFORM_MT_DWT' or `KDU_MULTI_XFORM_DBUF' flag
      is not supplied to `create'), which is often used with very small
      stripe buffers (typically only 1 line).
   -- In this release, the logic associated with identifying the point at
      which all jobs have been scheduled for a `kdu_thread_queue' object
      has been changed very slightly.  You will only find this relevant if
      you have developed your own multi-threaded processing objects that
      schedule their own jobs and also take the effort to provide the
      earliest possible notification of the point at which all jobs have
      been scheduled (none of this is mandatory but can improve efficiency
      in some circumstances).  Previously, the way to do this was to invoke
      `kdu_thread_queue::all_scheduled', but the new approach is to specify
      the `all_scheduled' condition in the `kdu_thread_queue::schedule_jobs'
      call itself.  For multi-threaded processing objects that rely entirely
      upon lockless processing (i.e., no critical sections at all), the new
      approach helps the programmer avoid possible race conditions that
      could arise if execution of the final scheduled job sets in motion events
      that could clean up the object's resources, before an explicit separate
      call to the old `all_scheduled' completed.  The new mechanism are all
      100% robust against such race possibilities and can be safely executed
      without taking out critical section locks.
2) Changes related to precinct/packet parsing and compressed data caches
   -- Fixed a possible source of race conditions (long present in Kakadu)
      whereby calls to functions like `kdu_resolution::get_precinct_packets'
      could potentially interfere with ongoing processing in other threads by
      code-block decoding engines.  In reality, it is unlikely that
      applications do such things concurrently within the same open tile, but
      it is possible and without the fix, could have caused a subtle and hard
      to track failure condition.  This problem was fixed while at the same
      time improving the efficiency of the JPEG2000 packet parsing process
      for cases in which the source of data is a dynamic cache whose contents
      are evolving as new data arrives.
   -- Slightly modified `kdu_resolution::get_precinct_packets' so that it
      never returns a value larger than the currently installed maximum number
      of apparent quality layers -- previously, the behaviour of the function
      under quality layer limitations was too dependent upon the underlying
      structure and random accessibility of the compressed source. This change
      should not adversely affect existing applications but make their
      behaviour more consistent.
   -- Added an extra argument to `kdu_region_composior::get_codestream_packets'
      so that the caller can limit the maximum number of quality layers over
      which the completeness of the available codestream content will be
      assessed.  This is useful for determining whether or not a dynamic
      cache contains all content up to some required number of quality layers
      for a given region of interest, for certain types of JPIP browsing
      applications.  This feature is also now employed by the "kdu_winshow"
      and "kdu_macshow" apps, whose JPIP progress bar now displays the
      degree to which a Window-of-Interest request has been satisfied, taking
      into account any restriction on the number of quality layers
      that are of interest (as specified via the `<' and `>' accelerators or
      the corresponding menu items).
3) New core codestream generation features
   -- Added a new codestream parameter attribute `ORGplt_parts', that can
      be used to control the points at which packet length information (for
      random access to precinct data) is partitioned into PLT marker segments.
      This is useful in constructing codestreams that adhere to recommended
      configurations described in the NITF JPEG2000 profile specification.
   -- Added a new core interface function `kdu_codestream::get_packet_bytes'
      that can be used to discover the total number of bytes generated for all
      written JPEG2000 packets (packet bodies and packet headers).  The
      difference between this value and that returned by
      `kdu_codestream::get_total_bytes' identifies the total number of bytes
      consumed by markers and marker attributes (i.e., non-packet header and
      marker bytes).  This value, in turn, is of interest because the
      `Creslengths' attribute specifically constrains the number of packet
      bytes (packet header and packet body bytes) only, whereas some profiles
      (notably broadcast profiles) require the overall codestream length to be
      constrained.  The "kdu_v_compress" application uses this feature to
      report the number of non-packet bytes (i.e., header and marker bytes)
      which can then be subtracted from a profile limit on the overall
      codestream size in order to determine a `Creslengths' value that is
      guaranteed to avoid profile violation.
   -- Significantly improved automatic selection of coding parameter
      attributes that have not been set where `Sprofile' indicates a broadcast
      profile.
   -- The rate control capabilities realized via `kdu_codestream::flush' and
      `kdu_codestream::auto_flush' have been extended to allow for the
      simultaneous use of both distortion-length slope thresholds and
      layer size limits (measured in bytes).  Previously, one or the
      other of these two methods were accepted for controlling the amount of
      content generated for each quality layer.  In particular, if both
      layer size limits and slope thresholds were supplied, only the slope
      thresholds would be used.  These two methods were demonstrated via the
      `-slope' and `-rate' arguments to demo apps like "kdu_compress" and
      "kdu_v_compress".  It is now possible to pass the auxiliary flag,
      `KDU_FLUSH_USES_THRESHES_AND_SIZES', to `kdu_codestream::flush' or
      `kdu_codestream::auto_flush', along with both a `layer_bytes' array
      and a `layer_thresholds' array (with a non-zero first entry), in
      which case the quality layer generation procedure is governed primarily
      by the slope thresholds found in the `layer_thresholds' array (as
      before), but the information found in the `layer_bytes' array will be
      interpreted as preferred lower bounds on the number of bytes included
      for each quality layer.  The idea is that unexpectedly compressible
      content would receive more bytes than might otherwise be assigned by
      the slope thresholds, so as to (preferably) achieve the specified
      lower bounds.  This feature has been requested for applications
      in video compression.  The feature works together with upper bounds
      specified via `Creslengths' -- the upper bounds always take preference
      over lower bounds or any other rate control specifications.  The
      feature also works with incremental flushing.
4) New/improved file format support: broadcast streams, video, JPX & jp2info
   -- Added support for the file-format aspects of the broadcast specification
      found in Annex-M of IS15444-1/AMD3.  Previous versions of Kakadu already
      supported the broadcast profiles defined in IS15444-1/AMD3 at the
      codestream level, but did not offer the JP2 box types defined for
      construction of broadcast streams.  These box types are now defined in
      "jp2.h", while interfaces to read and write broadcast streams may be
      found in "jpb.h" and "jpb.cpp".  Broadcast streams have been added as
      one of the file types supported by the "kdu_v_compress" and
      "kdu_v_expand" demo apps, which now support some additional command-line
      arguments -- see the `-usage' statement, as usual.
   -- Improved support for interlaced video in the interfaces offered for
      Motion JPEG2000 and Elementary broadcast streams, and extended the
      "kdu_v_compress" and "kdu_v_expand" demo apps to support interlaced
      video, as well as mixed interlaced/progressive content.
   -- Added a new generic facility for generating names and textual
      representations for JP2 boxes, as used in JP2, JPX and MJ2 files as
      well as elementary broadcast streams.  This facility is demonstrated
      by the new "kdu_jp2info" application that prints an XML-compatible
      representation of any JP2-family file or raw codestream, with options
      to control the level of detail printed.
   -- Greatly improved the usage of "const" qualifiers for "get" and other
      attribute accessor methods associated with the numerous interface
      classes offered in support of the JP2 and JPX file formats and
      JP2-family files in general.
   -- Added new methods `jp2_input_box::open_as' and `jp2_input_box::fork'.
      The former, allows a box's header to be read, after which the box can
      be closed and later re-opened without incurring any I/O overhead.  The
      latter allows a box's reading state to be reproduced in another
      `jp2_input_box', similar to `jp2_input_box::transplant', except that
      the forking source box is also left open.
5) Improvements to rendering by `kdu_region_compositor'
   -- Improved the way in which `kdu_region_compositor' scales and places
      compositing layers on the compositing surface.  Specifically, the
      underlying rational scaling factors now approximate the ideal scaling
      factors to sufficient accuracy to ensure placement and sizing of image
      layers correctly, even for compositions involving many image layers
      composed onto very large surfaces.  Previously, the accuracy with
      which scaling factors were approximated was related to the dimensions
      of the image layers in question, rather than the final composition
      surface -- this had the potential to lead to placement errors for
      composition surfaces much larger than the individual image layers.
   -- Fixed a minor bug in the `kdu_compositor_buf::get_region' function.
6) JPIP client/server improvements and fixes
   -- Slight improved the way in which JPIP `context' requests are converted
      into individual window regions and resolutions within each codestream
      associated with the `context', so as to avoid possible accumulation
      of sizing or positioning errors associated with complex JPX compositions
      with large compositing surfaces formed from smaller compositing layers.
      These improvements appear within both "kdu_servex.cpp" and
      "kdu_clientx.cpp".
   -- Corrected an oversight in the support of JPX composition instructions
      with orientation/flipping information.  Support for this feature was
      first introduced with KDU-7.0, within the `jpx_source' and `jpx_target'
      objects, along with corresponding support in `kdu_region_compositor' and
      the "kdu_server" application.  However, support for the feature was
      accidentally omitted from the `kdu_clientx' object that is used by
      Kakadu's JPIP client to inform servers of existing cached content.
   -- Fixed a problem in "kdu_server" that arose in rare circumstances where
      the `-phld_threshold' configuration option is set to a value larger than
      the size of the JPX composition box (where there is one).  In such
      cases, the contents of the box were not used by the server to translate
      JPIP "context" requests of type "jpxl" with composition instruction
      qualifiers.  As a result, in these circumstances the server would serve
      the wrong content in response to sophisticated JPIP client requests
      involving complex composited imagery.
   -- Fixed a bug in `kdu_client::read_tcp_chunk' which could have resulted
      in the return of multiple acknowledgement messages to the server under
      very high bandwidth conditions where the operating system's internal
      TCP buffer became temporarily full.
   -- Added a new status member function `kdu_client::target_started' that
      can be used to check whether or not the first reply to a JPIP client
      (the one that verifies the existence of the target resource on the
      server) has been processed.  This is useful because it allows a JPIP
      browsing application to easily avoid the situation in which image
      rendering (e.g., via `kdu_region_compositor') attempts to start up
      before the first reply has been fully processed, in which case the
      renderer may or may not see any information that can be loaded from a
      local cache file storing the results of previous browsing sessions.
      There is nothing fundamentally wrong with such a scenario, except that
      it can lead to inefficiencies and unnecessary client-server
      communication.  While the function is mainly of interest in improving
      the efficiency of the control flow where a local cache file is
      available, it may have other uses and it is open to extension in the
      future if Kakadu's client/server components support multi-target JPIP
      sessions -- something that has been requested by some users.
7) Implementation of new JPEG2000 features
   -- This release contains a preliminary implementation of the new
      fast block coding mode that is the subject of Ammendment 4 to
      IS15444-2.  This new coding mode is actually an extension of the
      original BYPASS mode to additional bit-planes in the embedded block
      coding process.  It may be accessed by the two new mode flags
      `Cmodes_BYPASS_E1' and `Cmodes_BYPASS_E2' and is supported by the
      demo apps that write and read JPEG2000 Part-2 codestreams and JPX files.
      Experimental results suggest that this feature can increase both
      compression and decompression throughput of Kakadu by approximately 30%,
      depending on the target compressed bit-rate, without significant loss
      in compression efficiency.
8) Other core system and Java interface fixes:
   -- Made minor changes to the Solaris makefiles and "kdu_elementary.h" to
      correct Solaris build problems that arose in the transition from Kakadu
      v6.x to KDU-7.
   -- Fixed a problem with the interaction between Java interfaces and
      Kakadu's multi-threading sub-system, where Kakadu funcions that called
      into a class member function implemented in Java would invoke the JNI's
      `AttachCurrentThread' function but never invoke the corresponding
      `DetachCurrentThread' function.  The consequence of this oversight was
      that applications that create and destroy `kdu_thread' objects regularly
      and pass to these threads (perhaps indirectly) instances of Java-derived
      objects with Java-implemented callback functions (examples include
      objects descended from `kdu_compressed_data_source',
      `kdu_compressed_data_target', `kdu_message' or `kdu_client_notifier')
      would experience a progressive accumulation of unreleased
      thread-specific resources, eventually leading to out-of-memory errors.
      For efficiency, and to ensure correct behaviour even with recursive
      calls in and out between C++ and Java, the new implementation caches
      the thread-specific jniEnv reference with the underlying `kdu_thread'
      object and detaches it when the thread exits from its entry-point
      function.  As a side benefit, `kdu_thread' objects now provide a
      generic facility for storing thread-specific instances of custom
      objects that are automatically cleaned up as the thread terminates.
      Moreover this information can be accessed from any point in the thread's
      execution.
   -- Fixed a bug in the DWT synthesis machinery that occurred during
      reversible decompression of a limited region of interest involving
      exactly one column, when a transposed codestream appearance transform
      is in effect.
   -- Fixed errors in the implementation of the 32-bit version of the
      `kdu_region_decompressor::pull_stripe' function.
   -- Fixed offset and minor scaling errors in the floating-point versions
      of the `kdu_region_compressor::push_stripe' and
      `kdu_region_decompressor::pull_stripe' functions and added rounding
      offsets to the high precision data conversion functions, where
      appropriate.
   -- Introduced rounding offsets to the data conversion code that handles
      floating point TIFF files in "image_in.cpp".
   -- Fixed a documentation error in the API documentation for the function
      `kdu_codestream::flush'.  Specifically, this function previously
      reported an equation for the relationship between logarithmic slope
      thresholds and the slope (delta_D/delta_L) on the operational
      distortion-length characteristic of the compression system.  The
      reported relationship was correct but for the case in which delta_D
      measures the change in total squared error in the image after
      normalizing the sample values to the range -128 to +128 (not -0.5 to
      +0.5 as previously reported).  The corrected API documentation makes
      reference to the earlier error, for clarification.

Changes from version 6.4.2 to KDU-7
-----------------------------------
This is a massive upgrade, along many dimensions.
The following is a summary of the principle features:
1) Changes to the core multi-threaded processing sub-system
   -- Kakadu's multi-threaded processing sub-system has been completely
      redesigned.  Major features of the new architecture are as follows:
   -- External interfaces are as similar as possible to what existed
      before.  For simple applications, little or no porting will be
      required to the new architecture.  The main step that should be taken
      for almost all applications is to call `kdu_thread_env::cs_terminate'
      prior to destroying a codestream that has been used with multi-threaded
      processing.  For quite a few applications, there may be nothing else
      to change.  In any case, when porting existing applications to the new
      multi-threading architecture, you should use the demo applications as
      a guide and pay particular attention to the documentation of the
      interface functions `kdu_thread_entity::join',
      `kdu_thread_entity::terminate' and `kdu_thread_env::cs_terminate' --
      reading the documentation of just these three functions should be
      sufficient as a guide to porting most applications.
   -- The new architecture is mostly lock-free, meaning that atomic
      manipulation of synchronization variables is used in place of critical
      sections to avoid the overhead and potential stalls associated with
      mutual exclusion primitives.
   -- Previously, safe multi-threaded processing involved a few exception-safe
      mutual exclusion locks that were shared by all codestreams that might
      be used with a `kdu_thread_env' environment.  However, now each
      codestream contributes its own locks to the threading environment, and
      they are also used much less often.  This will particularly improve
      the performance of systems that deploy groups of threads to work on
      multiple codestreams.
   -- Scheduling of executable jobs is now done in a distributed manner.
      There is the appearance of a central scheduler, but processing and
      scheduling operations no longer need to content for access to any
      kind of central dispatch mechanism.
   -- The previous architecture embodied a particular method for implementing
      multi-threaded processing within a tree-structured dependency graph,
      whereby threads that encountered a blocking dependency would donate
      their resources (if possible) to other processing jobs when they
      encountered blocking dependencies.  This approach essentially discovers
      the dependencies at the point when they become problematic; one can
      think of the approach as propating "dependency needs" downwards towards
      the leaves of the tree-structured graph.  These features are preserved
      in the new architecture, but they are augmented with the ability
      propagate "dependency availability" upwards towards the root of the
      tree-structured graph.  Actually, the system supports topologies other
      than trees, but that is most of what we need for JPEG2000 compression
      and decompression.  The upward propagation of "dependency availability"
      can be used by individual data processing entities (derived from
      `kdu_thread_queue') to ensure that jobs are scheduled only once their
      dependencies are satisfied.  All of Kakadu's major data processing
      objects (`kdu_encoder', `kdu_decoder', `kdu_analysis', `kdu_synthesis',
      `kdu_multi_analysis', `kdu_multi_synthesis') support fully implement
      the upward propagation of dependency availability as well as the
      downward discover of dependency needs, so that threads will always
      block and donate resources to other jobs if they need to, but jobs
      can be scheduled so that the only jobs that run are those which can
      run to completion with blocks or context switches.  As a result, the
      new architecture generally involves shorter thread execution stacks,
      fewer context switches and fewer blocking waits.
   -- The notion of a thread queue (embodied by `kdu_thread_queue') has
      changed radically in the new design.  For the purpose of backward
      compatibility, it is still possible to obtain an internally allocated
      thread queue using `kdu_thread_entity::add_queue' that has a
      compatible interface with its namesake in the old architecture; these
      can be used as before for structuring the queues allocated internally
      by data processing engines such as `kdu_encoder', `kdu_decoder',
      `kdu_analysis', `kdu_synthesis', `kdu_multi_analysis',
      `kdu_multi_synthesis' and higher level objects like
      `kdu_stripe_compressor', `kdu_stripe_decompressor' and
      `kdu_region_decompressor'.  However, `kdu_thread_queue' is no longer
      an opaque object, but a public class from which you can derive your
      own sophisticated processing objects if you wish.
   -- Codestreams themselves now come with a dynamically created
      multi-threading context that contributes background jobs to the
      workload managed by a `kdu_thread_env' environment.  This allows
      incremental flushing, trimming of initial code-block bit-streams
      during compression and pre-parsing of codestream structures that will
      be needed in the near future to all be performed concurrently with
      the main data processing operations, so that threads are blocked as
      little as possible by common dependencies.
   -- Although full exploitation of the new approach is still in progress,
      already we find substantial throughput improvements on systems with
      larger numbers of CPU cores (or hardware threads).  In some cases,
      throughput improvements of 2 or more can be obtained on modern
      processors with 8 or more hardware threads.
   -- The internal codestream memory management sub-system has been completely
      redesigned so that memory can efficiently moved to, from or between
      threads.  We no longer share the critical section associated with
      codestream parsing or content generation and in fact rarely need to
      enter a memory-management critical section at all.
   -- There is no longer any concept of a "synchronized job" that is performed
      once scheduled jobs have finished executing.  In fact, there is no
      real central dispatcher that knows or cares when jobs that were launched
      have been fully executed.  Although this may seem like a concern, the
      only place where synchronized jobs were used in any of the extensive
      set of Kakadu demo applications was for incremental flushing during
      codestream generation.  This is now handled much more elegantly by
      a new `kdu_codestream::auto_flush' function that can be configured
      to do incremental flushing at meaningful points, as they arise, without
      the need for an application to explicitly trigger this activity by
      periodically launching synchronized jobs.
   -- The member function `kdu_thread_entity::synchronize' no longer exists,
      but has been replaced by `kdu_thread_entity::join', which in most cases
      is a drop-in replacement.  In any case, the documentation explaining
      what these functions actually do should be clear now, having been
      very carefully written.
   -- The interfaces to major Kakadu processing objects remain unchanged,
      retaining the convention that multi-threaded processing is governed
      by constructing (or starting) a processing object with a non-NULL
      `kdu_thread_env' reference and (optionally) a non-NULL `kdu_thread_queue'
      reference from which the internal processing queues should be
      descended.  However, in many cases, the `kdu_thread_queue' objects
      used with these objects previously had to be unique -- that is,
      multiple processing engines could not be attached as descendants of
      the same super-queue.  In the new implementation, this restriction does
      not apply, so application developers should encounter fewer surprises.

2) Processor-specific enhancements
   -- The SIMD speedups that are enabled by the KDU_X86_INTRINSICS macro have
      been extended to incorporate SSSE3 instructions more widely, as well as
      the new AVX instructions, where appropriate.  In environments where
      AVX instructions instrinsics are not available, but X86 intrinsics are
      otherwise available, the `KDU_NO_AVX' macro must be defined.  Currently,
      this macro is defined for compilers other than Visual Studio 2010, but
      you can modify the relevant Linux makefiles if you have a recent version
      of GCC that supports AVX.
   -- As more SIMD optimization branches are added to the code, optimizations
      for older processors are not generally removed.  However, to minimize the
      clutter created by the numerous optimization branches offered by Kakadu,
      a new KDU_MIN_MMX_LEVEL macro has been introduced.  This macro identifies
      the minimum level of X86-style SIMD technology for which accelerated
      routines are compiled into the code, depending on the availability of
      multiple acceleration options.  Specifically, the macro takes values of
      1 for MMX, 2 for MMX to SSE2, 3 for MMX to SSE3, 4 for MMX to SSSE3,
      5 for MMX to SSE4.1 and 6 for MMX to AVX (subject to extension in the
      future).  If multiple accelerated code variants exist that would be
      supported at KDU_MIN_MMX_LEVEL, only the most advanced such variant is
      actually compiled into the code.  Where enabled, More advanced variants
      (i.e., beyond KDU_MIN_MMX_LEVEL) are also compiled.  Of course, if
      the CPU is found not to support any of the compiled acceleration
      options, the regular non-SIMD will be executed.
   
3) Changes to the JPX sub-system
   -- Added support for the new "group" box ("grp_") which allows
      non-semantic grouping of metadata boxes into arbitrary (potentially
      nested) groups.  Previously, Kakadu synthesized the funcionality of
      a grouping box by offering "free-asoc" boxes -- i.e., association
      boxes ("asoc") whose first sub-box was a free box ("free") and hence
      semantically meaningless.  Kakadu still supports free-asocs and can
      be forced to generate them in preference to grouping boxes, but this
      is not recommended for interoperability.  The grouping box is being
      introduced in IS15444-2/AMD3.  Non-semantic grouping is important in
      facilitating efficient access to JPX metadata stored in a remote file
      via JPIP.
   -- Major changes have been introduced into the way JPX metadata is read and
      written.  One of the major changes is that grouping boxes (and
      free-asocs) are now parsed in such a way that their contents (in the
      recursive sense) appear to be siblings -- that is, non-semantic grouping
      containers are removed from the view of the application.  Similarly,
      when a file is written, non-semantic containers are automatically
      inserted in a judicious fashion without the need for any application
      intervention.  This is all done to ensure efficient transport of
      rich metadata via JPIP.
   -- Numlist ("nlst") and Region of interest description ("roid") boxes are
      now correctly handled regardless of where they appear in the metadata
      hierarchy.  In particular, these boxes are properly understood when
      they appear as descendants of labels, XML and other metadata box types.
      The file writer preserves such hierarchical relationships without
      trying to force these boxes to appear at the top level of the file, as
      in previous versions of Kakadu..  Advanced archiving of
      these box types within searchable libraries is performed regardless of
      where the boxes appear.  All applications and API's which access
      number lists (i.e., image entity references) and region of interest
      descriptions for one purpose or another have been modified to ensure
      that they work uniformly regardless of where the numlist or roid box
      might appear within the metadata hierarchy.
   -- For some time, Kakadu has adopted a semantic classification of
      cross-reference boxes into "Grouping Links", "Alternate Child Links", and
      "Alternate Parent Links".  We believe this semantic classification is
      very sound and consistent with the original standard's specification
      for how cross-references should be interpreted.  In this latest release,
      the potential of "Grouping Links" has been expanded to fully support the
      construction of semantic groups through chains of Grouping Link nodes
      that point to each other.
   -- Quite a number of new interface functions have been added to the
      `jpx_metanode' interface to facilitate interaction with metadata.  The
      most notable additions are `jpx_metanode::get_next_descendant' and
      `jpx_metanode::get_prev_descendant' which present a variety of options
      for walking through lists of descendants of a metadata node, where an
      arbitrary subset of the descendants might be missing (typical during
      an interactive JPIP session).  These functions should be used in
      preference to the older, less flexible functions `get_descendant' and
      `count_descendants', because the list of apparent descendants of a node
      may change in unexpected ways due to the transparent collapsing of
      non-semantic grouping containers (as explained above).
   -- In previous versions of Kakadu, it was necessary to explicitly invoke
      `kdu_meta_manager::load_matches' to parse a JPX source for new metadata
      that might have become available (typical when browsing a file remotely
      via JPIP).  Now, however, certain `jpx_metanode' functions automatically
      perform whatever parsing is required to ensure that the results that they
      return are as up-to-date as possible.  This is generally much more
      efficient than a global call to the `kdu_meta_manager::load_matches'
      function, and a lot more convenient.  The `jpx_metanode' object's
      descendant walking and link following functions, as well as the
      functions that synthesize JPIP metadata requests are amongst those that
      implement this parsing-on-demand feature.
   -- Quite a bit of effort has been invested in ensuring that functions
      which might be used to repeatedly parse the metadata structure of a
      JPX source (including `jpx_meta_manager::load_matches') are as
      efficient as possible.  This is done by keeping hierarchical internal
      lists of boxes that are currently incomplete.
   -- The amount of memory required to keep track of a node in the metadata
      hierarchy has actually decreased due to more efficient internal
      representation of the information.  This means that very large
      collections of metadata can be handled in memory.
   -- JPX animation instructions with the special LIFE value 0x7FFFFFFF have
      always supposed to be treated differently, creating "pause for user
      input" frames.  However, Kakadu did not provide special treatment for
      this case.  This has been remedied.  In particular, the
      `jpx_composition::get_frame_info' and `jpx_composition::add_frame'
      now interpret illegal `duration' value of 0, as a reference to
      the presence of such "pause" instructions.  This is explained carefully
      with the API functions.

4) Client/Server changes
   -- The client now offers a special "Out-of-Band" (OOB) virtual request
      channel that applications can use to route high priority (typically
      small) requests around on-going (typically much larger) requests.  For
      example, an application can use this new mechanism to request important
      metadata while imagery is continually being streamed from the server.
      As with all communication mechanisms offered by the `kdu_client',
      the underlying communication is implemented using either physically
      separate JPIP communication chnannels (to the extent supported by the
      server) or by multiplexing virtual channels onto a single JPIP channel
      (or a smaller number of established JPIP channels), so the application
      does not have to worry about how the data is actually communicated.
   -- Both the "kdu_server" application and the `kdu_client' class now
      support communications via the new UDP transport described in
      IS15444-9/AMD5.  However, the UDP implementation should be considered
      somewhat experimental for the moment, in part because the implementation
      does not yet take full advantage of the opportunity to communicate
      with low delay over unreliable channels.  Communication has been
      tested under Windows and OSX so far, but should also work under Linux
      and other operating systems.
   -- Superior channel state estimation for auxiliary TCP and UDP channels.
   -- New JPIP request fields introduced with IS15444-9/AMD5 are now supported,
      such as the "handled" request, which allows clients to discover what
      features the server offers.
   -- Totally re-implemented the way in which cache model manipulation
      instructions are handled.  The new implementation is much more efficient
      in that cache model manipulation is deferred (wherever possible) to the
      point when the relevant cache model entries are actually accessed
      while serving a request.  This certainly makes stateless request
      handling much more efficient, since stateless requests typically involve
      a lot of cache model statements.
   -- Totally re-implemented the way in which cache models are stored
      internally.  The new implementation allows for the retention of
      information about holes in the cache model for any given data-bin,
      which is very useful when working with lossy communication channels
      (notably the new HTTP-UDP transport channel defined by JPIP).
   -- The new cache model implementation actually requires less memory than
      the old one on 64-bit architectures -- typically just 64 bits per
      data-bin.

   -- NOTE: need to modify the channel state estimation machinery just a
      little bit more.

5) Enhancements to `kdu_region_compositor' and `kdu_region_decompressor'
   -- Added support for the automatic generation of JPIP metadata requests
      that are relevant to a defined region of a composition.  This is done
      through the sophisticated `kdu_region_compositor::generate_metareq'
      function.  This function greatly simplifies the implementation of
      applications that need to work intelligently with content that is
      located on a remote server.  Use of this function is demonstrated
      by the "kdu_show" demo applications.
   -- Enhanced the way in which queued composition buffers are handled so as
      to facilitate better interaction between separate rendering and
      display threads for animated display applications.  Changes should all
      be backward compatible with existing applications.
   -- Added more SIMD accelerations to the internal processing machinery.
   -- Introduced significant changes to the sequence of internal steps that
      are performed when composited frames (or buffers with metadata overlays)
      are generated during animations (i.e., when composition buffers are
      pushed onto the composition buffer queue).  These changes should be
      transparent to the application but they serve to avoid redundant
      buffer initialization, copy and composition steps so long as the
      `kdu_region_compositor::set_surface_initialization_mode' function has
      been used to turn off pre-initialization (always recommended for
      applications involving animation).
   -- Fixed a problem that prevented regions of interest that were described
      on a codestream canvas with higher resolution than any image plane
      from being painted with full precision onto metadata overlays.  For
      example, the image data in a JPEG2000 codestream may be defined on a
      sub-sampled grid (not often done, but legal), allowing regions of
      interest to be described through the metadata to sub-pixel precision
      (with respect to the imagery itself).  Previously the faction bits in
      such ROI descriptions were discarded during metadata overlay painting --
      only really an issue when drawing a heavily zoomed-in version of the
      content.
   -- Slightly modified the region mapping functions offered by
      `kdu_region_compositor'  and `kdu_region_decompressor' to better handle
      the mapping of vertices and regions between composited buffer coordinates
      and codestream canvas coordinates.

6) New animation support
   -- A new `kdu_region_animator' object has been added to the special
      support objects found in the "apps/support" directory.  This object
      is designed to work with `kdu_region_compositor' in rendering
      applications that also need animation.
   -- In addition to managing the scheduling of frames for regular MJ2
      video tracks and JPX animations, this new object is capable of
      synthesizing novel animations from metadata within a JPX file.
      For example, a sequence of metadata nodes that contain or link to
      ROI description boxes can be automatically converted into an animation
      that pans between the various regions of interest within their
      respective compositing layers or within the fully composited frames
      to which they belong.  The animator provides special dynamic panning
      features which work together with `kdu_region_compositor' and a
      screen update engine (provided by the application) in a very
      simple way to synthesize smooth pans around the metadata of interest,
      with configurable speed and acceleration parameters, while keeping
      memory consumption very low (a constant feature in Kakadu).
   -- The animator is designed to work well with JPIP when the actual source
      file is located on a remote server.  The animator is even
      able to synthesize JPIP requests for the metadata required to
      determine the regions of interest and/or image entities that will
      comprise a metadata-driven dynamic animation, so that the number,
      sequence and identities of the animation frames is discovered
      dynamically in "just-in-time" fashion, while the animation is in
      progress.  In general, Kakadu strives to implement the dream of
      delay-free interactive rendering, by pre-requesting important
      structural elements using the special OOB request queue now offered
      by the `kdu_client', while relying heavily upon the fact that JPEG2000
      content can generally be rendered from a cache with almost any arbitrary
      amount of content, being asynchronously populated by the client as and
      when it receives data.  There are relatively few stall conditions for
      an animation and the animator attempts to avoid these by synthesizing
      its own JPIP requests.

7) Enhancements to the "kdu_show" applications
   -- The metadata catalog sidebar now offers much improved handling of
      ROI (region of interest) and numlist (image-entity of interest) nodes,
      providing an intuitive colour coding scheme to indicate whether metadata
      is associated with specific regions or just with specific image entities.
      Moreover, this works for region or image-entity metadata associations
      that are found anywhere in the metadata hierarchy.  When interacting with
      a remote server, ROI or numlist nodes whose descendants are not yet
      available are displayed using a special notation which disappears when
      descendant metadata arrives to provide more information.
   -- The application now has the ability to add grouping links to other
      link nodes, allowing for the construction of chains of related
      metadata.
   -- Cut, link and paste operations (using ctrl-X/cmd-X, ctrl-L/cmd-L and
      ctrl-V/cmd-V) now work in an expanded range of contexts.  For example,
      regions of interest can be used to form links or cut and pasted under
      other nodes, and this can be done either from the catalog sidebar or
      from the image view (based on mouse position).
   -- The application now provides metadata-driven animations.  The easiest
      way to access these is by holding the Shift key down while
      double-clicking on a metadata item of interest -- all of its descendants
      are then used to build and play an animation that visits the metadata
      elements of interest in an appealing manner.
   -- Video playback and animation is now implemented through a derived
      version of the `kdu_region_compositor' class that allows much
      better decoupling between the display thread and the main application
      thread.  Management of the composition buffer queue is now handled in
      a more collaborative fashion between these two threads so that the
      display thread does not require the queue to be serviced rapidly by
      the application thread.  Focus and dynamic Region-of-Interest
      information is now stored as part of the state of a queued composition
      buffer so that all aspects of a displayed frame can be updated
      synchronously.
   -- Additionally, video playback and animation is managed by the new
      platform-independent `kdu_region_animator' support object, which
      moves a lot of complexity from the demo application and also allows
      JPIP support for video and animations to be handled in a uniform
      manner in this and future versions of Kakadu.
   -- An animation control bar automatically opens if video/animation is
      offered by the source -- the animation bar fits in neatly with the
      status bar, JPIP progress bar and metadata catalog sidebar, all of
      which open and position themselves around the frame of each
      view window, as appropriate.  The animation bar exposes all the
      animation-related menu options in an easily accessible environment,
      along with a slider allowing the video timescale to be visualized
      and dragged around. In the future, this animation bar will also
      allow visualization of the JPIP cache occupancy as a function of time.
   -- The multi-threaded processing environment associated with each
      window opened by "kdu_show" is now destroyed each time the associated
      file (or data source) is closed, and recreated when one is opened.
      This ensures that the window can be used again properly, with full
      multi-threaded processing, after a data source is found to produce
      an error (unexpectedly) and closed.

8) Enhancements to the "kdu_v_compress" application
   -- For a long time now, we have been intending to provide a video
      compression demo application that can take full advantage of multiple
      CPU resources, in the same way that "kdu_vex_fast" does for
      decompression.  With Kakadu's new core multi-threading sub-system,
      however, it is not necessary to partition the video compression
      problem across multiple parallel frame processing engines.  Instead,
      all that is required to achieve most of the throughput available from
      machines with a modest number of CPU cores (e.g., 4 to 8) is to
      modify "kdu_v_compress" to handle file I/O (video reading and
      compressed video flushing) in background threads.  This is exactly
      what we have done.
   -- The new `-overlapped_frames' option provided by "kdu_v_compress" should
      be used to obtain maximum throughput.  For this reason, the option is
      almost always selected by default and must be explicitly disabled
      using the `-disjoint_frames' option, unless single threaded processing
      is selected.
   -- For high bit-depth video content, "kdu_v_compress" now also accepts
      VIX files whose header declares that the data bits are stored in the
      least significant bits of each data word, rather than just the most
      significant bits of each data word -- see the usage statement for an
      explanation of how the used bits are identified in the header.
   -- The new "kdu_v_compress" demo app can be run without an output file
      (i.e., "-o" is an optional command-line argument now).  In this case,
      all compression, rate control and codestream flushing operations are
      performed as normal, but the output is discarded rather than being
      written to disk.  This mode of operation is useful for estimating
      true throughput performance in applications where the compressed data
      is to be passed in memory to other parts of the application.  Some
      disk systems can present large latencies when programs attempt to
      interleave reads and writes to the same disk, which is what happens
      during video compression with both input and output files supplied.

9) Enhancements to the "kdu_v_expand" application
   -- Previously, the "kdu_v_expand' application offered overlapped frame
      processing and an "-in_memory" option that allows compressed data to
      be loaded into memory before decompression.  However, some of these
      tasks were performed on the main thread so that the throughput of the
      decompression processing could be unnecessarily limited by I/O
      bottlenecks.  Now, the "kdu_v_expand" application uses the same
      approach as the new "kdu_v_compress" application (their internal
      structures are almost identical). Kakadu's core multi-threaded
      machinery is used to perform background loading of compressed data
      (if `-in_memory' is specified) and saving of decompressed frames (if
      an output file is provided), in addition to decompression processing
      jobs (wavelet transform and block decoding).  This means that thread
      processing resources are maximally utilized, going idle only when there
      really is nothing to do.

10) Changes to the way "kdu_compress" and "kdu_expand" handle resolution
    tags/boxes.
   -- Although these are only demo applications, they are often used to
      perform conversion between JP2 and TIFF files.  It has come to our
      attention that TIFF resolution tags are usually best interpreted as
      describing CAPTURE resolution, rather than DISPLAY resolution, which
      was not the interpretation adopted in earlier versions of Kakadu.
      There is also now a more careful description of the interpretation
      of capture and display resolution sub-boxes in Ammendment 6 of the
      JP2 file format (IS15444-1/AMD6).  Accordingly, these applications
      now do the following:
      + If TIFF resolution tags with known units are found in a source file,
        they are reproduced with quite some care in a JP2 CAPTURE resolution
        box.  If resolution tags with unknown units are found, and the
        aspect ratio is not 1:1, the information is reproduced in a JP2
        DISPLAY resolution box, using a resolution of 1 grid-point per metre.
        This is done because the absolute resolution information provided by
        DISPLAY resolution boxes is to be treated as indicative/preferred
        rather than a physical property.
      + When expanding a JP2 file into a TIFF output, if a CAPTURE resolution
        box is found in the source, its contents are reproduced in TIFF
        resolution tags.  Failing this, if a DISPLAY resolution box is found
        in the source, its contents are reproduced in TIFF resolution tags,
        but if the vertical display resolution is lower than 10
        grid-points/metre (a ridiculous resolution to display anything
        at), the TIFF resolution units are set to UNKNOWN.
      + The effect of the above changes is to preserve TIFF resolution
        information across most applications and to preserve it as CAPTURE
        rather than DISPLAY resolution wherever this makes sense.

11) Other changes and fixes:
   -- The demo applications that accept a `-double_buffering' command-line
      argument now automatically configure a reasonable double-buffering
      stripe height whenever the number of processing threads is greater than
      1, but it is possible to disable double-buffering within the DWT
      machinery by supplying 0 for the `-double_buffering' argument.  Double
      buffering costs very little in memory, but almost always offers
      improved throughput with Kakadu's new multi-threaded processing
      sub-system.
   -- The "kdu_merge" demo application has been modified slightly to
      produce nicer photo albums with the "-album" option.  The initial
      set of "album pages" now have the special JPX "pause" attribute,
      meaning that a player will pause when it encounters them, waiting for
      user input, and skip over consecutive pause frames when started from
      a pause frame.  This is all part of the existing JPX standard and
      makes for a more natural interactive experience.
   -- Fixed a number of places in "kdu_params.cpp" where memory leaks could
      potentially arise if an error condition generated an exception through
      `kdu_error'.  This is important only for long-lived applications that
      may open numerous images, possibly containing errors.
   -- Fixed a subtle source of race conditions on 32-bit platforms,
      inside the `kdu_precinct_ref::active_deref' function.
   -- Fixed an error in the `kdu_resolution::get_precinct_samples' function
      which caused it to return 0 when executed at resolution 0 -- this bug
      may have had follow-on implications in creating an endless loop within
      "kdu_server" when serving content involving many codestreams, at the
      lowest resolution only.
   -- Fixed a minor memory leak in "kdu_vex_fast".
   -- Modified all processing time measurements to use the `kdu_clock' class
      defined in "kdu_elementary.h" rather than directly calling the ANSII C
      clock() function -- the `kdu_clock' class uses the clock() function on
      Windows, where the clock() function actually measures system time; on
      most other operating systems, however, the `kdu_clock' class uses
      other functions (like `gettimeofday') that measure system time.  This
      prevents inconsistent times being reported on non-Windows platforms
      where the ANSII C clock() function typically measures ellapsed CPU
      time.
   -- Modified "demo 5" within the "kdu_render" demo application to use N
      threads, where N is the number of available physical/virtual CPU's,
      rather than just 2 threads.  Also modified "demo 6" to use multi-threaded
      processing, so as to somewhat enhance the demonstration.
   -- Added multi-threaded processing (just a few lines of code) to the
      "KduRender2.cs" C# example application and the "KduRender2.java" Java
      example application.


Changes from version 6.4.1 to 6.4.2
-----------------------------------
These are all minor bug fixes, as follows:
-- In `kd_meta::find_active_scope_and_sequence',
   replace
            for (cne=0; ((rge=rg->expansion->access_range(cne)) != NULL) &&
                 !have_stream_match; cne++)
              have_stream_match = true;
   with
            for (cne=0; ((rge=rg->expansion->access_range(cne)) != NULL) &&
                 !have_stream_match; cne++)
              have_stream_match = scope->entities->test_codestream_range(*rge);
-- In `kdu_range_set::expand',
   replace
          from += (min_idx-from)/rg->step;
   with
          from += rg->step * ((min_idx-from)/rg->step);
-- In `kd_window_context::process_window_changes'
   replace
      for (csn=0; !(csrg=stream_set.get_range(csn)).is_empty(); csn++)
        { 
          ...
   with
      for (csn=0; !(csrg=stream_set.get_range(csn)).is_empty(); csn++)
        { 
          if (csrg.context_type == KDU_JPIP_CONTEXT_TRANSLATED)
            continue;
          ....
-- In "block_encoder.cpp", the convex hull analysis function has been modified
   slightly to improve efficiency and avoid introducing misleading convex
   hull points after the 8th coding pass.
-- In `kdu_servex::read_codestream_summary_info', a call to a new function
   `ensure_monotonic_log_slopes' has been introduced in order to better
   massage problematic RD slope information that might be found in the
   codestream comments and later used for rate-distortion optimized
   content delivery.  The original version could potentially leave the
   slope threshold tables without the strictly monotonic property that is
   expected by the `kdu_serve' object -- if this happens an endless loop
   could arise while simulating increments.

Changes from version 6.4 to 6.4.1
---------------------------------
-- Fixed three small bugs in "kdu_server" which could easily cause memory
   faults.
-- Fixed a bug in the support for broadcast profiles in "kdu_params.cpp",
   in which the x and y sub-sampling factors were accidentally reversed
   so that 4:2:2, for example, was not properly handled.
-- Added a "-no_decode" option to "kdu_expand", so that comments could be
   printed using "-com" without having to decode the entire image.
-- Also added a "-no_res" option to "kdu_expand", so that resolution
   information is not written to the output file (if the format supports it);
   this is only important for TIFF files containing Geo referencing TIFF
   tags, where at least one application (OSX Preview) currently cannot
   handle the combination of both types of tags -- almost certainly a bug
   in that application, rather than "kdu_expand".
-- Added static build configurations to the Xcode workspaces for the core
   system (builds a static library) and demo apps (builds a static version of
   the "kdu_show" application called "kdu_shows") for the convenience of
   MAC users.
-- Augmented the Java native interfaces generated by "kdu_hyperdoc" to include
   a check for NULL objects passed in place of by-reference arguments to the
   underlying Native C++ API function.  This could happen if a developer does
   not inspect the form of the C++ API function, in which case a segmentation
   fault might occur at run-time.  Of course, it is the programmer's
   responsibility to know how to use a function, but additional checks in the
   JNI marshalling interface help improve robustness.  The new implementation
   generates an appropriate Java exception (of type KduException) which
   contains an informative text message -- unlike the other Kakadu-inspired
   Java exceptions, which contain only the exception code.

Changes from version 6.3.1 to 6.4
---------------------------------
This release represent a very big leap forward indeed.  We would probably
be calling it version 7.0, were it not for the fact that we don't want to
save our licensees from having to take action to update their licensed
version.

Major changes in this release:
* This release includes major changes to Kakadu's server technology.  The
  main server application provides full support for IPv6 now, but the
  core `kdu_serve' object has been completely re-implemented from scratch
  so as to provide the following important features:
  >> JPIP service preference are now supported at the core data-bin increment
     generation machinery, enabling the client to control the sequencing of
     codestream data and, most importantly, whether or not the server limits
     the scope of the requested Window of Interest.
  >> The new `kdu_serve' implementation can support arbitrarily large
     window-of-interest requests, referring to an almost unlimited number of
     codestreams at once.  The implementation automatically sequences content
     through a multi-level cache hierarchy, keeping only a modest amount of
     the compressed imagery active in memory at any given time, yet still
     managing to achieve good progressive delivery of content.  This is done
     by managing progressive increments in resolution and quality over the
     entire content window, while staging content into active memory in a
     fashion which does not overly burden the server.  Tests suggest that
     hundreds of clients could be served simultaneously while each requesting
     enormous windows of interest.  It is still beneficial for the client to
     keep its requests small enough to fit entirely within the server's active
     memory window, since that leads to the most effective quality progressive
     experience.  Depending upon prevailing JPIP preferences, the server
     decides whether to limit the window of interest in this way or not.  The
     "kdu_winshow" and "kdu_macshow" applications take advantage of this to
     provide the best possible browsing experience, requesting the server's
     best progressive delivery or the server's full window support, depending
     on whether or not the interactive user is working with a defined focus
     box.
  >> The new `kdu_serve' implementation attaches itself to only a limited
     number codestream resources in the backing `kdu_servex' target at
     any given time (currently at most 6, but easily changed by modifying the
     constants defined in "serve_local.h").  This ensures that sources with
     a very large number of codestreams can now be served without having the
     server memory grow rapidly.  With regard to bounded memory consumption,
     the current implementation is not entirely complete, since it provides
     mechanisms to throw away intermediate transcoding resources and even
     arbitrary portions of its cache model, but these mechanisms are not yet
     used.  Importantly, though, the new implementation is a platform which
     will support highly efficient memory bounded servers in the future.  The
     next release should see further gains in this area.
  >> The new `kdu_serve' implementation handles stateless JPIP requests much
     more efficiently than its predecessor.  Stateless behaviour is now
     built in as an optional mode for `kdu_serve', rather than being
     implemented on top of a stateful server by explicitly clearing the
     cache model on each request.  The new implementation still needs to
     maintain cache model information within the span of servicing a single
     request, but it provides highly efficient means to completely discard
     the cache model information created during the service of a stateless
     request, in preparation for another one.
  >> The new `kdu_serve' implementation now natively supports multiple
     independent JPIP channels.  Previously, JPIP channels were supported at
     the application level by switching a single `kdu_serve' context back
     and forth between the windows of interest associated with each active
     JPIP channel -- of course, this was not efficient, but it did offer the
     first ever implementation of multiple JPIP channels.  The new
     implementation preserves separate contexts for each channel, each with
     their own windows into the internal cache hierarchy.
  >> Finally, the new `kdu_serve' implementation provides a more efficient
     mechanism for handling JPIP cache model manipulation statements.  These
     are collected into an object from which the relevant cache model
     manipulation statements are extracted on-demand while the relevant
     content is active in memory -- this can greatly reduce the memory and
     computational burden of handling cache model manipulation (and also
     stateless requests).  Care is taken, of course, to comply fully with
     the JPIP standard, which means that model manipulation statements which
     do not relate to requested content must still be applied to the cache
     model at some point (if not immediately), except in the stateless mode,
     moreover the point at which this is done correctly takes into account
     the presence of any requests on other JPIP channels -- the method is
     similar to that required to manage memory cache integrity in a multi-CPU
     environment.
  >> We have done our best to maintain the interface to `kdu_serve' as it
     was before, at least for applications which may have been using its
     most basic features; however, some interface changes were required to
     support multiple window contexts, service preferences and management
     of cache model manipulation instructions in a clean way.
  >> The new implementation has been extensively tested and debugged, but
     of course there are a lot of changes here, so please report any anomalies
     that you find.

* Kakadu now supports arbitrarily shaped region of interest metadata in
  JPX files.  This is a new features which is being standardized as part
  of Ammendment 3 to IS15444-2.  Previously, regions of interest were
  limited to horizontally/vertically aligned rectangles and ellipses.  Now,
  arbitrarily oriented ellipses and arbitrary quadrilaterals are supported,
  along with all unions of these shapes -- e.g., polygons, polygons with
  rounded corners, etc.  Kakadu provides full support for these general
  regions of interest through its JPX file format support, through the
  metadata overlay rendering machinery of `kdu_region_compositor' and through
  new tools created to support ROI shape editing and discovery of geometric
  attributes.  The `jpx_roi' object has been massively extended (in a
  backward compatible manner) to support the definition of quadrilaterals
  and oriented ellipses and the computation of important geometric properties
  such as skew; area; centroid; tightest bounding rectangle of any orientation
  (for orientation-independent width and length); and major/minor axis lengths
  and angles.
     In addition to this, Kakadu now provides a very powerful and
  completely platform independent tool to edit and discover higher level
  attributes of regions of interest.  This is the `jpx_roi_editor' object.
  It can be used to build a very powerful interactive shape editor, with
  the addition of an almost negligible amount of GUI-dependent code to
  detect mouse clicks and the like -- for a demonstration, see the
  "kdu_macshow" application which provides a very capable and intuitive
  shape editor.  The `jpx_roi_editor' object manages anchor points, selection,
  dragging, deletion and addition/extension of existing regions, while
  providing a set of simple functions which a GUI application can call to
  discover the edges and arcs that need to be drawn as changes occur.  The
  object presents three different editing modes, which affect the behaviour
  which occurs when a user drags anchor points around or adds new regions.
  The editor includes sophisticated "snap to vertex", "snap to boundary" and
  "snap to shape induced guidelines" features which work properly even for
  curved boundaries.  The editor recognizes the connections formed between
  vertices, edges and elliptical curves and exploits them to make editing
  natural.
     Of particular interest is the fact that `jpx_roi_editor' provides
  high level capabilities to interpret and create complex regions representing
  filled polygons and paths.  Paths are created by individual ROI segments
  which line up in a natural way, optionally with circular junction points.
  Paths are discovered automatically and revealed in the "path editing mode".
  The editor can automatically set the thickness of a path around its
  skeleton, based on user specifications and this is a very powerful way
  to draw useful regions of interest for inclusion in the metadata of a JPX
  file.  The editor can also detect and report path segments, path loops,
  and the like so that automatic tools could navigate metadata-induced
  paths within an image or composition.  Finally, the editor can automatically
  fill a path with an optimized polygon representation, using as few ROI
  elements as possible.  This makes it relatively easy for an interactive
  user to set up arbitrary polygon regions of interest without having to
  explicitly form them from the underlying quadrilateral/elliptical elements.
     Right now, only "kdu_macshow" provides a graphical interface to the
  `jpx_roi_editor' to show off its features for interactive editing.  However,
  the same features could (and almost certainly will) easily be incorporated
  into "kdu_winshow" or any interactive viewing tool on any platform.

* Along with the introduction of arbitrarily shaped regions of interest,
  the `kdu_region_compositor' object's metadata overlay drawing code has
  been dramatically improved.  Many new customization options exist and the
  regions are drawn much more beautifully than in the past.  Perhaps of
  greatest interest is the fact that metadata overlays are now blended onto
  their underlying compositing layers with a global opacity value that can
  be dynamically selected.  This allows the strength of the overlay to be
  visually modulated without having to redraw the metadata or re-render
  anything.  The whole system is optimized to update the minimal amount
  of the buffer surface as overlays are modulated, which allows metadata
  of interest to be efficiently "flashed" on/off or "pulsed" briefly to
  reflect changes, say, in the metadata selected within a catalog.  These
  features are exploited to create a much more visually pleasing and
  meaningful browsing experience within "kdu_macshow" and "kdu_winshow", tying
  together the textual metadata catalog side bar and the visual imagery
  view in an impressive manner.

* The `kdu_region_compositor' object manages multiple composited image layers,
  each of which is constructed from codestreams.  The object also provides
  important functions to access regions and metadata associated with these
  image layers.  In previous versions, these functions identified the
  underlying imagery through a combination of indices: JPX compositing layer
  indices; codestream indices; image component indices; MJ2 track and frame
  indices; etc.  Unfortunately, this was not always done in a uniform way
  as the function interfaces evolved.  Moreover, these indices remained
  insufficient to distinguish between different imagery layers which were
  constructed from the same underlying compositing layer (for example),
  but composited differently.
     In this new release, rather than trying to patch more changes onto the
  existing machinery, there has been a sweeping reform of many of the
  interface functions offered by `kdu_region_compositor', which is based
  around two new classes: `kdu_ilayer_ref' (an unambiguous reference to an
  imagery layer) and `kdu_istream_ref' (an unambiguous reference to a single
  codestream, as it is used within a single imagery layer).
     By and large, all the same functions are available as before except that
  the terms "compositing_layer" and "layer" have generally been replaced by
  "ilayer" in the function name, and the clumsy collection of indices which
  are passed across their interfaces (along with the sometimes torturous API
  documentation) have been replaced with a `kdu_ilayer_ref' or
  `kdu_istream_ref' instance.  New functions are provided to query the
  constitution of the imagery layers and/or codestreams associated with
  these ilayer/istream references.  Although the changes are likely to
  impact almost any application which currently uses `kdu_region_compositor',
  the changes required for compatibility with the new interface functions
  are generally minor and very obvious.
     Ultimately, this change is a necessary and very good thing.  With these
  changes in place, `kdu_region_compositor' now supports arbitrary cropping,
  rotation, mirroring, scaling and composition of arbitrary imagery pieces
  (JPX compositing layers, MJ2 tracks, even raw codestreams), reusing them
  at will to create interesting image surfaces.  With these changes,
  `kdu_region_compositor' can easily and transparently be adapted to handle
  multiple simultaneous image sources in the future -- e.g., dynamically
  composit an MJ2 video track with a raw codestream.
     The main catalyst for the change in this version was the need to uniquely
  and persistently identify imagery against which a JPX region of interest
  is considered registered during interactive editing of its shape, for
  cases where the same region of interest may appear simultaneously in
  different imagery layers.

* The core system's sample data processing machinery has been modified
  to ensure that whenever `kdu_codestream::change_appearance'
  has been used to transpose imagery (possibly with additional flipping), the
  underlying wavelet transform analysis and synthesis operations will be
  performed in such a way as to ensure that the transposition operation
  is implemented perfectly, even for reversible transforms.  In previous
  versions of Kakadu, transposition merely caused the coordinate systems
  to be transposed and code-block samples to be transposed on the fly during
  block encoding/decoding, after suitable rearrangement of the code-block
  and subband indices.  Unfortunately, this led to the introduction of minor
  imperfections in the least significant bits when used with reversible
  wavelet transforms, since the non-linear integer rounding steps in these
  transforms means that vertical and horizontal transform stages do not
  perfectly commute.  In the current version, the internal wavelet analysis
  and synthesis steps now take into account the presence of any such
  transposition, performing the horizontal and vertical operators in a
  different sequence if necessary.  This means, for example, that a rotated
  view of a losslessly compressed medical image (generated using Kakadu's
  super-efficient appearance transforms) is indeed 100% lossless,
  whereas only the original orientation was truly lossless in previous
  versions of Kakadu.

* The interpretation of the 16-bit unsigned `slope_thresholds', used by
  Kakadu's core codestream generation system for managing R-D optimization,
  has been changed slightly, but the actual description of these values,
  as supplied to `kdu_codestream::flush', has been converted from a vague
  one (with an arbitrary unspecified offset) to a precise description.
  Specifically, in previous versions of Kakadu, the slope thresholds were
  given by 256*(256-32+log_2(Delta_D/Delta_L)), where Delta_D is change in
  total squared error distortion (with samples normalised to a unit nominal
  range) and Delta_L is change in code length, measured in bytes.  This
  expression was never formally given in the Kakadu API docs, however, and
  all one could count upon was that the slopes would be of the form
  A + 256*log_2(Delta_D/Delta_L) for some constant A.  Now, the slope
  thresholds are formally defined to be: 256*(256-64+log_2(Delta_D/Delta_L)).
     Along with this change of definition, the R-D information which is written
  to a codestream COMMENT marker segment (by default) has also been changed,
  but a new heading is given for the comment, which is actually correct this
  time.  Previously, the heading was
  "Kdu-Layer-Info: log_2{Delta-D(MSE)/[2^16*Delta-L(bytes)]}, L(bytes)", but
  the actual values found in the first column were equal to
  log_2(Delta_D/Delta_L) - 32.  From version 6.4, the heading is
  "Kdu-Layer-Info: log_2{Delta-D(squared-error)/Delta-L(bytes)}, L(bytes)", and
  the values found in the first column are equal to log_2(Delta_D/Delta_L)
  exactly.  Applications wanting to use R-D information from codestream
  comments should now parse each of these two types of headings and
  interpret the column data accordingly.  Note that the original heading
  was quite misleading, because the offset of 2^16 inside the log, suggested
  log_2(Delta_D/Delta_L) - 16 instead of log_2(Delta_D/Delta_L) - 32.

Other Significant Changes:
* Substantially modified the support for Networking through "kdcs_comms.h",
  in the following ways: a) IPv6 is now fully supported; b) The services
  defined in "kdcs_comms.h" are now fully abstracted from the underlying
  platform-dependent specifics, so that dependent services and applications
  (notably, `kdu_client' and "kdu_server") are implemented in a platform
  independent way.  Name resolution, address discovery, etc., are all
  abstracted through the API's defined in "kdcs_comms.h" and this is now
  done through function and class interfaces, without the use of any macros.
  In this way, dependent tools need not explicitly or implicitly import
  any of the networking-specific system headers.
* Further to the above, the parsing and interpretation of HTTP headers and
  URL's, as performed by "kdu_server", "kdu_macshow" and "kdu_winshow", and
  as provided by various functions offered by the `kdu_client' class, has
  been enhanced to offer full support for IPv6 addresses.  Specifically,
  URI's are now formed and parsed according to RFC 3986, which prescribes
  a bracketed scheme for encapsulating IPv6 literals.  Moreover, such
  bracketed literals are explicitly excluded from the usual hex-hex
  coding procedures that are applied to host names.
* Added support for the Broadcast profiles, which are the subject of
  Ammendment 4 to IS15444-1.  Specifically, `Sprofile' now accepts the
  `BROADCAST' option and `Sbroadcast' has been added to describe the
  specific broadcast profile level, multi/single tile and reversibility
  attributes of the profile.  During codestream generation, the profile
  requirement are checked and informative messages are provided to help
  you set things up correctly.
* Modified the interpolation code in `kdu_region_decompressor' which is
  responsible for scaling imagery by arbitarily selected amounts.  The
  modified implementation uses 6-tap interpolation kernels (rather than 7,
  which was a bit silly upon reflection) and allows the kernels to gracefully
  degrade into 2-tap (bi-linear) interpolators as the image becomes heavily
  expanded.  There are two benefits of this: one is implementation speed; the
  other is the ability to control overshoot/undershoot which can become
  visible at large expansion factors when interpolating content with strong
  step edges.  A `ku_region_decompressor::set_interpolation_behaviour' function
  allows you to decide how these things happen.  The new implementation also
  contains additional SIMD acceleration paths for x86-family processors.
* Added an `ORGtlm_style' parameter attribute to the core system, which can
  be used to specify the formatting style of TLM marker segments introduced
  if `ORGgen_tlm' is used during compression.  This allows any of the 6
  possible TLM structures to be selected, which is needed to meet the
  expectations of at least one application-specific profile.
* Added a progress indicator feature to "kdu_compress" and "kdu_expand" which
  can give you a decent idea of what is going on inside at regular intervals --
  useful if you're processing some massive images.  This feature is accessible
  via the `-progress' argument in both applications.
* Added YUV file support to the "kdu_v_compress" demo app.
* Added an option to the kdu_render" demo app to include the rendering of
  region-of-interest metadata overlays, blended on top of the imagery itself,
  so as to provide a very simple example of the powerful overlay rendering
  features offered by `kdu_region_compositor'.  This may also be used with
  floating-point rendering precision, demonstrating the correct generation
  of metadata ovelays even for floating point frame buffers.
* Slightly modified the way in which codestream scaling factors are selected
  by the "kdu_merge" demo app when constructing a custom JPX compositing layer
  from multiple codestreams, each having different sizes.  The new
  implementation selects the rational scaling factors which best approximate
  the ideal codestream scaling factors required to perfectly register
  the component codestreams for a single compositing layer, subject to the
  constraints imposed by JPEG2000 Part 2 (IS15444-2).  This is only relevant
  for very advanced applications, since compositing layers almost always
  correspond to individual codestreams, or a colour codestream and an opacity
  codestream having simply related dimensions.
* Added an optional `flags' argument to `kdu_region_compositor::process' which
  provides the application with further control over the way in which the
  compositor sequences work and chooses to report regions which have been
  processed.  For example, you can now choose to defer the reporting of
  processed regions, arranging for them to be internally accumulated and
  aggregated across multiple calls to the `process' function.  The new
  options can be used to improve the efficiency of interactive applications
  (this is done in "kdu_winshow" and "kdu_macshow"), minimizing redundant
  or fragmented screen updates while still providing a highly responsive
  experience.
* Fixed a bug in `jpx_input_box::url_fopen' which could cause a segmentation
  fault if reading a JPX file which has relative links to codestreams when
  the original JPX file was not opened directly by `jpx_family_src'.
* Slightly modified the way in which regions of interest are mapped between
  codestream canvas coordinates and the composited image created by
  `kdu_region_compositor', such that the integer rounding conventions lead
  to much more precise localization under arbitrary scaling and rotation of
  the imagery.  This does not affect the way imagery itself is rendered or
  scaled, but it does very slightly affect the way in which regions of interest
  (as found in JPX ROI Description boxes) are aligned and rendered onto
  metadata overlays.  Previously, regions of interest did not quite perfectly
  line up with the imagery at less than full resolution.
* Augmented the `kdu_region_compositor' functions `map_regions',
  `find_point' and `search_overlays' in various ways.  For example, the
  last two functions can now assess the visibility of the `point' with
  respect to actual rendered opacity values at that location for the layer
  in question and/or overlaying layers.  `find_point' is also able to
  enumerate all layers in which a given location is contained and visible
  with respect to a visibility threshold.  These features are used by
  `kdu_macshow' and `kdu_winshow' to improve the sensitivity of the user
  interface to visibility and opacity.
* Made some very slight changes to the way "kdu_macshow" and "kdu_winshow"
  respond to double clicking of imagery which is associated with metadata
  shown in the catalog sidebar and double clicking of catalog nodes which
  are associated with imagery to improve navigation in complex situations.
  Also made some minor improvements to metadata editing features of these
  demo apps.
* Improved the efficiency of `kdu_region_compositor' in rendering
  dynamically changing metadata overlays by fixing some problems with the
  code which detects and avoids redundant processing.
* Added a "-com" feature to the "kdu_expand" demo, which allows for the
  command-line printing of textual COM marker segments found in the codestream.
* Fixed a minor bug in "image_in.cpp" in the reading of Palette tables for
  palettized TIFF images.  In some cases, the compiled code may have reversed
  the order of the palettes for individual components, since the table reading
  functions were invoked through an arithmetic expression whose evaluation
  order was not guaranteed to be preserved as Left-Right by the compiler.

Changes from version 6.3 to 6.3.1
---------------------------------
* Fixed a one-line bug in "image_out.cpp" which affected the way raw and Tiff
  files were written out by the "kdu_expand" demo application for reversibly
  compressed 8-bit data.
* Fixed a bug in the core system which prevented the fragmented compression
  option from being used successfully with images whose boundaries were not
  aligned with tile boundaries on the canvas coordinate system.
* Modified the way IPTC metadata is read from and written to TIFF file which
  contain the IPTC TIFF tag, in an attempt to maximize compatibility and
  re-usability with other TIFF generators/readers.
* Slightly changed the code in "kdu_server" which checks for target name
  consistency across requests, so that the comparisons are performed in a
  case insensitive manner if the server has been configured to use case
  insensitive file names.
* Fixed a bug in "jp2.cpp" which affected the generation of
  JP2/JPX dimensions boxes in the case where a Part-2 codestream has more
  multi-component transform output components than codestream components.
* Fixed a bug in "kdu_region_decompressor", which could result in an access
  violation when processing codestreams which use a multi-component transform
  to synthesize extra image components.
* Fixed a bug in the core system which could manifest with Part-2 codestreams
  in which the number of codestream components is 2 or more fewer than the
  number of multi-component transform output components.

Changes from version 6.2.1 to 6.3
---------------------------------
* Completely overhauled the `kdu_region_decompressor' object so that it
  supports pretty much any image resampling operations desired, allowing
  images with too few DWT levels to be rendered at vastly lower resolutions
  than those natively available.  Disciplined interpolation/anti-aliasing
  strategies are employed, along with SIMD instruction acceleration, where
  appropriate.  The result is that you can now render original content at
  arbitrary resolutions, with very high quality and throughput.
* Simultaneously upgraded `kdu_region_decompressor' with an internal floating
  point processing path and the capability to retrieve floating-point
  precision data via two additional overloads of the `process' function.  The
  internal processing paths are now: a) 16-bit fixed-point; b) 32-bit floating
  point; and c) native pass through from the codestream decompression engine
  (where no signal processing/colour conversion is required).  The
  internal modes are automatically matched to the required precision and
  the available data types from the `kdu_multi_synthesis' engine, which
  may vary from tile-to-tile if the image was compressed in a strange way.
  All required conversions are provided transparently so that any `process'
  function may be called.  The customizable "white stretching" capability,
  which expands the dynamic range of low bit-depth imagery to match the
  exact minimum and maximum values associated with the precision requested
  via `process', now even works with palettized imagery.  Moreover,
  the floating-point versions of the `process' function always expand the
  dynamic range of whatever original components (or palettes) are involved
  to match the range configured by the application.  This means, for example,
  that a 4-bit image (values from 0 to 15) and a 12-bit signed image (values
  from -2048 to 2047) can all be consistently shifted and scaled into floating
  point values with a range of, say, 0.0 to 1023.0, or, as another example,
  0.0 to 1.0.  This is very important when creating high precision compositions
  from multiple image components or even multiple codestreams.
* Modified `kdu_region_compositor' to take advantage of the new arbitrary
  resampling features in `kdu_region_decompressor'.  As a result,
  image compositions are rendered crisply at any resolution, regardless of
  what transformations are required for the individual image layers.  The
  external API hides all this very nicely and the `find_optimal_scale'
  function correctly and consistently limits its return value to maximum and
  minimum supported scales associated with any composition.  This means
  there is no longer a risk of a crash in "kdu_show" when zooming in or
  out by vast amounts.
* The `kdu_region_compositor' object has also been augmented to support
  high precision processing throughout.  In particular, all you have to do
  to get `kdu_region_compositor' to perform all decompression, blending,
  scaling and composition at floating point precision is to override its
  `allocate_buffer' function with one which allocates a buffer of interleaved
  floating point samples (4 per pixel), in place of a buffer of 8-bit sample
  (32-bits per pixel).  Although high precision data is been available from
  lower level API's in Kakadu for some time, this new innovation means that
  all high level interfaces in Kakadu now support high precision rendering
  for scientific, medical or other applications, while offering the full set
  of integrated features.
* Substantially changed the way `kdu_region_compositor' paints metadata
  overlays so that far fewer calls to the `process' function are required
  to reflect changes.  Region-sensitive metadata is now managed within
  segments which tile the rendering canvas, so that changes in metadata
  may be limited to the segments which are affected and metadata painting
  may be strictly ordered according to region size, without running the
  risk of large sorting costs (because the segments are kept small).  There
  are lots of benefits to the new implementation.  At the same time, the
  `kdu_region_compositor' object now provides a lot more flexibility in
  the way overlay painting can be configured.  Overlays can be dynamically
  weighted as they are being rendered and the weights can be changed
  by the application and repainted with very low computational overhead.
  This allows overlay animation to be implemented efficiently -- as
  demonstrated by the "kdu_winshow" and "kdu_macshow" applications.
  -- Note carefully that to realize the above enhancements, one function
     `kdu_region_compositor::customize_overlays' has been changed in a
     manner which is not backward compatible.  This is deliberate, in
     order to draw attention to developers who have been using this
     function with previous releases -- no functionality has actually
     been lost, however.
* Added a new global macro KDU_NO_SSSE3, which may need to be defined when
  building in compilation environments which do not provide the
  "tmmintrin.h" header required for Intel SSSE3 instruction intrinsics.
* Provided a formal type definition (`kdu_exception') that should be used for
  all exceptions thrown or caught across Kakadu interfaces.  All Kakadu
  objects comply with this definition.  Currently, `kdu_exception' is just an
  `int', so all the advice given in earlier versions about throwing/catching
  integer exceptions remains valid.  However, in case we choose to change
  this definition in the future, it would be a good idea to explicitly use
  `kdu_exception' rather than `int' in your catch clauses and when throwing
  exceptions from an error handler.  Moreover, Kakadu now provides some
  explicit exception values which are recommended.  In particular, you are
  encouraged to throw `KDU_ERROR_EXCEPTION' from within an error handler.
  In addition to this, Kakadu now catches and converts `std::bad_alloc'
  exceptions into the special `KDU_MEMORY_EXCEPTION' value (of type
  `kdu_exception') for the purpose of passing such exceptions across
  programming interfaces and between threads in a multi-threaded processing
  environment (`kdu_thread_env'). These exceptions are automatically converted
  back to `std::bad_alloc' if rethrown from wihtin Kakadu, but a special
  function `kdu_rethrow' is provided to help you rethrow any converted
  exceptions if they occur in a context for which you are responsible -- e.g.,
  when recovering the type of an exception in a call to
  `kdu_region_decompressor::finish' or `kdu_thread_entity::terminate'.  All
  of this basically means that exceptions are now handled and transported in a
  methodical and readily extensible manner, throughout Kakadu.
* When throwing and catching exceptions within Java, all the right things
  happen.  In particular, the `kdu_jni/KduException' class has been extended
  to take an optional exception code, which is used to preserve the
  `kdu_exception' value thrown within a native method.  When throwing
  exceptions from Java implementations of Kakadu callback functions you are
  also recommended to pass an appropriate exception code (otherwise
  `KDU_NULL_EXCEPTION' will be used) to the `kdu_jni/KduException' constructor.
  In particular, in a Java error handler, you should throw an exception
  constructed with `Kdu_global.KDU_ERROR_EXCEPTION' as the code.  See the
  "KduRender.java" demo for an example -- more explanation in the end notes.
* Carefully reviewed all the places where dynamic memory allocation occurs
  within Kakadu, in order to ensure: a) that a `std::bad_alloc' exception
  should always be thrown if there is insufficient memory; and b) that in
  the event of such an exception being thrown, the relevant objects should be
  left in a state which can be properly cleaned up via their destructors,
  allowing an application to respond to the exception and continue, if desired.
* Added a convenient feature to the "kdu_merge" demo app, which allows
  a large number of input files to be specified implicitly where the
  files following a common numbering scheme (file name + integer suffix +
  extension).
* Slightly modified the "-composit" argument to "kdu_merge" so that it can
  accept arbitrary scaling factors when building layer compositions, as
  opposed to just integer-valued scaling factors.  Kakadu has never been
  limited to integer-valued scaling of compositing layers when forming
  JPX compositions; only this option to "kdu_merge" had the limitation.
* Added a command-line option to "kdu_server", allowing the user to configure
  it for case sensitive or case insensitive treatment of target file names in
  client requests.  Specifically, you can specify "-case_sensitive yes" or
  "-case_sensitive no".  The default behaviour is set to case sensitive for
  non-Windows systems and case-insensitive for Windows.  For more information
  on the impact of case sensitivity attributes, consult the usage statement.
* Slightly modified the `kdu_client' object so that any `kdu_client_notifier'
  object installed in the client prior to a call to `connect' will remain
  installed, at least until the network management thread terminates.  This
  allows you to have a notifier up and ready to go right from the beginning
  of a connection.  Interface documentation has been expanded to explain this
  and also point to the fact that there will be an additional, much more
  specific notification capability provided by future version of the client.
* Augmented the "kdu_render" demo application with a new rendering example,
  accessible as "-demo 6", which demonstates the use of `kdu_region_compositor'
  with floating-point precision frame buffers and high precision processing
  throughout.  Also added a "-scale" argument to this application, allowing
  images to be rendered at any desired scale (not just powers of 2) -- all
  6 examples (those based on `kdu_region_decompressor' and those based on
  `kdu_region_compositor') illustrate robust techniques for rendering with
   arbitrary scaling factors and they are all still very brief.
* Significantly improved the way in which focus boxes in "kdu_winshow" and
  "kdu_macshow" are mapped between different image elements (frames,
  compositing layers and codestreams) during interactive navigation
  (especially the type of navigation which involves a combination of
  the metadata catalog sidebar and the image view).
* Fixed some problems in the core system which could arise in the event
  that the maximum number of DWT levels (32) is used to encode an image --
  not that this makes much sense for just about any practical application.
* Fixed a couple of small yet important bugs in the implementation of
  the Part-2 multi-component dependency transform (probably the least
  utilized of the MCT transform types) in the core system.
* Fixed a very small yet critical bug in the "kdu_server" application
  which would almost certainly have caused crashes in large scale
  deployment scenarios where separate clients close after having contended
  for resources.
* Fixed a minor bug in `kdu_region_compositor' which could sometimes cause
  the `cull_inactive_layers' function to remove a shared object which is
  being used to render a single image component from a single codestream.
* Fixed a minor problem in "kdu_winshow" with the use of scrollbars within
  the catalog sidebar while image content is arriving dynamically from a
  remote server via JPIP.
* Fixed some problems with both "kdu_winshow" and "kdu_macshow" in the
  way the "Duplicate Window" menu action is handled, so that the frame
  index is correctly preserved through duplication and the duplicate window
  is more consistently resized.
* Corrected the language compliance problems which generated a large
  number of warnings under the most recent version of GCC (gcc 4.2).

Changes from version 6.2 to 6.2.1
---------------------------------
* Made some minor changes to "kdu_winshow" so as to bring its user interface
  fully into line with that of "kdu_macshow".  In particular, the "ENTER",
  "BACKSPACE" and arrow keys now have exactly behaviour in both viewers,
  when applied within the catalog sidebar -- as a consequence, the tooltips
  which explain useful keys within the sidebar are now also the same in both
  the Windows and Mac viewer.  The "kdu_winshow" application now provides more
  obvious indication of when the catalog sidebar has keyboard focus -- closely
  resembling the appearance of the Mac viewer.  Finally, "kdu_winshow" has
  been modified so that it compiles correctly when the Unicode character set
  is selected globally (a Microsoft-specific thing -- bit of a nuisance in my
  opinion) as well as when the Multi-byte character set is selected.  Lots of
  function interfaces change when the Unicode character set is selected
  (defines _UNICODE globally, which affects the interpretation of a huge
  number of MFC interfaces).  However, this is the only way to ensure that
  all control windows support the display/editing of Unicode text.  Now, when
  "kdu_winshow" is compiled with the Unicode character set (shipped that way
  by default), foreign scripts display (and edit) properly within the
  metadata editor as well as the catalog side bar. This sort of thing
  happened almost for free on the Mac.
* Enhanced the user interface of "kdu_winshow" and "kdu_macshow"; in
  particular, double-clicking an entry in the metadata catalog (or hitting
  enter while an entry is selected), as well as double-clicking within the
  image view itself, now allows you to not only highlight any relevant
  region of interest but also sequentially advance the image view through
  all of the image entities which are associated with the same metadata.
  Also, when navigating the image view based on metadata which is specific to
  an entire image entity (e.g., a compositing layer) in composited frame
  mode, the relevant image entity as a whole is highlighted in the display,
  unless it occupies the entire visible region.
* Modified the "-album" switch in the "kdu_merge" demo application so as to
  create multiple index pages (as required) at the front of a photo album
  and (more interestingly) to automatically add some initial metadata which
  can conveniently be edited within the "kdu_show" demo app to create
  interesting annotated photo albums.
* Fixed a couple of minor bugs in the implementation of `kdu_serve' which
  were accidentally introduced while bringing the behaviour into line with
  the current JPIP standard in v6.2 -- specifically, when byte limits are
  applied to a request, the attempt to satisfy the byte limits precisely,
  while counting the cost of variable length headers, had the potential to
  erase a section of bytes from a codestream header data-bin or to miss the
  delivery of some trailing bytes from a metadata-bin under unusual conditions
  (v6.2 only) and this has now been fixed.
* Fixed minor bugs in `kdu_region_decompressor' and `kdu_region_compositor'
  in which the role of the numerator and denominator variables was accidentally
  interchanged in each of the two `find_render_dims' functions, when
  calculating half-pixel shifts.  There is a slim possibility that this bug
  could indirectly trigger an access violation during vertical interpolation
  within `kdu_region_decompressor'.

Changes from version 6.1.2 to 6.2
---------------------------------
* Substantially re-implemented all the communication-related aspects of the
  "kdu_server" application, with two major implications:
  1) The new implementation is based on a collection of platform-neutral
     communication primitives, which allows the server to be deployed on
     Windows, Unix, Linux and Mac platforms in a uniform manner.
  2) The new implementation supports multiple JPIP channels within a session,
     allowing for much more efficient communication with clients that need
     to open multiple simultaneous windows into an image and manage multiple
     interactive windows of interest.  Previously, the only way to achieve
     this with the Kakadu server was to interleave requests from each window
     of interest.
  The new server design also solves a number of minor compliance issues with
  respect to the latest version of the JPIP standard (IS15444-9):
  a) No longer issues multiple "Request Pre-empted" EOR response messages
     within a single response chunk under the HTTP-TCP transport (in an earlier
     committee draft of the standard this was legal).
  b) No longer issues "JPIP-len" response headers as a means of changing a
     client's requested response length limit (such changes are not allowed).
  c) No longer ignores "pref" request fields.  The new implementation correctly
     responds to all preference requests.  It can also correctly handle
     bandwidth slicing and global bandwidth limiting preferences.
  d) Now performs hex-hex decoding of query strings on a field-by-field basis
     so that hex-hex encoded '&' characters (e.g. in a `target' string) have
     no opportunity to be mistaken for field separators.

* Completely re-implemented the `kdu_client' object, with the following
  two major implications:
  1) The new implementation is based on a collection of platform-neutral
     communication primitives, which allows the server to be deployed on
     Windows, Unix, Linux and Mac platforms in a uniform manner.
  2) The new implementation offers applications multiple request queues, each
     of which can have its own independent window of interest.  The client
     can manage any number of request queues, regardless of server
     capabilities (it can even be stateless) and for both the HTTP and HTTP-TCP
     transport protocols.  However, the client does ask for multiple JPIP
     channels from the server, and if the server grants them this generally
     makes the communication more efficient.  These features go hand in hand
     with the new Kakadu server to deliver the best performance for
     applications which require multiple windows of interest into a single
     image, video or other server-side resource.
  In addition, the new client implementation provides a number of less major
  enhancements:
  a) Status information associated with recent requests is now made available
     to the application.
  b) The application can now explicitly choose whether the client should be
     used for ongoing communication or just issuing a one-shot request.
  c) The client rigorously performs hex-hex of URL's or URL-components (e.g.,
     server name, resource, initial query fields) it receives from the
     application and hex-hex encoding of all relevant strings passed to the
     server so as to ensure strictly URI-legal behaviour -- the client
     API is carefully documented to make it clear what strings it expects to
     have contain URI-legal hex-hex encodings from the application.

* Augmented the `jpx_meta_manager' and `jpx_metanode' interfaces with the
  ability to manage JPX cross-reference boxes and to recognize cross-references
  to recognized metadata within the same source file as semantic links.  You
  can also create links, or copy them from another file's metadata via
  `jpx_metanode::add_copy' or `jpx_meta_manager::copy', and all the correct
  links will be discovered and even saved correctly as new cross-reference
  boxes in a generated JPX file.  These features work correctly even when
  cross-references are discovered (or copied) incrementally, which might
  happen when browsing a remote source via JPIP or when editing a source --
  the internal machinery is capable of deferring its discovery of links until
  all the relevant boxes are available.  Links provide a mechanism for
  creating rich metadata indices, cross-referenced against codestream/frame
  components, regions of interest, or each other, providing an interactive
  user with rich means for navigating a complex source.  All of this works
  correctly when the source is remotely located and served via JPIP.
  Kakadu interprets cross-reference links within a JPX file as belonging
  to one of three different semantic categories, based on how the link is
  formed -- grouping links; alternate child links; and alternate parent links.
  The Kakadu interpretation is natural with respect to the underlying JPX
  constructions and enables the introduction of functions which can query
  the relationship between arbitrary metadata items in a meaningful way.
  A generic function `jpx_metanode::find_path_to' is introduced to perform
  powerful relationship queries within the metadata graph, navigating links
  in a manner which is robust to self-referential loops.

* Extended the metadata overlays feature of `kdu_region_compositor' to support
  limiting the displayed regions of interest based on an arbitrary
  sum-of-products expression involving relationships in the metadata
  graph.  This feature uses the generic `jpx_metanode::find_path_to'
  function mentioned above, which is very efficient.  This allows you to
  interactively hide or unhide regions of interest based on their relationship
  to other metadata (e.g., text labels, index terms, etc.) in a natural
  way.

* Augmented the `jpx_metadata_manager::enumerate_matches' method with the
  ability to specify whether codestream or compositing layer constraints
  can be ignored for numlist boxes which do not specify any codestreams
  or compositing layers.  This feature is now used by `kdu_region_compositor'
  for determining the regions of interest which should be displayed as
  overlay information -- specifically, it allows regions of interest to
  be displayed only if they match the relevant JPX compositing layer, unless
  the associated numlist boxes do not specify any compositing layers.  The
  original JPX standard makes no comment on how the different types of
  entities in a numlist box should be interpreted when both codesteams and
  compositing layers are specified, but the new interpretation by
  `kdu_region_compositor' is more natural and useful than the previous one
  which displayed everything with a matching codestream, regardless of the
  compositing layer information provided by its numlist box.

* Improved the `kdu_region_compositor::search_overlays' function so that
  it returns a match only if the point of interest lies within the actual
  region of interest, as opposed to the ROI description box's bounding
  rectangle.  The enhanced implementation searches within the individual
  rectangular and/or elliptical regions of the ROI description box after first
  finding a match with the bounding rectangle..

* Extended the "kdu_macshow" demo applications in the following ways:
  1) Introduced full support for remote browsing of images via JPIP; in
     fact, JPIP browsing on the MAC is more exciting than on Windows machines
     with "kdu_show", since the MAC viewer allows multiple open windows to
     the same remote resource to share a common client-side cache and manage
     separate request queues for a separate interactive focus box in
     each window.
  2) Introduced a new feature which allows open windows to be duplicated
     (you can use the Command-D accelerator for this) and shrink wrapped
     around any focus box.  This allows you to rapidly create windows for
     regions of interest within an image, and works perfectly in conjunction
     with JPIP for remote image browsing.
  3) Added extensive support for editing, visualizing and navigating with
     metadata links (these are realized using cross-reference boxes, as
     described above).  The metadata editor itself and the metadata catalog
     side bar are considerably enhanced for this purpose.  For examples
     of richly annotated images built using these tools, visit the "Demos"
     section of the Kakadu web-site.
  4) Extended the metashow tool so that it properly displays non-ASCII
     UTF8 label content and so that it also displays the absolute file
     location of the header of each box.  When the image is accessed remotely
     via JPIP, the "metashow" tool also reveals the identifiers of
     the cache data-bins in which each box header (H...) and the box body
     contents (B...) are found -- shown in the box header within curly
     braces {}.  This is a most invaluable tool for identifying and debugging
     any problems with metadata communication by JPIP clients and servers.
  5) Improved metadata rendering in the "Catalog" sidebar by: a)
     properly supporting truncation of long non-Ascii UTF8 text labels; b)
     avoiding the instantiation of memory to represent labels until they
     are explicitly visualized within the catalog bar; c) colour coding
     different types of catalog entries; d) adding tooltips; e) adding
     an auto-collapse feature to prevent the catalog from becoming too
     unwieldly when auto-expansion of relevant metadata is triggered by
     user interactions within the image window; f) added features to
     automatically reorganize the prefix labels associated with large
     metadata collections, when changes of any type occur (e.g., editing or
     receiving data from a JPIP server); g) added a feature to stabilize
     the visual presentation of the catalog while changes occur in the
     background, so that a selected item always appears in the same visual
     position, even if the catalog structure is reorganized around it;
     h) added a feature to collapse catalog entries with the same name and
     data type (i.e., the same visualization) into a single entry, together
     with a visual position counter which the user can readily adjust using
     the arrow keys, in order to select any one of the visually identical
     catalog entries; i) added forward/backward buttons for navigating
     historically selected entries in the catalog (as found on all web
     browsers and most file-system browsers); j) gave the metadata catalog
     the ability to generate metadata requests, to be appended to imagery
     requests during JPIP image browsing (the JPIP implementation logic
     even issues metadata-only requests for high priority metadata updates
     required to keep the catalog responsive, whenever it has keyboard
     focus -- the catalog thus behaves almost exactly like a web-browser,
     except that content, requests and navigation are tightly coupled to
     the imagery).

* Completely overhauled the original Windows "kdu_show" application so
  that it support virtually all the same features as "kdu_macshow".  Along
  the way, the source structure has been completely reorganized so that
  it mirrors that of "kdu_macshow" and the Windows viewer is now renamed
  as "kdu_winshow".  Once compiled, both applications are known as "kdu_show"
  on their respective operating systems.  Unfortunately, a lot of things
  are quite a bit more difficult in Windows, compared to the Mac.  One
  most unfortunate thing is that Windows does not properly support
  international strings (foreign scripts) unless the entire application
  is compiled for UNICODE, which is a huge mess for applications which
  prefer to use ASCII text most of the time.  By contrast, on the MAC,
  everything can be handled beautifully as UTF-8, which is equivalent to
  UNICODE when strings are internationalized.  I tried to use unicode
  variants of important text rendering tools for "kdu_winshow" -- in
  particular, the catalog side bar text is first converted from UTF-8
  to unicode.  However, unfortunately, Windows needs unicode enabled deep
  down in the core (when Window classes are built) in order for its
  Windows to actually handle foreign character sets.  This is also
  OS-version dependent.  As a result, the new "kdu_winshow" application
  will display foreign scripts correctly in the catalog side bar when
  running under Vista -- but this had to be disabled under XP.  If only
  Microsoft had gone for UTF-8 rather than creating 2 parallel versions
  of all their GUI messages, controls, notification and a whole ton of
  standard C/C++ functions, selected globally at compile time.  16-bit
  unicode isn't quite complete anyway, so UTF-8 is a much better solution.
  Anyway, licensees can feel free to develop their own unicode-specific
  apps, since Kakadu itself does properly handle foreign scripts -- as
  demonstrated best by "kdu_macshow".

* Augmented the "kdu_merge" demo application with the ability to import
  metadata from one JPX file into another -- this is done with the new
  `-jpx_meta_swap' command-line argument which allows you to take imagery
  from one set of input files and metadata from another (potentially
  intersecting) set of files.  This is a very useful feature, since it
  provides you with a simple means of modifying the compressed imagery in
  a file while reusing all the existing metadata.

* Also augmented "kdu_merge" with the ability to import raw codestreams.
  This allows you to merge any number of raw codestream files into a single
  JPX or MJ2 file, or add raw codestreams to existing JPX or MJ2 files.
  You can also create JPX files which reference the original raw codestreams,
  JP2 or MJ2 files from which their content was sourced, via links.  Unlike
  other types of content which can be imported by "kdu_merge", the number
  of raw codestreams which can be imported in a single invocation of the
  tool is not limited by any operating system limits on the number of open
  file descriptors, so you can merge a massive number of them in one go.

* Increased the maximum line length for switch files ("-s") imported via
  `kdu_args' (all the Kakadu command-line demo executables use this) from
  2kB to 100kB, just to cover extreme cases in which machine-generated switch
  files are used to supply a massive number of arguments to one of these
  tools.

* Modified `kdu_cache::acquire_lock' and `kdu_cache::release_lock' so that they
  actually lock/unlock an internal mutex rather than leaving this to a derived
  object.  This way, all `kdu_cache' instantiations can safely add cache
  contents and retrieve cached data from separate threads, without having to
  bother with implementing exclusion devices in a derived object.  You can
  still override these function to implement your own mutex, which might be
  useful if for some reason you are compiling Kakadu on a platform with
  `kdu_mutex' does not do anything.

* Added a `kdu_threadsafe_family_src' class, derived from `jp2_family_src'
  which provides the mutually exclusive locking services of
  `jp2_family_src::acquire_lock' and `jp2_family_src::release_lock'.  This
  is not itself a big deal, but it encourages applications to use the
  thread safe version.  This is very important for applications which use
  a multi-threaded environment (`kdu_thread_env') to perform multi-threaded
  decompression and rendering functionality in the background while another
  thread (typically the main thread) accesses boxes via the `jp2_family_src'.
  This can happen somewhat transparently if you are using the services
  offered by `jpx_source' to dynamically parse the metadata of a complex
  file that might be ultimately sourced from a dynamically growing cache,
  as an example.

* Extended the core system `kdu_codestream_comment' interface and associated
  internal machinery to support binary codestream comments, in addition to
  ASCII text comments.

* Added support for the BigTIFF file format to the "kdu_compress" and
  "kdu_expand" demo applications by modifying Kakadu's `kdu_tiff' class
  to support both regular TIFF and the new BigTIFF format -- modifications to
  "image_in.cpp" and "image_out.cpp" to support BigTIFF were almost negligible,
  but modifications to the `kdu_tiff' class that they use for TIFF support
  were more substantial.  In any event, you can now read and write TIFF files
  much larger than 4GB in size.

* Added support for cropping input files on the fly in "kdu_compress". This
  has many potential uses, but perhaps the most important is the ability to
  perform fragmented compression of a massive input image (could easily exceed
  1 Terabyte) by simply invoking "kdu_compress" multiple times on the file,
  each time generating 1 or more tiles from the original input image,
  which incrementally build up the comprete implementation.  Using this
  "-icrop" option, together with "kdu_compress"s existing "-frag" option,
  you could, for example, compress a 1 Terapixel (1M by 1M) image with 256
  invocations of "kdu_compress", each generating a single 4 Gigapixel
  (64K x 64K) tile of the result, with say 12 decomposition levels -- this
  can be done on a regular desktop computer without much difficulty.  At the
  lowest viewing resolution, the entire image would have a dimension of only
  256x256 due to the use of only a very small number of tiles.

* Added an automatic interpolation/extrapolation procedure to "kdu_compress"
  to fill in missing distortion-length slope thresholds, if the "-slope"
  argument is used to supply fewer slope thresholds than there are quality
  layers.  The automatic algorithm is documented briefly in the usage
  statement.

* Slightly modified "kdu_compress" and "kdu_expand" so that they read/write
  XMP and IPTC metadata found in TIFF files and JP2 files, as suggested
  by Greg Coats.

* Added the resolution scaling adjustments suggested by Greg Coats to
  GeoJP2 information written back to TIFF files by "kdu_expand", so that
  any resolution reduction is taken into account.  Simultaneously modified
  the way in which display resolution attributes are adjusted, so that
  resolution information recorded in the output file (BMP or TIFF) remains
  correct under both resolution reduction and cropping to a region of interest.

* Introduced a new core `kdu_message'-derived object, `kdu_message_queue',
  which can be used (amongst other things) to build application-specific
  messaging services to be supplied to `kdu_customize_errors' and/or
  `kdu_customize_warnings'.  The new object takes some of the pain out of
  implementing robust error/warning handlers for multi-threaded environments.
  This is particularly useful, for applications in which messages can only
  be rendered within a specific thread -- e.g. in GUI environments with
  graphical services which are not completely thread safe.  Even thread-safe
  graphical environments can face difficulties if they try to render an
  error message within one thread which relies upon the existence of a
  window that might die in another thread.  The new object queues messages
  until they can be handled by the preferred thread.  In most cases,
  applications need only override one function
  (`kdu_message_queue::pop_messages') to do everything required -- render
  messages if in the right thread, or signal the right thread to do the same.
  The "kdu_show" and "kdu_macshow" applications both now use this object as
  the base for all their error/warning handling, which is robust to all kinds
  of multi-threadign issues.

* Added methods `jp2_output_box::get_box_length' and
  `jp2_output_box::get_header_length' which can be meaningfully invoked on
  a `jp2_output_box' immediately before or after closing it.

* Removed the older VC6 and .NET2003 project workspaces, since it
  is getting difficult to maintain them along with newer releases of the
  Microsoft development tools.

* Added a .NET2008 project workspace to complement the .NET2005 version.

* Set up compilation directives in such a way as to avoid the incorrect
  warnings issued by the .NET C# compiler when building Kakadu's managed
  native interfaces ("kdu_mni.dll") in the "managed" work space.

* Removed the DSTO-contributed port of the earlier Kakadu client-server
  technology to Unix, since the new `kdu_client' and `kdu_server'
  components are now platform neutral, and considerably expanded from
  their earlier realization.

* Fixed a bug in `kdu_cache' which prevented temporary holes in the
  available bytes for a data-bin from being correctly coalesced, when the
  missing data later appeared.  This was not previously all that important,
  because few if any applications caused holes to appear.  However, with
  the new server and client implementations, supporting multiple JPIP channels
  and associated parallel communication channels, data-bin byte ranges do
  frequently arrive in non-sequential order and need to be merged properly
  within the cache.

* Fixed a bug in `jp2_output_box::write_header', which caused boxes with length
  between 2^32 and 2^33 to be written with a truncated box length and also
  caused boxes with long headers to be written incorrectly if they were
  marked with `write_header_on_close'.  These errors may have impacted the
  generation of JPX and MJ2 files with very large embedded codestreams or
  (for MJ2) a large number of embedded codesteams.  Interestingly, the problem
  affected massive MJ2 files generated with "kdu_v_compress", for example, but
  not generally those generated with "kdu_merge" -- the difference lies in
  whether box header lengths could be known up front or had to be rewritten
  once the box was closed.

* Fixed a minor bug in `kdu_region_compositor' which caused errors when
  trying to render metadata overlays for extremely small elliptical regions
  of interest.

* Fixed a minor bug in `kdu_resolution::get_precinct_packets' which allowed
  it to return a negative number of `packet_samples' in some circumstances
  when working with compressed data sources which are interactive caches
  (almost invariably means JPIP communications).  This bug affected the
  performance of progress indicators in the "kdu_show" application during
  initial browsing.

* Fixed a bug in `kdu_client' which caused JPIP metadata requests which are
  expressed relative to a JPIP data-bin to write the data-bin id incorrectly
  in the request string.

* Fixed a bug in `kdu_serve' which could under some conditions cause the
  server to fail to honour its obligation under the JPIP standard, to
  transmit the entire contents of metadata-bin 0 to a client.

* Fixed quite a few subtle bugs in the metadata serving logic of
  `kdu_serve' (and associated `kdu_servex' support machinery).
  These bugs were not catastrophic, but could cause the server to
  produce much more metadata in response to a request than was actually
  required.  In particular: a) non-intersecting region-of-interest metadata
  may previously have been served in response to window-specific metadata
  requests; b) recursive JPIP metadata requests (those with a ":r" suffix)
  were previously served in a manner which recursed fully into the hierarchy,
  ignoring any requested depth limitations; and c) JPIP metadata-bin
  requests which specify a root data-bin were previously interpreted in such
  a way as to include the box whose contents are spanned by the data-bin,
  which is potentially much looser than the correct interpretation which
  applies such requests only to boxes found within the data-bin.
  It was previously very difficult to comprehensively test out these logic
  flaws, because Kakadu did not have a demo application which could
  illustrate rich metadata browsing alongside image browsing.

* Fixed a subtle bug in the `jx_metaloc_manager' which is used to
  implement dynamic discovery and recording of box locations in the
  `jpx_meta_manager' machinery -- this is critical for the discovery of
  links between elements in the metadata hierarchy via cross-reference
  boxes.  The bug was aroused only while testing dynamic delivery of
  cross-linked metadata via JPIP.

* Corrected deficiencies in the URL interpreting code used by `jpx_input_box'
  to transparently open external file references as if they were embedded
  JPX boxes.  Previously, URL's were stored and used in a very loose way,
  as arbitrary absolute or relative file paths.  Now, references to local
  files are stored as RFC2396-compliant URL's, with hex-hex encoding and
  a "file:///" prefix (protocol + empty authority).  Facilities are also
  provided to extract local file pathnames from ingested URL's, stripping
  the protocol prefix and performing hex-hex decoding, as required.  These
  features are offered via the new `jp2_data_references::get_file_url' and
  `jp2_data_references::add_file_url' functions and an enhanced version of
  `jpx_codestream_target::add_fragment'.  You can still read/write raw URL's,
  without error checking, but the new features enable you to properly write
  and/or ingest URL's which represent local file pathnames.  All the relevant
  demo applications implicitly or explicitly use these new features.  As a
  result, when writing JPX files with linked codestreams via "kdu_merge"
  or "kdu_show"/"kdu_macshow", the generated files contain more correctly
  encoded URL's for the file references, but when reading or serving files
  using any of the existing tools, loosely formatted URL's are still
  acceptable.  It is worth noting that applications based on older versions
  of Kakadu may not correctly read files with the new compliant URL
  formatting, but this cannot really be helped.

Changes from version 6.1.1 to 6.1.2
-----------------------------------
* Modified the order in which the EBX register is set inside __asm blocks
  within the "msvc_dwt_mmx_local.h" and "msvc_colour_mmx_local.h" source
  files, so that EBX is not altered until after the last stack variable has
  been loaded into registers.  This overcomes a potential problem created by
  the VC 9 compiler in .NET 2008, when compiling the Microsoft inline assembly
  code for debug mode -- it appears that the compiler uses the EBX register
  for stack frame manipulation.

* Corrected an oversight in v6.1 and v6.1.1 whereby the names of the
  "kdu_aux" DLL produced by the "managed" Microsoft build environments was
  left as "kdu_a60.dll" instead of "kdu_a61.dll". This had no consequence on
  functionality or usability, though.

* Corrected an error in the marshalling of boolean arrays when building
  Java native interfaces.

* Corrected an error in the destruction of `kdu_params' objects within the
  core system, which could result in a memory leak for codestreams with
  component-specific and tile-component coding parameters.

Changes from version 6.1 to 6.1.1
---------------------------------
* Introduced almost immediately after version 6.1 to fix the following minor
  issues:
  -- Failure to build properly under XCODE 3.1 on MAC platforms due to
     identification of the PIC register %rbx as clobbered in "kdu_arch.cpp".
     Problem fixed by saving temporarily to another register.  May possibly
     have caused problems on other 64-bit Unix builds.
  -- Difficulty building "kdu_macshow" for backward compatibility with
     OS-X 10.4, due to changes in the Cocoa interface definitions between
     10.4 and 10.5 SDK's.  This has been resolved by including some
     conditional compilation directives.
  -- Minor changes to documentation, help and MAC package maker files.

Changes from version 6.0 to 6.1
-------------------------------
* Provided a complete set of XCODE build environments for the MAC, to
  complement the existing Makefiles and Microsoft Visual Studio build
  environments.

* Dramatically improved the metadata editing capabilities of "kdu_show",
  while adding new options to save edited files and maintain as much
  metadata as possible in JP2 files (JPX is, of course, the preferred
  format to save images with rich metadata).

* Introduced a new viewing utility, "kdu_macshow", for the MAC (runs under
  OSX 10.5 on G4, G5 and Intel processors; should also run under OSX 10.4).
  This utility is more elegant than its long standing Windows cousin,
  "kdu_show".  It contains all the same features as "kdu_show" (with the
  exception of JPIP support, which will be enabled in v6.2), but adds
  automatic metadata cataloging and navigating features for JPX sources,
  manages multiple open windows and allows for synchronized commands to be
  delivered to multiple windows (e.g., start playing video in all windows at
  once).  It uses mostly the same accelerator keys as "kdu_show".
     It is worth mentioning that the "kdu_macshow" application is built using
  Kakadu's platform independent API's, together with Cocoa (basically,
  NextStep).  As a result, it seems likely that interested parties could port
  the application quite easily to Linux and other environments, via GnuStep.

* Improved JPX metadata management considerably, as follows (these features are
  all used by the new "kdu_macshow" application and, to a lesser extent by
  "kdu_show", for sophisticated metadata editing, navigation and integration,
  not previously offered by a Kakadu viewer).
  1. Introduced new member functions to `jpx_metanode' to make metadata editing
     more convenient.  These allow the type and contents of an existing node to
     be changed and the parent of a node to be changed.  Previously, these
     operations required sub-trees of the metadata hierarchy to be copied, in
     order to preserve all nodes other than the one that was being changed.
  2. Altered the internal operation of `jpx_meta_manager' so that metanodes are
     not physically deleted until the `jpx_source' or `jpx_target' object
     itself is destroyed.  This provides increased robustness to
     editing applications which might not take care of eliminating
     references to objects which have been deleted.  More importantly,
     it allows applications to explicitly retain references to nodes
     which have changed or been deleted and to discover such facts by
     calling `jpx_metanode::is_changed', `jpx_metanode::is_deleted' or
     `jpx_metanode::parent_changed'.
  3. Added a facility to keep track of original file locations (or
     copy source) associated with the metadata managed by
     `jpx_metanode' objects, providing the application with the
     ability to efficiently locate metanodes based on the original box
     locations (see `jpx_meta_manager::locate_node').  In the future,
     these same internal mechanisms will be used to recover the semantic
     associations (as links) implied by the use of cross-reference
     boxes and to preserve them through editing and copy operations so
     that interactive users can add, remove and navigate such links.
  4. Modified the code which reads metadata into a `jpx_source' object so
     that errors encountered in non-essential metadata can be non-fatal,
     just eliminating the affected metadata nodes from the generated
     metadata tree.
  5. Added a facility to allow applications to save an arbitrary state
     reference internally with each metadata node, to allow them to
     conveniently reconcile changes in the metadata structure (due to editing
     or JPIP delivery) with application-defined metadata structure.
  6. Added a service to efficiently and conveniently identify the set of
     metadata nodes which have been changed, deleted, added, or recently
     parsed into a `jpx_source' (e.g., because they became available in a
     dynamic cache, during JPIP browsing).  The application can now efficiently
     scan all such newly available nodes using the
     `jpx_meta_manager::get_touched_nodes' function.

* Modified the "kdu_compress" and "kdu_expand" applications to allow images
  with a given declared sample data precision to be read or saved as though
  they had a different sample data precision.  One reason for doing
  this is to overcome a weakness in the support offered
  by some third party TIFF reading/writing applications, in not properly
  supporting the packing/unpacking of sample values with non-power-of-two
  precisions.  Another reason for adding these features is that 16-bit TIFF
  files are commonly used to store data with substantially lower precision
  (e.g., only 11 or 12 bits), which will appear almost entirely "black"
  after regular compression with Kakadu (since all the values are close to
  the lower bound of the declared sample data range).  You can now conveniently
  instruct "kdu_compress" to treat the file as though the sample values had
  a lower precision.
     These modifications have nothing really to do with Kakadu
  proper, since TIFF file reading/writing in "kdu_compress" and "kdu_expand"
  is just for demonstration purposes, but a number of users requested these
  capabilities.  Be sure to carefully read the usage statement for the new
  `-fprec' (force precision) argument provided with each of "kdu_compress"
  and "kdu_expand".

* Slightly modified the summary data printing portion of the "kdu_expand" and
  "kdu_compress" demo applications so that they print large numbers
  (kdu_long's) in a nicer way.  This is achieved by augmenting the core
  `kdu_message' object with a special operator<< overload for the type
  "kdu_long".

* Provided a new core system feature to keep track of the number of compressed
  bytes in each quality layer, in each tile, image component and resolution.
  This can help applications to make intelligent choices regarding the number
  of quality layers they might choose to decode, where speed is particularly
  important.  The new feature centres around the function
  `kdu_tile::get_parsed_packet_stats', which comes with extensive comments
  to explain performance implications with different types of codestreams and
  compressed data sources.  The "kdu_expand" application is also augmented with
  a \"-stats\" argument to demonstrate the collection and printing of such
  parsed packet statistics.

* Augmented `kdu_codestream::flush' with the ability to impose constraints
  not only on the overall compressed size of each quality layer, but also on
  the size of the quality layer at each resolution and/or each leading subset
  of image components.  This is useful primarily for ensuring the generation
  of legal codestreams for Digital Cinema applications. The new feature is
  conveniently controlled via the codestream parameter system, using various
  flavours of the "Creslengths" parameter attribute.  This allows the feature
  to be added to just about any existing application without adding any
  (or hardly any) lines of code.  Along with this new feature, support for the
  Digital Cinema profiles CINEMA2K and CINEMA4K has been greatly upgraded,
  to include proper checking for legal digital cinema codestreams and
  automatic selection of suitable coding and structural parameters, where
  defaults must be supplied.  For example, you can now generate a legal 4K
  24fps digital cinema codestream using the following quite straightforward
  invocation of the kdu_compress demo app:
    >> kdu_compress -i in.tif -o out.j2c Sprofile=CINEMA4K \\
       Creslengths=1302083 \
       Creslengths:C0=1302083,1041666 \
       Creslengths:C1=1302083,1041666 \
       Creslengths:C1=1302083,1041666

* Significantly improved robustness of the core system to illegal or highly
  unusual conditions which might require the allocation of massive amounts
  of memory.  The core system should be able to correctly clean itself up
  (assuming the application calls `kdu_codestream::destroy') even after
  throwing an exception from a call to `new' which exceeds the available
  memory.

* Modified the function `kd_tile::read_tile_part_header' to check for
  tile-parts whose total length is 12 (an illegal value) and change it to
  14.  Lengths of 12 were accidentally recorded by Kakadu versions 6.0 and
  earlier, for empty tile-parts, when generating TLM information.

* Fixed the above-mentioned problem with creation of empty tile-parts with
  the illegal length value of 12 (changed to 14).

* Modified `kdu_resolution::get_precinct_packets' to take a `kdu_thread_env'
  pointer as its second (optional) argument, which is relevant only if the
  `parse_if_necessary' argument (previously the second, optional argument)
  is true.  This modification is deliberately intended to generate compilation
  errors for any application which was previously using the function in a
  manner which was potentially not thread-safe.  Read the interface description
  for more information.

* Fixed a bug in `kdu_codestream::trim_compressed_data' which has been in the
  core system since Part 2 arbitrary decomposition styles were introduced in
  version 5, but was only detected in version 6.0.  This bug could have
  potentially caused a memory access violation, but apparently hardly
  ever did so.

* Fixed a minor core system bug in `kd_thread_local.h', in which thread queue
  memory was released using `delete' rather than `free'.

* Fixed a long dormant bug in `kd_marker' which could manifest itself when
  a `kdu_codestream' object is restarted with a new set of coding parameters
  and an error is subsequently encountered in error resilient mode.

* Eliminated a potential race condition in the dereferencing of precincts
  which are candidates for recycled within the core system (one thread might
  recycle them while another is testing to see if they can be safely accessed)
  by using a volatile reference.

* Augmented the descriptions of `kdu_codestream::destroy',
  `kdu_codestream::share_buffering' and `kdu_codestream::open_block' to
  spell out the potential pitfalls associated with sharing buffering
  between multiple codestreams in multi-threaded mode (such as when one
  codestream is destroyed while another is still in use by a different
  thread).  Of course, applications which might do this are probably quite
  rare.

* Very slightly modified the return condition for
  `kdu_region_compositor::process' so that it adheres precisely to the
  explanation given in the header file (and hyperdoc API documentation).

* Fixed a more serious bug in `kdu_region_compositor', which caused it to
  generate a memory fault if a codestream, previously used to write alpha
  data for a compositing layer was recycled for use in single-component
  viewing mode.

* Fixed some bugs in the `jpx_meta_manager' and associated objects
  (i.e., `jpx_metanode').  Internally, there was a bug in the counting of
  descendants which could cause an error with some usages in the past.
  There was also an error in `jx_regions::write' which caused all ROI
  description boxes to be written with a height equal to the width.  These
  bugs have now been fixed.

* Fixed a bug in `kdu_client' (and the DSTO Unix/Linux port thereof) in which
  100-series responses (e.g., HTTP 1.1 "100 Continue" responses from
  intermediate proxies) were not properly passed over.

* Incorporated temporary fixes provided by a third party into the original
  DSTO Unix/Linux port of the "kdu_server" utility, so as to accommodate
  more than two clients.  The original Windows version of "kdu_server" is
  still the most robust implementation, until its methods get properly
  ported to a single more platform neutral realization (expected in
  Kakadu v6.2).

Changes from version 5.2.6 to 6.0
---------------------------------
* Added fast SIMD DWT and colour transformation code for the 32-bit
  precision sample processing path, which largely mirrors that already
  available in the 16-bit precision sample processing path.  This code
  can greatly accelerate high precision image compression/decompression,
  particularly at low bit-rates.  The accelerations are also available to
  DWT-based Part 2 multi-component transorms, which are more likely to
  require higher precision implementation.  These extra accelerations are
  available only on X86 platforms with SSE2 support or above.  Moreover,
  to gain access to the full set of accelerations, you should compile with the
  `KDU_X86_INTRINSICS' symbol defined.  This is automatic for 64-bit
  Windows/Linux/Mac compilations.
* Introduced additional methods on the core `kdu_thread' object, to support
  the assignment of thread priorities and the binding of threads to specific
  CPU sets (CPU affinity) in a platform independent way.  CPU binding can
  also now be optionally specified in calls to `kdu_thread_entity::create'.
* Added a new core codestream management feature which allows decompressed
  image quality to be traded for computational speed by stripping away final
  coding passes from selected code-blocks.  This has a similar effect to
  discarding quality layers, but works even when the original codestream
  was created with only one quality layer.  The mechanism is non-destructive
  so that the same codestream can be successively decompressed with different
  "truncation" policies, drawing from a single "persistent" `kdu_codestream'
  manager.  The feature is accessed via `kdu_codestream::set_block_truncation',
  which may be called at any point, allowing the quality/speed tradeoff to
  be managed dynamically, even while a single image is being decompressed.
  The feature may be demonstrated by the new "kdu_vex_fast" demo application,
  described below.
* Introduced several efficiency improvements to the management of
  compressed data in the core system, with the upshot that a speed
  improvement of several % is achieved when working with high bit-rate
  imagery, particularly if compressed data sources offer the new
  `KDU_SOURCE_CAP_IN_MEMORY' capability -- see the comments appearing
  with `kdu_compressed_source' for a full description.
* Substantially modified the `kdu_region_decompressor' object which is
  also the workhorse of the `kdu_region_compositor' object, so that these
  objects are both able to process horizontal strips of tiles together,
  in each call to their respective `process' functions.  This speeds up
  rendering of heavily tiled images, while the number of tiles processed
  together is still regulated so that limits on the amount of processing
  done in each call to `process' are respected.  For images with large
  tiles, the function still processes a user-controlled number of lines
  of one tile in each `process' call.  Along with this improvement in
  tile processing, the `kdu_region_decompressor' (and hence
  `kdu_region_compositor') object is now able to process multiple tiles
  in parallel, on machines with multiple CPU's (in addition to processing
  code-blocks within a tile in parallel); moreover, it is able to
  automatically start processing code-blocks in a new tile or strip of
  tiles once a current tile or strip of tiles nears completion so that
  some CPU's might otherwise become idle.
* Augmented the capabilities of the `jp2_input_box' and `jpx_input_box'
  objects to support pre-loading of their entire contents into memory
  via the new `load_in_memory' member function.  This method may be used
  to provide a compressed data source which offers the new
  `KDU_SOURCE_CAP_IN_MEMORY' option for the most efficient handling of
  compressed data throughout the core codestream management sub-system.
  The new capability is of principle interest for high performance
  video applications, including digital cinema.
* Augmented the "kdu_v_expand" demo application to offer a new "-in_memory"
  command-line option, which exercises the capabilities described in the
  above two bullet-points.  This allows you to explore the impact of
  in-memory compressed data sources on video rendering speed.
* Introduced a new demonstration application, "kdu_vex_fast", whose purpose
  is to demonstrate the fastest possible means of rendering JPEG2000
  compressed video content for real-time applications, including
  software-only digital cinema.  When compiled on Win64 and Win32 platforms,
  this application provides a DirectX 9 display interface with real-time
  frame rate control for high quality tear-free display.  If you don't
  have DirectX 9 support on your platform, you should undefine "KDU_DX9"
  when compiling this application.  When you do this, or when you compile
  under GCC, you get everything except display.  The application can
  write decompressed data to VIX files, exactly like "kdu_v_expand".  Also,
  as for "kdu_v_expand", omitting the output file from the command line
  argument allows you to assess the true decompression speed, without
  the limitations of file writing -- no other steps are skipped when you
  omit an output file name.  This application uses in-memory data sources
  and provides you with the flexibility to choose how processing threads
  will be distributed between frame processing engines and whether or
  not they should be tied to specific CPU's in NUMA environments -- see
  the "-usage" statement under the heading "-engine_threads" for more on this.
* Included a Unix/Linux port of the "kdu_server" utility, based on a port
  originally provided by Australia's Defense Science and Technology
  Organization (DSTO).  In the future, this port may be absorbed into
  the regular "kdu_server" application, but for now the ported files and
  relevant Make files are located under the separate "contrib" directory.
* Modified the implementation of `kdu_rijndael' in "kdu_security.cpp", fixing
  a minor error in the interpretation of the AES standard (not that it really
  matters for the way Kakadu uses AES) and correcting a platform byte order
  dependence, so that "kdu_server_admin" and "kdu_server" can interact
  successfully across different platforms; this is important now that the
  server can be compiled to run on Sparc and other processors which use a
  big-endian byte order.
* Modified "kdu_hyperdoc" to correctly cross-reference all global
  constant definitions, so as to make their accessibility and use more
  obvious, particularly for Java and C#/VB developers.
* Modified the way "kdu_hyperdoc" builds the "kdu_jni.cpp" source file
  for Java bindings, so that the class loading code is protected by a
  global mutex.  It is unclear whether any race conditions have ever
  been observed within the 3 machine instructions where the class loading
  code is potentially vulnerable; however, the mutex protection should
  guarantee that such race conditions are not possible.
* Added two new low level classes `kdu_compressed_source_nonnative' and
  `kdu_compresssed_target_nonnative' which can be inherited by foreign
  language classes (typically Java and C#) to provide fully custom
  compressed data sources and targets in those languages -- e.g., memory
  mapped compressed data sources/targets.
* Modified the make "managed/make" makefiles so as to put all
  relevant kakadu object code into a single shared library for both
  "libkdu_jni.so" and "libkdu_a60R.so", so that these libraries can
  be imported more reliably by other applications, such as JVM's (for
  the JNI bindings).  Moreover, special precautions are now taken to
  ensure that these libraries do not use X86 compiler intrinsics on 32-bit
  Linux platforms, due to the stack alignment problems which can arise.
  At the same time, though, the regular applications built with GCC do
  get to benefit from the most comprehensive collection of processor
  speedups, which are implemented via the X86 compiler intrinsics.
  The -fPIC option is now used on all relevant builds, which should also
  maximize portability.
* Created top-level build environments for Linux, MAC and Solaris operating
  systems, which build all applications, libraries and managed interfaces
  in one hit.  These are found under the new top-level "make" directory.
  Moreover, the Makefile-MAC-x86-all" makefile builds both 32-bit and 64-bit
  binaries for Intel MAC's and then joins the 32-bit and 64-bit JNI library
  into a unified library which can be used from both 32-bit and 64-bit
  JVM's -- this is done in preparation for the arrival of Leopard, in case
  it has a 64-bit Java Virtual Machine.
* The "managed/make" makefiles and the .NET build environment in the
  "managed" directory all now find the base location of the Java SDK
  by expanding an assumed environment variable "JAVA_HOME", which may
  well already be defined on your system.  This saves you having to
  modify the build environments for Java with each new Kakadu release.
  See the "Compilation_Instructions.txt" file for more on "JAVA_HOME".
* Added a "-com" command-line argument to the "kdu_compress" demo application
  which allows one or more user-supplied COM marker segments to be inserted
  into the codestream.
* Added a "-cpu" option to "kdu_render", which can be used to properly
  evaluate the computational throughput of the `kdu_region_decompressor'
  object, which is central to image rendering with Kakadu.  The speed of
  this object is identical to that of `kdu_region_compositor' for simple
  (non-composit) images.
* Fixed a subtle bug in the core system, which could have caused
  spurious error messages during codestream parsing (e.g.,
  "Illegal inclusion tag tree encountered ...").  This problem was
  occasionally reported with large tiled imagery, in conjunction with
  "kdu_server", but not previously resolved.  The errors were caused by the
  interaction of one non-fatal bug with one subtle oversight.  The non-fatal
  bug is that recycled typical tile resources did not have their
 `kd_precinct_pointer_server' object's re-initialized so that random
  access packet length information in the codestream was ignored.  This
  left the situation where some tiles could have seekable precincts, while
  others do not -- something that could happen anyway in legal, but
  unreasonable codestreams.  The fatal problem occurred when random
  access was made to precincts of one tile, while another purely sequential
  tile was still being actively parsed.  This type of problem could have
  been excited by the "kdu_show" or "kdu_server" demo applications only.
* Fixed a bug in "kdu_threads.cpp", which caused any dormant queues to be
  incorrectly shutdown by calls to `kdu_thread_entity::terminate' and
  `kdu_thread_entity::destroy'.
* Fixed a subtle bug in the core system functions `kd_tile::initialize'
  and `kd_tile::recycle', in which `sum_depths' was downshifted by 2 instead
  of 1, leading to incorrectly computed quantization step sizes for
  irreversible processing with "Cderived=yes".  Interestingly, this bug
  virtually never caused incorrect decompression, since an exact power of
  2 error in the quantization step size is always compensated by a
  corresponding incorrectly computed K_max value, representing the number
  of nominal bit-planes in a subband's code-blocks.  Moreover, the error
  can only occur in the higher frequency subbands.  Nevertheless, this bug
  may have caused incorrect treatment of ROI regions if the "Max Shift"
  ROI encoding method had been used with derived quantization.  This bug
  was accidentally introduced with the introduction of Part-2 quantization
  kernels and found by Roddy Shuler.
* Fixed a bug in the implementation of the `kdu_event' platform-independent
  synchronization object, for the case of auto-reset events on pthreads
  platforms (i.e., Unix/Linux and Mac OS) -- this problably had no impact
  on pre-existing applications.
* Identified and resolved another obscure bug with the way in which
  non-symmetric DWT kernels (only allowed in JPEG2000 Part-2) are handled
  in the case of certain boundary conditions.
* Fixed two non-compliance problems with the way Kakadu handles
  Part-2 multi-component transforms.  The first problem relates to an
  oversight in previous versions of Kakadu, whereby the presence of
  matrix- and/or dwt-based multi-component transforms was not signalled
  in the COD marker segment -- this is an entirely redundant signalling,
  but required by Part-2 of the JPEG2000 standard.  The second problem
  is that previous versions of Kakadu stored the transform coefficients
  for reversible matrix-based (SERM) multi-component transforms in a
  transposed order from that specified by the standard; the problem in
  this case originated from the fact that the main figure in
  Annex J of IS15444-2 suggests the transposed order, whereas the true
  order is only revealed by the order of subscripts in the equations.
  To fix these compliance problems, while remaining as compatible as
  possible with past versions of Kakadu, the following steps have been
  taken:
  1) A new `Cmct' coding parameter attribute has been introduced to the
     `COD_params' object, which reflects the required MCT-dependent
     modifications to the COD marker segment for Part-2 codestreams.  This
     parameter attribute is automatically generated during finalization
     of the `COD_params' object(s) so you don't need to explicitly worry
     about it.
  2) A new `MATRIX' option has been created for the `Mxform_blocks'
     attribute, to be used in describing matrix-based multi-component
     decorrelation transforms, in place of the old `MAT' option.  In source
     code, these correspond to `Cxform_MATRIX' and `Cxform_MAT', respectively.
     You should no longer use `Cxform_MAT'.  If you have an existing
     application which uses `Cxform_MAT' with Kakadu, it will continue
     to compile successfully, but you will receive an informative error
     message when you attempt to generate a codestream using this option.
     This is our way of making you aware of the need to transpose any
     reversible multi-component matrix transform coefficient array that
     you might be using in an application compiled against previous
     versions of Kakadu.
  3) Codestreams generated by previous versions of Kakadu, including
     reversible multi-component matrix decorrelation transforms can still
     be successfully decoded; indeed, the relevant transform blocks will
     show up as the old type `MAT' (`Cxform_MAT') in place of the new
     `MATRIX' (`Cxform_MATRIX') if you inspect the coding parameter
     attributes explicitly after the codestream has been ingested.  Kakadu
     detects the presence of the old-style non-compliant conventions by
     observing the absence of the `Cmct' parameter attribute when a
     multi-component transform is being used.  It should be noted, however,
     that other applications (other than Kakadu) are unlikely to correctly
     decompress codestreams generated by previous versions of Kakadu
     which incorporated reversible matrix-based multi-component transforms.
* Fixed a bug in the `kdu_thread_env' core multi-threading framework which
  allowed race conditions when multiple output codestreams are repeatedly
  opened and closed, with a single multi-threaded environment processing
  those codestreams.  Along the way, the implementation has been improved
  so that it is no longer strictly necessary to terminate all thread queues
  via the `kdu_thread_entity::terminate' function before destroying any
  codestream, so long as all work on that codestream has ceased -- in
  particular, this means that it is sufficient to wait for just those
  thread queues which are associated with a codestream to terminate, while
  other threads may be getting on with processing other codestreams; this
  is true for input, output and interchange codestreams.
* Fixed a bug in "kdu_serve.cpp" which could produce a divide-by-zero error
  for certain illegal JPIP requests, thereby compromising the integrity of
  the server application.
* Fixed another bug in "kdu_serve.cpp" which prevented JPIP "metareq"
  requests which contain box-type wildcards (*) from being correctly processed.
* Fixed a bug in `kdu_client' which caused the wrong JPIP syntax to be
  used for meta-data-only requests -- the server was changed to support the
  correct syntax back in early 2006, but the corresponding fix in the client
  was long overlooked.
* Also fixed an error in the way `kdu_client' communicates with HTTP
  proxies, so as to properly conform to HTTP/1.1 conventions.
* Fixed a bug in the TIFF reading code used by the "kdu_compress" demo
  application, which causes LZW compressed tiled images to be read
  incorrectly.  Problem reported by Greg Coats, with fix provided by
  Margaret Lepley.
* Fixed a bug in the server delegation code in Kakadu's "kdu_server" app.
* Fixed a bug in the implementation of `kdu_window::contains' so as to
  avoid possible erroneous treatment of novel client windows as non-novel
  in either the server or the client.
* Fixed a couple of bugs in rarely used data processing paths within
  `kdu_region_decompressor', which may have affected applications with
  high rendering bit-depth requirements.

Changes from version 5.2.5 to 5.2.6
-----------------------------------
* Fixed a minor bug in `tif_in::tif_in' within "image_in.cpp" which
  caused Planar Configuration TIFF files to be misread on some
  platforms, depending on how uninitialized member variables of
  classes are treated.
* Fixed a bug in `kdu_region_compositor::find_point' which could cause
  dereferencing of a NULL pointer when trying to match a screen location
  to a codestream during JPIP browsing, when the server has sent enough
  information to identify the compositing layer, but not enough information
  to identify its codestreams.  This is a very rare condition, but
  could be excited in the "kdu_show" application by depressing "ctrl"
  in the early phases of browsing a large JPX photo album.
* Made the destructor for `kdu_cache' virtual -- this was an oversight,
  since derived objects do have virtual destructors.
* Extended the cases under which tiles are considered "typical" for the
  purpose of recycling their structures.  This helps speed up the
  decompression of heavily tiled images at reduced resolution, where
  each tile can be very small.  The new conditions help with cases
  where the tiles may have different quantization or ROI parameters.
  In the process, a potentially bug-prone condition was uncovered
  and fixed.

Changes from version 5.2.4 to 5.2.5
-----------------------------------
* Fixed some backward compatibility problems with Visual Studio 6
  in the way two new message handlers were added by .NET to the
  "kdu_show" demo app.
* Fixed some minor problems with the "managed/managed_2005.sln" workspace
  and related projects, for Visual Studio 2005 users.
* Fixed a minor initialization problem in the "kdu_stripe_compressor" and
  "kdu_stripe_decompressor" objects, which could impact applications
  relying on the `get_recommended_stripe_heights' function.

Changes from version 5.2.3 to 5.2.4
-----------------------------------
* Fixed a minor bug in "kd_tcp_transmitter::configure_flow_control"
  which caused the function's arguments to be ignored.  This function is
  used to adjust min/max RTT times based on command-line instructions to
  the server.
* This version comes with separate build environments for Visual Studio
  .NET 2003 and Visual Studio .NET 2005, the latter including both Win32
  and Win64 build configurations.  Some very minor changes were introduced
  into the code to eliminate errors/warnings during Win64 builds.
* The "kdu_hyperdoc" utility has been upgraded to allow conformance with
  either the older style (V1) or the newer style (V2) syntactic conventions
  for Microsoft's Managed Extensions to C++.  The newer style conventions
  are used by default, but these are appropriate only when building with
  Visual Studio 2005.  To access the older style conventions, for builds
  under Visual Studio 2003, use the "-old_managed_syntax" argument.  If
  you use the standard build environments ("managed_2003.sln" or
  "managed_2005.sln") everything is built automatically for you, using
  the appropriate conventions.
* The multi-threading environment managed by `kdu_thread_env' now comes
  with support for the maintainance of initially dormant queues, which
  are automatically moved into the foreground for processing once the
  system seems to have entered a state in which the available processing
  threads will otherwise be permanently under-utilized.  This can be used
  to build multi-threaded applications which keep all physical processors
  active even more of the time than with previous versions (which were
  already very good at utilizing available processing resources).  The
  new capability is described in connection with the optional
  `bank_idx' argument accepted by the `kdu_thread_entity::add_queue' function.
  So far, this capability is exploited only by the "kdu_v_expand" demo
  application, which offers a new "-overlapped_frames" command-line
  argument.  Try playing around with various combinations of the
  "-overlapped_frames" and "-double_buffering" arguments on your
  multi-processor system (particularly if it has more than 2 CPU's).

Changes from version 5.2.2 to 5.2.3
-----------------------------------
* Modified the CPUID testing code in "kdu_arch.cpp" yet again, this time
  so as to protect the EBX register on x86 platforms, since that register
  is reserved for position independent code when compiling with the "-fPIC"
  option under GCC, and marking it as a clobber variable does not work in
  that context.
* Modified the tile-part header marker segment reading code in
  "kdu_compressed.h" to ignore tile-part numbers, since Adobe encoders
  get them wrong, causing premature termination of the decoding process
  in compliant decoders.
* Modified the `kdu_thread_env' thread management code to allow multiple
  codestreams to be processed simultaneously by a single Kakadu multi-theaded
  environment.

Changes from version 5.2.1 to 5.2.2
-----------------------------------
Minor changes as follows:
* Arranged for .NET build environments to write the debug ".pdb"
  files associated with the core system (kdu_v52D.pdb) and
  managed DLL's (kdu_a52D.ddb, kdu_jni.pdb and kdu_mni.pdb) into
  the "bin" directory instead of the temporary "v5_generated"
  directory, so as to facilitate debugging from other workspaces.
* Added the "KDU_AUX_EXPORTS" to the virtual functions which are
  offered by the "kdu_a52?.dll" auxiliary DLL, in addition to the
  existing non-virtual functions.  This allows Windows users to
  access these functions directly, rather than having to indirectly
  access them through function pointers.  No impact on Unix builds.
* Corrected the source of compilation warnings in GCC regarding
  base functions hidden by derived objects.
* Corrected and expanded the set of foreign language member access
  functions provided for the `kdu_sampled_range' object.
* Added an optional argument to `kdu_region_compositor::set_buffer_surface',
  allowing you to set (or change) the background colour (and transparency)
  of the rendering surface onto which imagery is composited.  The value
  is relevant only when imagery does not fully cover the compositing
  surface or when it is partially transparent.  The default (backwards
  compatible) value is an opaque white background.  You can use this
  new feature to blend rendered imagery onto another buffer managed
  by the application.
* Added `add' and `subtract' member functions to the `kdu_coords' object,
  as substitutes for the `operator+=' and `operator-=' functions which
  cannot be exported to foreign languages.

Bug fixes as follows:
* Fixed a bug in "kdu_server" which could cause the server
  to hang when used with delegation over HTTP transport channels.
* Fixed a bug in the core system which could cause a crash in the
  event that a tile had only empty tile-parts, when working in
  random access mode with seekable codestreams.
* Fixed a minor bug in `kdu_region_decompressor' caused by
  `post_convert_colour' and `pre_convert_colour' not always being initialized.
* Modified the test inside an "assert" statement found inside
  `kd_decoder::init' and `kd_encoder::init' to allow for subbands of 0
  width in multi-threaded processing -- no impact on release code used in
  final applications.
* Fixed a couple of bugs in "kdu_threads.cpp" which manifested themselves
  only when multiple synchronization conditions were simultaneously
  installed in a multi-threaded system.  This condition typically only
  happened when incremental flushing was used with a multi-threaded
  compressor, so that the synchronized flush worker job and the synchronizing
  event associated with closing down a tile processing engine could be
  registered simultaneously (depending on the circumstances).  This and
  related problems should now all be fixed.
* Fixed a minor bug in the incremental flushing routine implemented in
  the `kdu_stripe_compressor' object, which manifest itself only in
  multi-threaded environments.  The implementation has also now been
  made more efficient.
* Fixed a minor bug in the use of the `Kdu_coords.Minus' function in
  "KduRender2.java" and "KduRender2.cs" (Java and C#) demo apps, which
  affects imagery not centred at the origin.
* Made minor fixes to the code executed during Windows compilations with
  KDU_NO_SSE defined.

Changes from version 5.2 to 5.2.1
---------------------------------
This is just a bug fix release, as follows:
* Fixed a bug in "kdu_hyperdoc" which caused it to generate duplicate
  delete statements for local copies of size-incompatible buffers
  passed across JNI interfaces.
* Fixed a bug in the `kdu_region_decompressor' object which could
  cause it to crash when non-initial components are used in isolation
  from a codestream.  This bug was accidentally introduced in v5.2 by code
  which tests for the usability of some accelerated buffer manipulation
  paths.
* Fixed a remaining bug in Kakadu's server delegation feature, for the
  HTTP-TCP protocol.  The bug was in the way the `kdu_client' object
  processed notifications of server address changes.
* Fixed a couple of GCC complaints in relation to virtual objects with
  non-virtual destructors -- these were for objects which have no
  real destructors and are never actually derived.
* Fixed remaining compiler warnings for WIN64 builds.
* Modified the implementations in the X86 intrinsics headers to provide
  pure SSE/SSE2 implementations of all processor speedups, with a compiler
  switch to disable all of the older 64-bit MMX speedups.  This is done
  to cater for WIN64 builds which refuse to link 64-bit MMX instructions
  (possibly because WIN64 does not save 64-bit MMX state during context
  switches -- 64-bit LINUX has no such difficulties mixing SSE/SSE2 and MMX).
* Fixed the Macintosh makefiles (for both PowerPC and X86 processors) so
  that they build dynamic rather than static libraries, and hence can be
  immediately used with Kakadu's Java native interfaces.  The new makefiles
  were developed in collaboration with Greg Coates.
* Fixed the feature checking code in "kdu_arch.cpp" for GCC builds on
  X86 platforms -- the assembly code there previously clobbered several
  registers without letting GCC know, which sometimes caused problems in
  the initialization sequence.  In some cases, this might have led to
  GCC builds not recognizing MMX/SSE/SSE2 support, so that the SIMD
  optimizations would not be applied.
* Fixed a minor source of numerical inaccuracy in the fixed-point
  implementation of irreversible wavelet kernels when SIMD processor
  speedups are not available -- not very often, seeming as they are
  available for X86, PowerPC and Sparc processors.

Changes from version 5.1.1 to 5.2
---------------------------------
* Previous deficiencies of Kakadu on Win64 systems have been corrected.
  Most importantly, the .NET compiler cannot use Microsoft inline assembly
  for 64-bit builds, which had forced developers to disable the MMX/SSE/SSE2
  speedups provided by Kakadu when building 64-bit systems.  This problem
  did not exist for 64-bit Unix builds.  The problem has been remedied
  by providing new implementations of the SIMD speedup code using
  processor intrinsics.  These are contained in header files with names
  of the form "x86_xxx_local.h".  Conditional compilation logic selects
  the most efficient, compliant implementation based on the targeted
  machine.  For more information, see the "Compiling_Instructions.txt"
  file, which has been substantially revised.
* Additional makefiles have been provided for MAC systems with Intel
  processors.  Also, the makefile naming conventions have been revised
  for increased clarity, as have the names of the separate directories
  into which they write their results, sitting under the "lib" and
  "bin" directories.  The new names should be self-explanatory.
* "kdu_hyperdoc" now builds C# and Visual Basic interface bindings, along
  with a more comprehensive set of Java bindings.  All of these interface
  bindings and corresponding foreign language examples are now found in
  the new "managed" directory, which contains makefile, MSVC and .NET
  build environments to build everything for you -- after building and
  running "kdu_hyperdoc" from the "apps" directory in the usual way.
* One new Java application example and two C# application examples
  have been provided in the "managed/java_samples" and
  "managed/csharp_samples" directories.  The examples are in one-to-one
  correspondence between the two languages.  They demonstrate use of
  the "kdu_region_decompressor" object and now the more generic
  "kdu_region_compositor" object to render images in these languages.
* "kdu_hyperdoc" now creates directories as required, if they do not
  already exist.  This should avoid problems encountered by new users
  who do not have the correct directory layout configured on their system.
* "kdu_hyperdoc" now copies all public API header files to a common
  location, "managed/all_includes" for convenience of application
  developers.  It also writes an auxiliary DLL / shared library
  named "kdu_a52R.dll" (debug version "kdu_a52D.dll") / "libkdu_a52R.so",
  which includes all useful generic classes from the "apps" directory.
  You can simply include this DLL/shared library along with the core
  system DLL/shared library ("kdu_v52R.dll", "kdu_v52D.dll" or
  "libkdu_v52R.so" as appropriate) to access all of the functionality
  declared in the headers in "managed/all_includes".  To build this
  auxiliary DLL/shared library, use the makefiles or Microsoft
  compiler build environments found in the "managed" directory.
* "kdu_hyperdoc" now allows you to explicitly specify which classes,
  global functions or even member functions you would like to receive
  Java, C# or Visual Basic language bindings via the new "-bind"
  option.  Of course, if no "-bind" argument is used, you will get
  bindings for the whole lot, as before -- actually more than before.
* `kdu_region_decompressor', `kdu_region_compositor' and all things
  which depend uon them now perform premultiplied alpha blending, in
  addition to regular alpha blending which has been available for
  a very long time.
* `kdu_region_decompressor' provides a new mode-setting function,
  `set_white_stretch', which may be used to control how low bit-depth
  imagery is rendered into higher bit-depth buffers.  In particular,
  the use of this option allows you to ensure that low bit-depth
  images will exactly span the maximum dynamic range of 0 to 2^{B-1}
  associated with a B-bit rendering buffer, at the expense of some
  computation.  The function can set a threshold at which stretching
  happens, so that very few cases incur the small computational
  increment.  The default value of this threshold in `kdu_region_decompressor'
  ensures exact backward compatibility with previous versions of the object,
  but `kdu_region_compositor' sets the threshold for optimal rendering
  to 8-bit/sample displays.
* The "kdu_server" application has been augmented with a "-cd"
  command-line option, which allows you to select a different directory
  for storing the ".cache" files which are created to ensure consistent
  serving of image files.
* A nice summary of all compilation directives now appears in the
  "Compiling_Instructions.txt" file, which has been substantially
  restructured to make things more accessible.
* Bug fixes as follows:
  -- In "kdu_serve.cpp", used by the "kdu_server" JPIP server application,
     a minor bug in the computation of bytes associated with previously
     served packets was corrected.  Specifically, in function
     `kd_serve::simulate_packet_generation', the line
       "for (cum_packets=1; cum_packets < tp->num_layers; cum_packets++)"
     is changed to
       "for (cum_packets=1; cum_packets <= tp->num_layers; cum_packets++)"
  -- In "kdu_hyperdoc", an earlier fix for a bug in the source parsing
     code accidentally caused bare functions (i.e., functions without
     classes) to be omitted from the automatically generated documentation
     and in the construction of Java native interfaces.  This has now
     been created.
  -- In "kdu_region_decompressor" in function `interpolate_and_convert',
     an unlikely condition which could cause the final column
     of a rendered region to be unwritten, in the event that the fast
     SIMD speedups are used, has been corrected.
  -- Fixed a bug in the "kdu_server" application's delegation feature,
     which has been present for some time but manifested itself only
     when the delegated server has a different IP address to the delegating
     server (as opposed to just a different port number).  The cause of the
     problem was a single typo, where the delegating server used syntax
     "hostname=" instead of the correct JPIP syntax "host=" which is
     expected by the client.
  -- Fixed a bug in the core coding parameter management system, which
     has been in existence since version 4.2, where sparse parameter
     instantiation on the tile-component grid was introduced in order to
     dramatically cut the cost of parameter management for heavily tiled
     (or componented) images.  Unfortunately, the instantiation logic did
     not pick up on the fact that separate instances of the quantization
     parameter object `qcd_params' need to be instantiated for image
     components whose precision, as recorded in the SIZ marker segment,
     differs from that of the first component, when reversible compression
     is involved, due to dependencies between precision and reversible
     dynamic range parameters.  The problem was very easy to fix by
     slightly modifying `qcd_params::finalize'.

Changes from version 5.1 to 5.1.1
---------------------------------
* Minor features added as follows:
  -- Upgraded the `jp2_colour_converter' object to allow for the conversion
     of 4 colour spaces to RGB, which means that "kdu_show" and other
     tools now automatically handle rendering of CMYK to RGB.
  -- Significantly modified the TIFF reading and writing code used by the
     demo applications kdu_compress and kdu_expand, so as to correctly handle
     a much wider range of TIFF files.  The code now handles both tiled and
     untiled images, with both planar and contiguous pixel organizations.
     In addition, a bug in the palette handling was removed -- this but
     was accidentally introduced in v5.1 when generic baseline TIFF handling
     was provided through native Kakadu tools.  TIFF reading and writing is
     still not really considered part of the Kakadu toolset, since it is
     used only for demonstration purposes; however, a lot of people have
     asked specifically for this, so they can more conveniently use the
     demo apps.
* Bug fixes as follows:
  -- The "kdu_tiff" object introduced in version 5.1 contained a
     subtle yet important bug when handling tags whose value is
     rational, where the file byte order differs from that of the
     native machine.  The fix involved restricting the swapping
     of 4-byte words to double precision floating point data types,
     rather than all 8-byte data types, as suggested by Michael
     Wildermoth.
  -- Two fixes to the raw file reading code in "kdu_compress", both
     suggested by Margaret Lepley.
  -- Removed the second writing of TIFF tag RowsPerStrip when
     generating GeoJP2 boxes in "image_in.cpp", which was a typo.
     This fix was provided by Greg Coats.
  -- Removed a typo in the call to `_addr_to_kdu_int32' in
     `simd_upshifted_interleave' within "gcc_dwt_altivec_local.h".
  -- Fixed a memory leak in `jpx_codestream_source::access_dimensions'
     where the `kdu_codestream' object which was created by this function
     in order to completely finalize compatibility information, was not being
     destroyed.
  -- Fixed a foolish error in `kd_serve::process_window_changes' where
     a `kdu_window' object was directly assigned to another rather than
     using the `kdu_window::copy_from' function.  This caused `kdu_server'
     to crash on connection closure, which is a behaviour that should have
     been caught prior to release of v5.1 were it not for the fact that the
     bug was introduced immediately prior to release, as a fix for another
     much more subtle error.
  -- Modified the colour space interpretation code in "kdu_compress" so
     as to allow proper representation of 4-colour spaces such as CMYK --
     of course, this is just a demo, but it's nice for the demo to be general.

Changes from version 5.0 to 5.1
-------------------------------
* The main new feature in Kakadu v5.1 is the inclusion of extensive
  multi-threading facilities to exploit the computing resources
  available on multi-processor, multi-core and hyperthreading platforms.
  This is achieved through two new core system objects, `kdu_thread_entity'
  and `kdu_thread_env'; the latter is derived from the former.  You can
  completely ignore these objects, compiling your applications exactly
  as before, in which case only one thread will be involved in processing.
  Alternatively, the simplest way to reep advantages on multi-threaded
  platforms, is to create a `kdu_thread_env' object and pass it (as an
  optional argument) to the objects you are using to perform your
  processing.  All of the high level sample processing objects
  (`kdu_multi_analysis', `kdu_multi_synthesis', `kdu_stripe_compressor',
  `kdu_stripe_decompressor', `kdu_region_decompressor' and
  `kdu_region_compositor') provide simple mechanisms to include the
  processing resources offered by a `kdu_thread_env' object you have
  created.  You control the number of working threads by using the
  `kdu_thread_entity::add_threads' function.  In most cases, you should
  arrange for the total number of working threads to be equal to the
  number of distinct real or virtual processors -- a value which Kakadu
  attempts to find for you via its `kdu_get_num_processors' function.
     Of course, there are lots of lower level ways to use the multi-threading
  framework, but the high level objects will be simplest to understand
  and require only a few extra lines of code.  To learn more about how
  to program with Kakadu's multi-threading environment, consult the
  demo applications -- "kdu_render", "kdu_compress", "kdu_expand",
  "kdu_buffered_compress", "kdu_buffered_expand", "kdu_v_compress",
  "kdu_v_expand" and "kdu_show" all provide facilities to use (or not use)
  the multi-threading environment and to control the number of threads used
  (typically a `-num_threads' argument).
     There is a lot of scope to do more with multi-threading than the demo
  applications can illustrate.  For example, the "kdu_v_expand" and "kdu_show"
  applications process each frame in a video sequence completely
  (synchronizing on the completion of all threads) before moving on to a
  new one.  This incurs some start-up and close-down costs where only one
  thread can execute.  It is possible, however, to arrange for overlapped
  processing of multiple frames.  This may be demonstrated explicitly
  in future versions of Kakadu.  Depending on the number of processors
  available on your platform, you may also find it useful to play around
  with double buffering options -- see, for example, the `-double_buffering'
  options to "kdu_compress" and "kdu_expand", which are used to enable and
  tweek double buffering features offered by `kdu_multi_analysis' and
  `kdu_multi_synthesis' to parallelize the processing of multiple
  tile-components.
     Use of the multi-processing framework with multiple threads incurs a
  memory overhead for the double buffering of code-block samples, since
  block encoding and decoding operations provide the greatest
  opportunity for parallelism in JPEG2000.  The extra memory is allocated
  automatically, using an algorithm which backs away from full double
  buffering as the image dimensions become very large.  For very large
  single-tiled images, the memory penalty associated with multi-threaded
  processing reaches around 30%.  If you are interested in playing with
  the internal algorithm which controls these costs, take a look at the
  `kd_encoder::init' and `kd_decoder::init' functions -- in particular,
  you may adjust the way in which the `num_jobs_per_row' and
  `buffer_height' member variables are initialized for each subband's
  `kdu_encoder' or `kdu_decoder' object.
     During compression, it is possible to overlap incremental codestream
  generation with ongoing processing, when using Kakadu's incremental
  flushing features.  This is demonstrated in "kdu_compress" and is also
  implemented for you within `kdu_stipe_compressor'.  It is achieved by
  defining so-called "synchronized jobs", which are scheduled at the first
  convenient opportunity after a synchronization condition is reached
  (typically, the processing of all samples pushed into the DWT engines
  so far).  The positioning of synchronized flushing jobs for maximum
  parallelism is a little tricky, so it is best either to use
  `kdu_stripe_compressor' or to read the explanation appearing within the
  "kdu_compress" function `compress_multi_threaded'.
     One nice feature of Kakadu's multi-threading environment is that you
  can compile and test applications based on the `kdu_thread_env' and
  `kdu_thread_entity' objects even if your platform does not support
  multi-threading.  To compile without actual multi-threading support,
  define the `KDU_NO_THREADS' symbol.  All that will happen in this case
  is that attempts to add additional threads to a `kdu_thread_env'
  (equiv. `kdu_thread_entity') object which you create will fail, leaving
  you with only one thread of execution.  This single thread (the one
  your program started in) will then be scheduled onto the various tasks
  automatically, as required.  The same thing happens if you never add
  extra threads to a `kdu_thread_env' object before using it.
    For true multi-threading, the Kakadu implementation supports both
  POSIX threads (Pthreads) and Windows threads.  If neither is available
  on your platform, you should define the `KDU_NO_THREADS' symbol, as
  mentioned above.

* Added new `push_stripe' and `pull_stripe' interface functions to the
  `kdu_stripe_compressor' and `kdu_stripe_decompressor' objects, respectively,
  to support 32-bit integer and floating point image sample values, in
  addition to the existing 8- and 16-bit precision interfaces.  These
  additional interfaces ensure that the high level stripe-oriented
  objects support all of the data precisions that are supported by the
  underlying core Kakadu system.

* Added the Digital Cinema profiles (CINEMA2K and CINEMA4K) to the list
  of profiles recognized in the codestream SIZ marker segment.  These
  profiles were added about a year ago as an ammendment to Part 1 of
  the JPEG2000 standard.

* Added significant support for TIFF and GeoTIFF image I/O to "kdu_compress"
  and "kdu_expand".  Even though image I/O is not really part of the scope
  of Kakadu, it is required for demonstration purposes and lack of
  comprehensive TIFF support has been a source of concern for some new users.
     This has been complicated by the fact that GeoJP2 files include a JP2
  box which is really an encapsulated TIFF file.  To solve both problems in
  one go, this new release of Kakadu comes with a simple yet general native
  TIFF parser/creater, which is not based on any external libraries and
  integrates well with the other abstract I/O services endemic to Kakadu.
  As a result, TIFF files can now be read and written by the "kdu_compress"
  and "kdu_exand" demo applications without the need to link against LIBTIFF
  (as before).  GeoTIFF tags are also extracted by "kdu_compress" and used
  to create a GeoJP2 box if appropriate.  GeoJP2 boxes may be unpacked (if
  required) and/or written back to GeoTIFF files by "kdu_expand".
     The native TIFF managing code is small and does not explicitly manipulate
  imagery; it only manages TIFF directories -- see "kdu_tiff.h" or lookup
  the hyperdoc documentation for `kdu_tiffdir'.  Kakadu is actually quite
  agnostic about the GeoJP2 format, but since a GeoJP2 box is just a UUID
  box which reads like a TIFF file, any such box can be supplied directly
  to `kdu_tiffdir::opendir', giving you read/write/modify access to its
  embedded tags -- double-precision world coordinates and such.
     The demo code in "kdu_compress" and "kdu_expand" only reads and writes
  uncompressed TIFF files -- although it handles arbitrary bit-depths from 1
  to 32 bits/sample and both signed and unsigned sample formats.  If you
  need to read compressed TIFF files, this is still possible by defining
  `KDU_INCLUDE_TIFF' and linking against the public domain LIBTIFF library.
  If you do this, the services of LIBTIFF are used only for decompressing
  source TIFF files (and then only when the file indicates that it uses a
  compressed sample format) -- all other interaction with the TIFF directory
  is managed natively.

* "kdu_hyperdoc" now generates a much more extensive set of Java
  interfaces to Kakadu functions.  In particular, most combinations of
  default arguments in the C++ interface functions are now converted
  into distinct Java bindings, for programming convenience.  Also,
  functions which accept or return opaque pointers (i.e., pointers to
  objects whose definitions are not publically visible) are now
  mapped to Java bindings, with the pointers represented as Java "long"
  arguments.  There are some important high level interface objects,
  such as "jpx_frame_expander" which manipulate such opaque pointers and
  so were not previously available in Java.

* Quite a number of minor bugs have been fixed (should include all bugs
  reported to date).  These include:
  -- Fixed a minor bug in the parsing of open-ended ranges (e.g., unbounded
     codestream indices) in JPIP communications.
  -- Fixed a minor syntax error in the parsing of the "metadata-only"
     qualifier for JPIP requests -- this qualifier consists of a "!!" found at
     the end of "metareq" request field, where kakadu previously expected
     ",!!".
  -- Made some minor improvements to the distortion-rate slope prediction
     algorithm at the heart of the EBCOT block encoding machinery in the
     core system; this algorithm is used to prematurely truncate the
     encoding process where a minimum slope or an overall maximum length
     threshold has been provided to the codestream management machinery.
     The previous implementation very occasionally truncate code-blocks
     much too early.  The new implementation is much less likely to do
     this, but sacrifices nothing in speed.
  -- Fixed a minor bug in the "kdu_hyperdoc" utility which caused it to
     append "[SYNOPSIS]" style comments found against protected or private
     class members to the descriptions of preceding public class members.
  -- Fixed a minor race condition in the "kdu_client" object, which could
     manifest itself when both server and client try to close a connection
     in very close proximity.
  -- Fixed a minor bug in the order in which Kakadu's JPIP server component
     processed cache model manipulation statements from the client.  This
     caused model addition statements to be discarded if they were
     delivered on a first request which accessed a particular codestream.
  -- Fixed a minor oversight in "kdu_compress" which prevented
     fragmented compression from working correctly with JPX files -- the
     problem was that the contiguous codestream box was not being placed
     into the "rubber length" mode when writing JPX files (any application
     can easily select this mode), so that extra fragments which appended
     this box were not included in the indicated box length.
  -- Fixed a minor bug accidentally introduced into the Altivec (PowerPC)
     speedup code in version 5.0.  The bug involved 64-bit pointers being
     cast to 32-bit integers for parity checking, which fails to achieve
     the desired result in the big-endian PowerPC architecture.  The bug
     only manifested itself near the left edge of some regions when using
     Kakadu's region-based decompression features.

Changes from version 4.5.2 to 5.0
---------------------------------
* Four new dead-easy demo code fragments implemented inside the
  "kdu_render" demo app, to get you up and running as quickly as
  possible.  The most sophisticated of these (embodied in function
  `render_demo_4') renders any raw codestream, JP2 file, JPX file,
  JPX animation frame or MJ2 file to a memory buffer, performing
  colour space conversions as required, inverting multi-component
  transforms as required, etc., etc. -- it is less than 50 lines long,
  but if you like one-liners, just consider a call to `render_demo_4'
  as your one line solution.
* Windows builders note that some applications compiled against the
  core system DLL may need to define the symbol "CORESYS_IMPORTS".  It
  is good practice to do this universally in all your applications which
  import the Kakadu core system DLL.
* Windows builders using Microsoft VC6 need to ensure that they have the
  processor pack installed (see "Compilation_Instructions.txt").
* Added full support for Part-2 arbitrary transform kernels
* Added full support for Part-2 arbitrary decomposition styles, including
  wavelet packet decomposition structures and unbalanced sub-sampling
  structures in which successive resolution levels might have identical
  horizontal dimensions or identical vertical dimensions.
* Added full support for Part-2 multi-component transforms, including
  all possible transform block types (reversible/irreversible decorrelation,
  dependency and DWT transform blocks with arbitrary kernels).  Also
  ensured that all other elements from rendering to interactive distribution
  with JPIP work correctly when multi-component transforms are employed.
  Note that JPIP syntax only allows requests for multi-transformed components
  to be signalled when they are wrapped as JPX compositing layers -- use
  the `-jpx_layers' argument to "kdu_compress", together with "-jp2_space sLUM"
  if you want to create one compositing layer for each multi-transformed
  image component (say in a medical volume) and then interact with it using
  JPIP.  See "Usage_Examples.txt" for more ideas or read the updated overview
  document, "kakadu.pdf" -- see especially the new Section 4 in this updated
  document.
* Modified codestream parameter sub-system to use correct Part-2 codestream
  syntax for codestreams which use the alternate code-block alignment
  required for compressed-domain rotation and flipping (as performed by
  "kdu_transcode" for example) -- previous versions included this only as
  an experimental feature, without correct marker segment syntax.
* Improvements to platform-dependent speedups allow throughput increases of
  around 20% at low to moderate bit-rates on Pentium-4 platforms, while the
  Altivec speedups for the Power PC now function correctly under all
  conditions, including regoin-of-interest decompression -- previous versions
  had an alignment-induced problem under some conditions, but the new version
  aligns all critical accesses on 16-byte boundaries, on all platforms.
  This will also improve cache performance when multi-processor speedups are
  introduced in a future release.
* Rendering speed at low to moderate bit-rates via `kdu_region_compositor' or
  `kdu_region_decompressor' has increased by 40% to 50% on Pentium 4 platforms,
  due to the selective inclusion of SSE speedups in the decompressed data
  processing path -- smaller speedups should already be observed on other
  platforms, but it should be very easy indeed for interested persons to port
  the small Pentium 4 speedup routines in "msvc_region_decompressor_local.h"
  and "msvc_region_compositor_local.h" to other platforms (e.g. the Power PC),
  following the general approach taken in the core system (see gcc
  platform-specifics in "coresys/transforms" for examples).
* Fixed a bug in the fragmented codestream compression feature, whereby
  the quality layer target lengths supplied to `kdu_codestream::flush'
  were incorrectly scaled when multiple fragments were used.  Some
  licensees chose to avoid this problem by using slope thresholds to control
  quality layer sizes, which is probably the best strategy for fragmented
  codestream compression.  Others pre-scaled their target layer lengths to
  compensate for the internal bug.  This latter group will now need to
  remove this pre-scaling, since the internal problem has been corrected.
* Introduced a heuristic adjustment to the efficient rate control strategy
  implemented by the "kdu_v_compress" demo application so as to prevent
  occasional hicups reported by others.
* Added the capability for "kdu_compress" to read raw files in little-endian
  word order (just use files whose suffix is ".rawl" rather than ".raw").
  Also added the ability to read raw files which contain multiple
  concatenated image components (for simplicity when compressing image
  volumes).
* Added a `-codestream_components' option to "kdu_expand", so that users
  can specify whether they want to produce only the codestream components
  (these are the components produced after block decoding and inverse
  wavelet transformation) or the complete output components (these are
  the components produced after any specified colour or multi-component
  transformations have been performed).  If you do not specify this option,
  you will get output components.
* Incorporated 3'rd party fixes to the "kdu_hyperdoc" utility to generate
  Java (JNI) interfaces which avoid a rare race condition previously
  encountered in multi-threaded Java apps.

Some existing applications may require the following changes in order
to fully support Part-2 codestreams.
1) In order to ensure correct generation of JPX image header boxes
   when Part-2 codestream features may be used, your application must now
   be sure to invoke the `jp2_dimensions::finalize_compatibility'
   function, after finalizing all `kdu_params' objects and before invoking
   `jpx_target::write_headers'.  This is a new requirement.  Previously, it
   was sufficient to just call `jp2_dimensions::init' any time after
   the `siz_params' data was available.  While not strictly necessary, it
   is a good idea also to call `jp2_dimensions::finalize_compatibility'
   right before generating a regular JP2 file's header (via
   `jp2_target::write_header') so that the logic can verify that the
   codestream being embedded in the JP2 file is indeed a Part-1 codestream,
   since Part-2 codestreams must be embedded in JPX, rather than JP2 files.
2) If your application uses `jp2_dimensions::copy' to copy imagery from
   an existing JPX file to a new one, you should ideally first
   invoke the `jp2_dimensions::finalize_compatibility' function on the
   source object.  To facilitate this process, the
   `jpx_codestream_source::access_dimensions' function now takes an optional
   `finalize_compatibility' argument, which you should set to true when
   accessing the source `jp2_dimensions' interface which you intend to supply
   to `jp2_dimensions::copy'.  Except where such copying is required, it
   is more efficient to leave the `finalize_compatibility' argument equal to
   its default value of false.
3) If your application needs to directly access codestream subbands, you
   should use the new `kdu_resolution::get_valid_band_indices' function
   to obtain the range of subband indices which are valid for any given
   resolution level.  In previous versions, the application could assume
   that resolution level 0 had only the subband with index `LL_BAND' while
   all other resolution levels had subbands with indices `HL_BAND',
   `LH_BAND' and `HH_BAND'.  This is still true for Part-1 codestreams, but
   you need to be more careful with Part-2 codestreams.  Very few applications
   should need to directly access subbands.  The one notable exception is
   transcoding applications, which typically copy/transcode the relevant
   code-blocks one-by-one, accessing them via their containing subband
   interfaces.
4) If you want your application to work correctly on images which have
   been compressed using JPEG2000 Part-2 multi-component transforms, some
   minor changes may be required as follows.  I believe that these represent
   everything you need to consider.  Also, none of these changes
   are required if you don't need to decompress images which use Part-2
   multi-component transforms, and many applications might not require any
   changes at all.
   a) If you are using `kdu_stripe_decompressor', `kdu_region_decompressor'
      or `kdu_region_compositor' you don't need to do anything differently;
      however, if you are creating `kdu_pull_ifc' derived objects
      (`kdu_decoder' or `kdu_synthesis') directly to construct decompression
      engines, you will only end up decompressing the raw codestream
      components.  To migrate to a decompression engine which also correctly
      inverts the multi-component transform is actually very easy.
      Essentially, you just need to replace all of your independent
      component processing engines with a single `kdu_multi_synthesis'
      object, which does the whole thing -- then you no longer need to look
      out for whether a YCC colour transform needs inverting either.  If in
      doubt, take a look at how the compression engine creation and usage
      code has been changed in the "kdu_expand" demo app.
   b) For completely general decompression, you need to bear in mind that
      there can be a difference between output image components (produced
      after any multi-component transform is inverted) and codestream
      components (the ones you may have previously been associating directly
      with decompression engines).  The difference may involve differences
      in order (and hence possibly dimensions) and number (there can be
      more or less multi-component transform output components than raw
      codestream components, in the general case).  For backward compatibility,
      Kakadu's interface functions which return information related to
      components or indexed by component indices actually return information
      about the raw codestream components by default.  However, all of these
      functions now contain an optional final argument (`want_output_comps')
      which you should set to true if your application wants information
      about final output components (more likely than not this is the case).
   c) If your application directly invokes
      `kdu_codestream::apply_input_restrictions' you need to know that there
      are now two versions of this function.  If you call the original
      version in the way you did before, it will make multi-component
      transforms appear not to be present (this is required for backward
      compatibility with applications which might have been directly
      creating low level decompression engines).  To avoid this, set the
      optional final `component_access_mode' argument to
      `KDU_WANT_OUTPUT_COMPONENTS', or consider using the second, much
      more flexible version of the function which now allows you to
      restrict your region of interest to any arbitrary set of image
      components, optionally including permutations.

Changes from version 4.5.1 to 4.5.2
-----------------------------------
This is just a bug fix version.
* Fixed a minor bug in "kdu_transcode" which arises when extracting
individual image components.
* Fixed a minor bug in `kd_message_block::peek_block' which caused
problems when the Kakadu client is communicating with a server via
a proxy which rechunks the HTTP response.
* Fixed a couple of accidentally introduced bugs in `kdu_region_compositor'
and the way it is used by "kdu_show", which caused the objects to be
used in a manner which was much less responsive to interactive control
than it had been in versions prior to v4_5.
* Made a minor change to `CKdu_showApp::save_as_jpx' to ensure that
large files could be re-saved without unnecessary intermediate buffering
of the embedded codestream box.

Changes from version 4.5 to 4.5.1
---------------------------------
NB: This version exists only to correct a few of minor issues
discovered immediately after the release of Version 4.5.

* Minor bug fix in `kdu_region_compositor' to avoid a problem
which can cause the number of available rendering resolutions to
be reduced by 1 (bug was accidentally introduced in v4.4 with
the move to arbitrary rational composition scaling factors).
* Minor correction to `kdu_region_decompressor' to ensure that
high bit-depth imagery which requires colour conversion will
be handled correctly, even if not with maximum accuracy.
* Added the `JPX_SF_sYCC' feature flag to "jpx.h" in accordance
with a recent ammendment to the IS 15444-2, which describes
the JPX file format.  This should not impact any existing
applications.

Changes from version 4.4 to 4.5
-------------------------------
* Introduced all the support required to internationalize and/or
  customize all error/warning messages produced by the Kakadu
  system or derivative applications.  This is done in an almost
  seamless manner, which remains backward compatible with previous
  uses of the "kdu_error" and "kdu_warning" services and which
  is entirely platform independent.  Original error/warning text
  remains in the source files where it is easiest to follow and
  edit.  However, new constructors are provided for "kdu_error"
  and "kdu_warning" which allow text to be registered with unique
  identifiers.  This is done using macros, so that there is
  very little editing of the existing error/warning calls.  The
  same macros are used by a new tool, "kdu_text_extractor", to
  collect all the registerable text into separate language files.
  You can create as many versions of these as you like, translating
  text into new languages (and even using unicode for languages
  with large alphabets).  To use any of these external language
  files, you compile the original code (core system, applications,
  etc.) with the `KDU_CUSTOM_TEXT' macro defined.  You then simply
  include the language files of interest into your end application
  and the translation process is complete.  If you like, you can
  construct separate language-specific DLL's (Windows) or
  shared librares (Unix) containing the language files.  Your
  application then just needs to load the language DLL or shared
  library of interest at run time.
    By default, `KDU_CUSTOM_TEXT' is not defined, and everything
  behaves exactly as it did in previous versions of Kakadu.  In
  this case, there is no need (indeed no point) including the
  language files.
    For more information, see the "Compilation_Instructions.txt" file.
* Extended `kdu_region_compositor' to support Motion JPEG2000 data sources,
  in addition to JP2/JPX files and raw codestreams.  The new support includes
  the ability to composit and individually reorient tracks in accordance
  with the movie specifications found in the Motion JPEG2000 source.  Some
  of these new features are demonstrated by new features in "kdu_show".
* Extended "kdu_show" to handle Motion JPEG2000 files, in addition to its
  current input formats.  Also added playback control which operates in the
  same way for Motion JPEG2000 movies and JPX animations.
* Updated the `mj2_source' and `mj2_target' objects to bring them into
  compliance with aspects of the Motion JPEG2000 standard which were
  changed/clarified by a corrigendum.  The interface functions offered by
  `mj2_source' now offer the support required for handling data sources
  which are fed by an asynchronous dynamic cache (for JPIP browsing
  applications); however, the internal implementation does not yet support
  caching sources.  This can wait until we have a full JPIP implementation
  for video browsing.
* Extended the "kdu_merge" demo application, to support writing of both
  JPX and MJ2 files, based on input from one or more JP2/JPX/MJ2 files.
  The new features allow you to create MJ2 files from a sequence of JP2
  files, from the compositing layers in a sequence of JPX files, and/or
  from a sequence of existing MJ2 tracks.  You can write multi-track
  MJ2 files and you can even merge fields together to form interlaced
  MJ2 tracks -- however, note that "kdu_show" will currently only play
  the first field (correctly scaled) in each frame of an interlaced track.
     These services, while simple, close an important hole in
  the demo applications relating to Motion JPEG2000.  Many people had
  difficulty using the "kdu_v_compress" and "kdu_v_expand" tools in the
  past because they were not sure how to create "vix" files.  Now you
  can create MJ2 files from JP2 files and use them to write sample VIX
  files to play around with -- VIX is a mindbogglingly simple format though.
* Fixed a minor bug in `kdu_region_decompressor' which could affect the
  image produced when using rational expansion factors.  Also, replaced
  nearest neighbour interpolation with an efficient bilinear interpolation
  strategy for improved visual performance when non-integer rational
  expansion factors are used.
* Fixed a minor bug which was introduced into `kdu_region_compositor' in
  version 4.4 -- the bug could generate an assertion failure when zooming
  into rotated compositions.
* Fixed two minor bugs in the core system which affect transcoding.  You
  can now use incremental codestream output with `kdu_codestream::trans_out'
  for transcoding applications (lots of memory saving potential) -- a simple
  demonstration of this is included with the "kdu_transcode" application.
* Fixed a minor bug in the core system which has always been in Kakadu --
  it had the potential to cause an assertion failure (or memory overwrite)
  when re-opening a precinct with an increased quality layer threshold,
  but only in persistent mode where the codestream contains no PLT
  information, and then only with certain uncommon combinations of
  parse/read sequences.
* Removed the call to `kdu_error' within `kd_encoder::~kd_encoder' if not
  all lines were pushed into the compression engine.  This allows you
  to abort compression processing without errors or memory leaks.

Changes from version 4.3 to 4.4
-------------------------------
* Support for arbitrary scaling factors has been introduced into
  `kdu_region_decompressor' and `kdu_region_compositor'.  This means
  that you are no longer restricted to integer expansion factors and
  power-of-2 decimation.  It also means that correct interpolation
  can be applied prior to colour transformation or alpha blending, in
  the case of uncommon chrominance subsampling arrangements or low
  resolution alpha specification.  Most importantly, however, it means
  that `kdu_region_compositor' can correctly composit multiple
  codestreams onto a JPX compositing surface, taking into account their
  respective scaling requirements to produce correct alignment under
  all circumstances.
* Added capabilities to derive progress information in JPIP
  interactive browsing applications, based on the number of quality
  layers which are available at any given time for the codestream
  precincts required to reconstruct a region of interest.  This support
  is introduced by the low level functions `kdu_resolution::get_precinct_area'
  and `kdu_resolution::get_precinct_packets', and the much higher level
  function, `kdu_region_compositor::get_codestream_packets'.  The
  new functionality is explicitly demonstrated by the provision of a
  progress status indicator for "kdu_show", which shows up when the status
  bar is toggled into the download statistics mode during JPIP remote
  image browsing.
* Implemented all the machinery required to create and handle fragmented
  and externally referenced codestreams in a JPX file.  This is demonstrated
  by the provision of a "-links" argument to "kdu_merge" which can be
  used to create a JPX file whose codestreams reside in other files (e.g.
  a photo album whose contents reside elsewhere).  All
  demonstration applications correctly handle such JPX files, including
  "kdu_show" and "kdu_server".  When JPX files with external links are
  served using JPIP, streaming equivalents are used to make them appear
  local.
* Moved the MMX optimizations for Linux builds from assembler source files
  (*.s) to gcc inline assembly code.  This should have no impact on
  performance, but avoids the various problems which have been encountered
  in the past when compiling with different versions of GAS and GCC.
  Compilation for Linux should now be a hassle-free process.
* Added "-log" and "-wd" arguments to "kdu_server", following the suggestion
  of Michael Owen, so that it can be invoked as a registered Windows service.
* Some minor adjustments have been made to the definitions in
  "kdu_elementary.h" to facilitate compilation under 64-bit Solaris
  environments; also, revised makefiles for the sunpro compiler have
  been contributed by Margaret Lepley.
* Fixed a number of bugs accidentally introduced by the more extensive
  changes in v4.3.  Notable among these is a bug which manifests itself
  if `kdu_codestream::augment_buffering' is used.
* Fixed a number of bugs connected with more exotic colour rendering.
* No known bugs remain.

Changes from version 4.2.1 to version 4.3
-----------------------------------------
* Significant improvements in dynamic memory management for tiled images
  -- Previous versions of Kakadu provided for dynamic unloading of
     unused precincts, subject to the provision of appropriate pointer
     information in the code-stream.  This functionality has now been
     extended to the automatic unloading (and on-demand reloading) of unopen
     tiles from heavily tiled images.  While this is most efficient if the
     code-stream contains TLM marker segments to point to the tiles from
     its main header, the code-stream machinery now builds the TLM
     information on the fly if necessary, while parsing through seekable
     compressed data sources.
     + See `kdu_codestream::augment_cache_threshold' to understand how
       dynamic tile unloading/reloading has been integrated into the
       existing automatic memory management features -- by and large,
       applications should be able to get the benefits of these features
       without any implementation changes, remaining blissfully unaware
       of the numerous code-stream structures which could be presented
       to the internal machinery.
     + The new function `kdu_codestream::set_tile_unloading_threshold'
       gives you additional control over dynamic tile unloading, if you
       should require it.
     + The `kdu_codestream::get_compressed_state_memory' function has
       been slightly redefined to take advantage of more comprehensive
       memory accounting by Kakadu's code-stream memory management
       system.  In particular, the cost of tile, tile-component,
       resolution, subband and precinct address structures are now
       included along with the cost of precinct and code-block structures
       which were previously reported by this function.  By and large,
       applications which were using this function for gathering memory
       statistics, can continue to work as before, except that the statistics
       will be more comprehensive.

* Improved the efficiency with which finely tiled images are compressed
  and manipulated at very low resolutions, where there can be an enormous
  number of tiles.  This is achieved by maintaining a cache of "typical"
  tile objects within the code-stream management machinery, rather than
  instantiating a distinct object each time a tile is opened.  This
  functionality leverages off the changes first introduced in v4.2,
  wherein the coding parameters of finely tiled images are represented
  very efficiently wherever possible.

* The core codestream generation machinery can now generate TLM marker
  segments itself, rather than having to defer this process to a postprocessing
  phase using the "kdu_maketlm" program.

* The core codestream generation machinery can now compress tiled images
  in fragments, where each fragment is compressed independently and can
  consist of any subset of the tiles in the full codestream.  Fragments
  are automatically stitched together and a correct set of TLM marker
  segments can be generated for the full set of fragments, by rewriting
  selected segments of the main header.  These capabilities extend
  Kakadu's ability to compress large images at least into the tens of
  tera-bytes, and are demonstrated with the aid of the new `-frag'
  argument to the "kdu_compress" demo utility (see "Usage_Examples.txt").

* The `kdu_region_decompressor' object now offers even more flexibility
  in the way its `process' functions set the dynamic range and signed/unsigned
  attributes of the samples which they produce.  You can supply a
  `precision_bits' argument of 0 to get the `process' function of interest
  to derive its precision and signed/unsigned decisions from the original
  image sample attributes recorded in the codestream and/or file-format
  headers as appropriate.  The `kdu_channel_mapping' object's configuration
  state is expanded to handle this information and also allow
  application-specific overrides for each individual logical rendering
  channel.  These changes seem to address the concerns of a wide range
  of applications, as expressed on the Kakadu mail reflector.

* The JPX file format reader can now read JPX files which were mistakenly
  written without the reader requirements box by Algovision -- JPX
  files without any reader requirements box were technically illegal, but
  the reader requirements box is so poorly defined as to be next to
  useless to practical applications, so it is better not to require its
  existence, rather than fail to read existing non-compliant files.

* The JPIP client/server implementation now allow a TCP transport
  channel to be preserved between JPIP sessions, so you can start
  and close multiple JPIP sessions without having to close and
  re-open the TCP link (good for apps which need to make lots of
  JPIP requests without looking like they are trying a denial of
  service attack).  To do this, you must pass a non-default value
  for the `keep_transport_open' argument to `kdu_client::disconnect'.
  The operation should succeed in preserving the channel until the
  next call to `connect' so long as you are using the HTTP transport
  method and the connection was idle when you called `disconnect'.
  You can test the functionality out in "kdu_show" by explicitly
  clicking File->Disconnect, after you see the idle status appear in
  the statusbar.  The next connection attempt will try to use the
  same TCP channel; if the server has dropped the channel already
  (e.g., due to timing out on waiting for more communication on it --
   "kdu_server" uses a default timeout of 5 seconds), a new one
  will be opened.  Note, however, that in this mode there is no
  checking to see if the new channel belongs to the right server, so
  you may get an error message back if you are really trying to
  request a resource from a different server.  The application is
  supposed to avoid chaining requests which belong to different
  servers on the same TCP channel.

* Fixed a number of bugs, including
  -- one subtle bug in "kdu_server" (provided by a licensee), which could
     cause assertion failure under some circumstances when the same image
     is being accessed from multiple threads;
  -- several other rare bugs in the synchronization logic used for
     the producer-consumer threads in "kdu_server" -- these fixes may
     have resolved previously reported occasional server hangups;
  -- a number of inter-related bugs affecting complex colour conversion
     processes in the `jp2_colour_converter' object -- in particular, Lab
     colour space conversion now works correctly.
  -- a bug introduced accidentally in v4.2 which prevented RGN marker
     segments from being written out during code-stream generation.  This
     bug was responsible for problems encountered in the generation
     of images with explicit ROI features.


Changes from version 4.2 to version 4.2.1
-----------------------------------------
* This is a bug-fix release.  Below is a summary of some of the bugs fixed:
  -- A bug in the SPARC optimizations (VIS instruction set) which occurred
     when decompressing regions of interest.
  -- Various bugs in the "kdu_server" utility.  Most of these manifested
     themselves only when serving certain types of image files; however,
     I believe I have fixed a long standing bug which would cause the
     server to sometimes experience a memory fault after running for a
     month or so.
  -- A minor bug in the "kdu_show" application which manifested itself
     sometimes while closing and re-opening JPIP sessions.
  -- Various problems with the TIFF reading code originally contributed by
     a third party to the "kdu_compress" demo app -- substantially rewritten.
  -- Some bugs in the `kdu_stripe_decompressor' object and associated
     demo apps, which caused some types of data precision conversions to
     be performed incorrectly.

Changes from version 4.1 to version 4.2
---------------------------------------
* JPIP Client-Server extended to fully support JPX
  -- the client-server implementation has been updated to support the new
     "codestream-context" request field defined by JPIP.
  -- using codestream contexts and context translation, the client-server
     implementation now allows for highly efficient browsing of complex
     JPX images, containing multiple compositing layers, with different
     scale factors, cropping, and placement.  Also supportes efficient
     browsing of remote images which use multiple codestreams, potentially
     at different scales, to construct each image layer.
  -- the client-server implementation now fully supports the interactive
     transfer of metadata, including textual labels, XML, etc.  The
     server automatically figures out what metadata is relevant to any
     given requested image region.  Moreover, it is able to do this in
     the case of complex images, having multiple codestreams at different
     scales, cropping, and placement, each with its own codestream-registered
     region specific metadata.  The bare minimum amount of metadata is
     delivered to satisfy any particular request.  Moreover, all
     metadata-specific JPIP requests ("metareq" request field) are
     properly honoured and scoped.
* Dramatic speedups for heavily tiled images
  -- many thanks to Margaret Lepley for pinpointing the source of serious
     slow downs which have been observed when compressing/decompressing
     images with a very large number of tiles.  The problem was in the
     linear list searching performed in the codestream parameter sub-system
     ('kdu_params' and its derived objects).  This sub-system was actually
     the very first code implemented when Kakadu was originally developed.
     The internal implementation of "kdu_params" has been significantly
     changed to dramatically reduce access times (by up to 3 or 4 orders
     of magnitude) and memory consumption, while leaving the interface
     functions almost entirely unchanged.  No changes should be required
     by any existing application-level code, although some applications
     which use the binary methods to access parameters could stand to
     save memory now by supplying a value of "true" for the new "read_only"
     argument to `kdu_params::access_relation'.  See the interface
     documentation for this function if you wish to take advantage of this.
* SIMD platform-specific speedups to the wavelet and colour transform code
  are now provided for the UltraSparc (Sparc VIS instruction set) and the
  PowerPC G4 (Altivec vector processing instruction set), to complement
  the existing SIMD speedups provided for Intel processors (MMX instruction
  set).  Many thanks to Monroe Williams for contributing the Altivec
  code, while working on Linden Lab's "Second Life" project.
* To further demonstrate the sophisticated JPX features, the "kdu_merge"
  application has been extended to include:
  -- a "photo album builder" function, whereby an arbitrary collection of
     JPEG2000 compressed images can be combined into a single photo album,
     which can then be served as a monolithic entity for highly efficient
     interactive remote image browsing; and
  -- merging of metadata from all source images into the single output JPX
     image, after correctly adjusting and/or inserting cross-references
     between metadata items and image elements.
* The "kdu_show" application has been slightly updated to allow image-wide
  metadata to be displayed and edited interactively.  In the simplest
  instance, this allows the user to view labels which may be attached
  by a content provider to each compositing layer in an image.  In the case
  of photo albums built using the "kdu_merge" tool, each image in the
  album is a separate compositing layer, so the labels might be image
  descriptions.  Of course, these all get delivered correctly by the JPIP
  client-server machinery, during interactive remote image browsing.
* Removed the Intel SSE instruction, PEXTRW, from the code used to accelerate
  image blending in "kdu_region_compositor.cpp", so that plain-old MMX
  compilers and processors will be able to use the accelerations.  This
  results in a very slight slow-down in the accelerated compositing code.
  The blending accelerations have not yet been implemented for other
  SIMD architectures (e.g., Sparc VIS and Altivec), so these will just use
  the default platform-independent implementation.
* "kdu_compress" now supports reading of TIFF images, due to some
  code contributed by John Novak.  To enable TIFF reading, you need to
  define the macro, KDU_INCLUDE_TIFF; you need to obtain the "libtiff"
  package yourself and link against it, but this is very simple.

Changes from version 4.0.3 to version 4.1
-----------------------------------------
* All-new extensive support for JPX files, including the following:
  -- read and write all aspects of virtually all interesting JPX files,
     with only the following exceptions (these should both be included in
     the next release):
     * cross reference boxes and fragment tables not yet implemented
     * no explicit parsing or writing of desired reproduction boxes
  -- read and write multiple codestreams and multiple compositing layers
     * optimizes the use of default parameters where possible, during
       writing.
     * generates a comprehensive reader requirements box
     * maximizes potential for generated files to be JP2 compatible
  -- supports the extended colour descriptions offered by JPX, and manages
     multiple alternate colour descriptions.
  -- extended colour conversion capabilities:
     * allows conversion of the vast majority of JPX colour representations
       to sRGB; of the large number of enumerated JPX colour spaces, only
       CIEJab cannot currently be converted; of the embedded ICC profiles,
       only those involving 3D lookup tables cannot be converted.
     * application can control the trade-off between accuracy and speed
       for colour conversion.
     * application can request extended-gamut conversion
  -- supports all aspects of the JPX composition box, including composition,
     cropping, scaling, positioning and animation.
  -- extensive metadata management facilities
     * imposes a scale-space hierarchy on top of all spatially-sensitive
       metadata to facilitate efficient access.
     * uses scale-space structure to write metadata in a manner which is
       optimized for remote client-server delivery via JPIP.
  -- designed from the ground up to work with client-server systems
     * all aspects of the JPX interface allow for data to arrive
       asynchronously and out of order; interfaces to codestreams,
       compositing layers, etc, may become available incrementally and
       aspects of the data source are parsed only on demand.
     * capabilities proven and tested in the context of Kakadu's JPIP
       client-server architecture
* v4.1 provides a new very high level object, `kdu_region_compositor',
  which builds enormously on the capabilities of `kdu_region_decompressor'.
  This object provides the following new services:
  -- wraps up all the buffer management logic which was currently embedded
     in "kdu_show" within a single platform independent object.
  -- provides full support for compositing multiple images onto a single
     rendering surface, implementing the instructions represented by a
     JPX composition box; supports animation of complex image compositions.
  -- provides an efficient platform-independent alpha-based compositing
     engine.
  -- allows up to two codestreams to be used to build each compositing
     layer (one for colour and one for alpha), and any number of compositing
     layers to be combined on the rendering surface; all of this works
     together with interactive navigation, and dynamic delivery of image
     data from a remote JPIP server, if relevant.
  -- provides extensive integration of metadata (labels, xml, etc.) with
     the imagery, including overlays to symbolically represent the presence
     of spatially sensitive metadata; these are exploited and demonstrated
     by "kdu_show".
* many new features for the "kdu_show" demonstration viewer
  -- exploits all the new JPX features and the many features of the new
     `kdu_region_compositor' object.
  -- allows interactive stepping through animated image compositions,
     individual compositing layers, and even individual components or
     individual codestreams; these capabilities are also hyperlinked to
     the metashow tool.
  -- indicates the presence of spatially sensitive metadata via overlays
     (all implemented by `kdu_region_compositor'), including dynamically
     changing (blinking) overlays.
  -- dynamically overlays metadata text labels as the interactive user
     moves the mouse cursor (while control key is depressed).
  -- provides metadata editing and inspection facilities, with the ability
     to save images whose metadata has been edited; this provides an easy
     to use facility for marking up regions on an existing image.
* a new tool, "kdu_merge", allows complex JPX files to be generated by
  combining information from multiple JP2, JPX and/or MJ2 files
  -- can create composited and animated image sequences in a single JPX file.
  -- can build custom new JPX compositing layers by combining information
     from individual (or even multiple) codestreams in existing files with
     user-defined colour-space specifications.
* new JPX features reflected in various other demo applications, including
  "kdu_compress" and "kdu_expand".
* some minor fixes to the "kdu_server" application
  -- note, however, that kdu_server has yet to be upgraded to a JPX aware
     server.  One reason for this is that some changes to the JPIP standard
     are expected at the December 2003 WG1 meeting, particularly to meet
     the needs of serving JPX files properly.  While JPX files can be served,
     complex files (e.g., composited or animated images) will not
     automatically be served in a manner which meets the expectations of an
     interactive user.
* a memory growth problem has been fixed in the MJ2 file format writing logic.
* some fixes have been included for 64-bit addressing environments; these
  should fix some erratic behaviour during decompression of massive
  compressed images (4GBytes and up, compressed).
* slightly augmented the `kdu_region_decompressor::process' interface so
  as to allow the application to control whether or not the buffer row
  gaps are expressed in terms of pixels or samples -- the changes are
  transparent to existing users of the interface, but allow new applications
  to write directly to buffers with non-pixel-oriented alignment constraints
 (e.g., Windows bitmap buffers).

Changes from version 4.0.2 to version 4.0.3
-------------------------------------------
* Updated the JPIP implementation to make it current with the FCD produced
  out of the Strasbourg ISO/IEC JTC1/SC29/WG1 (JPEG) meeting.  There are,
  unfortunately, a couple of non-backward-compatible changes, so the client
  and server components of this version are not compatible with those of
  version 4.0.2.  The only significant respect in which the current
  implementation is not quite compliant with the JPIP specification is that
  the "mset" request field is not yet implemented -- there is no point in
  implementing this until I have a demo involving targets (e.g. video) with
  a large number of code-streams.  It is unlikely that the "mset" feature
  will be used for regular image browsing applications.  Preference and
  capability signalling by the client are also currently ignored, but in
  most respects Kakadu contains a very comprehensive implementation of the
  JPIP standard.
* Corrected an accidental change introduced to the behaviour of
  `kdu_codestream::flush' made in v3.4, in which the automatic layering
  heuristic introduced 1 layer per octave change in bit-rate -- the
  interface documentation states that the heuristic inserts 2 layers
  per octave (factor of 2) change in bit-rate where upper and lower
  bounds are not explicitly supplied by the application.  The documented
  policy has been restored.
* Included some changes provided by Michael Owen, to add an alternate
  interactive panning mechanism to "kdu_show".  Pressing the shift
  key together with the left mouse button, you can drag the view
  window around with the mouse in a natural way.
* Added a hex dump facility to the "metashow" tool in "kdu_show".
* Added arguments to "kdu_v_expand" to control the quality (number of
  layers), resolution (number of DWT levels) and spatial region associated
  with decompressed video frames.
* Added an optional array argument to the sample processing interfaces
  offered by the high level support objects, `kdu_stripe_compressor' and
  `kdu_stripe_decompressor' so that the application can explicitly control
  whether each of the image components uses a signed or an unsigned
  representation.
* Some minor bug fixes.

Changes from version 4.0.1 to version 4.0.2
-------------------------------------------
* Fixed a bug in the core system which gets excited under
  some configurations of tile opening and closing operations
  when running in the non-persistent mode.  This bug had always
  existed, but could not be uncovered by any of the demo
  applications prior to "kdu_buffered_decompress".
* Fixed a minor bug in the determination of whether or not
  a precinct is significant (in calls to `kdu_precinct::size_packets')
  which sometimes caused the Kakadu server to deliver certain image
  regions very late.  This bug was introduced by some new code to
  improve the efficiency with which small precincts are delivered
  by Kakadu's JPIP server.  Significance determination was not
  available prior to version 4.0.
* Added some new code to avoid needless reading of packets and
  tile parts which are known to be irrelevant to the application,
  where the image is tiled.  Kakadu has always been able to
  selectively parse the code-stream based on the available coding
  options, the application's needs and the availability of random
  access information, but these capabilities have principally been
  targeted toward untiled code-streams.  Since certain military
  applications prefer to use tiles, Kakadu is now much more careful
  about abandoning the parsing of tile data as soon as possible
  when operating in non-persistent mode (persistent mode provides
  no guarantees as to what elements of the code-stream may be
  required at a later stage, in which case efficient access is
  possible only if the code-stream is also endowed with sufficient
  pointer marker segments).

Changes from version 4.0 to version 4.0.1
-----------------------------------------
* Fixed a bug in 1-line of code in "kdu_params.cpp" which adversely
  affected any application which required transcoding services in
  which the original image used precincts with different dimensions
  in at least three different resolution levels.

Changes from version 3.4 to version 4.0
---------------------------------------
This is a MAJOR upgrade, introducing many new features, and providing
an implementation of the new JPIP standard for interactive imaging, which
is now becoming more stable, having reached CD (Committee Draft) status.
A summary of some of the key new features appears below, with the more
important changes listed last.
* All reported bugs have been fixed. For many of these, incremental workarounds
  have previously been published on the Kakadu public news forum, unless
  they were particularly esoteric.  Amongst the various bug fixes are the
  following:
  -- A bug was detected with the compression of tiled video frames using
     "kdu_v_compress".
  -- A bug was reported a long while ago with the construction of Java
     native interfaces.
  -- A bug was reported in connection with the use of certain image
     offsets (canvas coordinates) during compression -- bug was introduced
     accidentally while implementing the incremental code-stream flushing
     feature.
  -- While not exactly a bug, a number of users have reported occasional
     artefacts created while compressing images using Kakadu's block
     truncation prediction algorithm (not everyone is aware of the fact
     that this algorithm is enabled by default).  Although this algorithm
     is an optional speed enhancement, and not guaranteed to be 100% reliable,
     the new version contains a small code fix which dramatically improves
     its reliability with negligible impact on compression speed.
* The `kdu_codestream::set_max_bytes' function now contains an optional
  `simulate_parsing' argument which can be used to simulate the effects of
  parsing away any image components, layers or image resolutions which are
  rendered irrelevant by previous calls to `apply_input_restrictions', before
  applying a byte limit to the size of the resulting code-stream.  Otherwise,
  the byte limit simply truncates the compressed code-stream on input.
  This mode is useful for simulation work.  It may be enabled from the
  "kdu_compress" application, by specifying the `-simulate_parsing' command
  line option.
* "kdu_compress" now supports 32-bit BMP files, the definition of an
  explicit alpha channel (via `-jp2_alpha'), and inclusion of arbitrary
  additional meta-data in JP2 files (via `-jp2_box').
* The widely used `kdr_region_decompressor' object, originally designed as
  the platform independent component of the interactive rendering application,
  "kdu_show", has been given a major face lift.  It is now known as
  `kdu_region_decompressor' and found in the "apps/support".  The object
  now supports the decompression of additional (non-imagery) channels
  such as alpha channels, plus a wide range of buffer structures and data
  precisions to suit just about any region-based decompression application.
  The need to customize this object to meet specific application requirements
  should no longer exist.
* Two new high-level support objects, `kdu_stripe_compressor' and
  `kdu_stripe_decompressor' now wrap up almost all the steps required
  to build a memory-based compression application or a memory-based
  decompression application (although for interactive rendering, you will
  find `kdu_region_decompressor' more convenient than
  `kdu_stripe_decompressor').  They support a wide range of memory buffer
  architectures and data precisions to make application development much
  simpler than before.  Optional configuration arguments allow virtually
  all important Kakadu features to be accessed via these high level objects,
  with the image being compressed/decompressed from/to either a single
  buffer which holds the whole image or incrementally in stripes.
* The "simple_example_c" and "simple_example_d" introductory demo applications
  have been modified to use the `kdu_stripe_compressor' and
  `kdu_stripe_decompressor' objects mentioned above, making them much simpler
  again, while also more powerful.
* Two new example applications, "kdu_buffered_compress" and
  "kdu_buffered_expand" have been added to provide more comprehensive
  (though not complete) demonstration of the `kdu_stripe_compressor' and
  `kdu_stripe_decompressor' objects, and to bridge the gap between the
  very simple examples, "simple_example_c" and "simple_example_d", and
  the much more complex and comprehensive examples, "kdu_compress" and
  "kdu_expand".  The "kdu_buffered_expand" application can be significantly
  more computationally efficient than "kdu_expand" if you are working with
  a huge image which has been tiled.  This is because "kdu_expand" always
  tries to decompress one line at a time from the image, no matter how many
  tiles that line may span -- a reasonable approach in most circumstances,
  but certainly not the only way to use Kakadu.
* The "kdu_show" application has been augmented by the inclusion of a
  meta-data viewer (you can enable the "meta-show" facility by using the
  'm' accelerator, or from the view menu).  While the results might not
  be particularly interesting at present, this code demonstrates walking
  through an arbitrary JP2 family file, caching references to its elements
  and, MOST IMPORTANTLY, dynamically updating the meta-data display as content
  is dynamically transferred by a JPIP server.  The tool has been used to
  carefully test Kakadu's support for dynamic dissemination of both
  imagery and meta-data within the context of the new JPIP standard.  Without
  knowing much about JPIP, you might not be able to see much happening in
  the "meta-show" window yourself right now, but it is a key tool in the
  future road map for Kakadu.
* Kakadu's support for the JP2 file format has been completely revamped from
  the inside out.  If you are just creating and reading JP2 files, you will
  notice only one difference from previous versions.  You no longer open
  the JP2 source or target object directly.  Instead, you first open a
  `jp2_family_src' or `jp2_family_tgt' object, and then pass this to the
  `jp2_source::open' or `jp2_target::open' function.  Behind the scenes,
  however, the new architecture is very different:
  -- Firstly, Kakadu now provides a generic solution for all JP2 family
     file formats, by providing powerful `jp2_input_box' and `jp2_output_box'
     objects, which allow an application to navigate the contents of an
     arbitrary JP2 family file format.  Whenever a code-stream box is
     encountered, it may be passed directly to `kdu_codestream::create'
     to interact with or create the relevant imagery.  Multiple boxes may
     be open into a single file at any given time, allowing the application
     to interact with multiple code-streams simultaneously (amongst other
     things).
  -- Most important of all, the new `jp2_input_box' and `jp2_output_box'
     objects provide complete support for interacting with any object which
     is filling the role of a JPIP interactive image client.  They recognize
     and handle the special facilities provided by the new JPIP standard
     for hierarchically resequencing and streaming the content of an arbitrary
     JP2 family file, along with all of its meta-data.  These mechanisms
     are concealed from applications and file format parsers which are
     built on top of the `jp2_input_box' and `jp2_output_box' objects so
     that full support for efficient, user-sensitive, dynamic interactive
     services is automatically incorporated into any file format for which
     a parsing utility exists at the client side.  Currently, Kakadu provides
     the full set of parsing tools only for the JP2 file format, but the
     next version should bring JPX fully to life.
  -- One consequence of the above is that it is now a simple matter to
     add extra custom boxes to a JP2 file, and to parse these boxes out,
     opening any code-stream boxes as separate images.  These possibilities
     are demonstrated by the "kdu_compress" and "kdu_show" example
     applications, respectively.
* The `kdu_serve' object, representing the platform independent component
  on which Kakadu's image serving capabilities are built, has been thoroughly
  revamped to provide generic support for the various JP2 family file
  formats, interactive delivery of files with multiple code-streams
  (including video), and very much reduced memory consumption when
  deliverying huge images (those in the multi-Gbyte range).  It provides
  automatic as well as client-hinted scheduling of meta-data and image
  data in accordance with file-format and application-specific information
  provided by an auxiliary object, derived from the new `kdu_serve_target'
  interface.  Currently, only one `kdu_serve_target' object is implemented,
  which provides the intelligence necessary to digest raw code-streams and
  JP2 files, but all that is required to support custom JPIP service
  applications or new file formats is to derive an appropriate object
  from `kdu_serve_target'.  We expect to provide such an object for the
  JPX file format with the next release of Kakadu (v4.1).
* The platform dependent client and server components of Kakadu have
  been modified to conform with the syntax and conventions of the new
  JPIP standard (JPEG2000, Part 9), which is currently at CD (Committee
  Draft) status.  For more on JPIP and Kakadu's support for JPIP, consult
  the "jpip-links-and-info.html" file found in the "documentation" directory.
* The various documents, particularly the survey document "kakadu.pdf", have
  been updated to include introductory descriptions and overview diagrams
  for many of the new features offered by version 4.0.

Changes from version 3.3 to version 3.4
---------------------------------------
* The most significant change here is the introduction of incremental
  flushing of the code-stream during compression.  This allows platforms
  with only modest memory resources to compress truly massive images,
  provided you are careful to select the appropriate spatially progressive
  packet progression sequence.  Incremental flushing works with all modes
  of code-stream generation, although to use this feature efficiently you
  should review the extensive documentation which appears in the description
  of the `kdu_codestream::flush' function.
* The rate control policies have also been modified slightly to ensure that
  the most appropriate behaviour occurs when requested quality layer bit-rates
  span many orders of magnitude (e.g., "-rate -,0.001 Clayers=30").
* 64-bit data types and 64-bit file I/O are now supported when compiling
  under GCC, as well as Win32 environments where they have long been
  supported.  Be careful to set _FILE_OFFSET_BITS to 64 in the relevant
  make file, if you want this feature.  The Unix makefiles now also generate
  shared libraries.
* A significant problem has been fixed with the `kdu_server' application.
  The application had no memory or handle leaks per se, but some resources
  were not properly cleaned up until the server was in the process of being
  shut down due to failure to call the key clean-up function regularly.  The
  server also now responds more appropriately to HTTP received from browsers
  which are not JPIP capable.

Changes from version 3.2 to version 3.3
---------------------------------------
* The most significant change is the provision of new
  client and server implementations and support services.
  These are embodied principally through the platform
  independent objects, "kdu_cache2" and "kdu_serve2"
  and the derived platform dependent (WIN32) objects,
  "kdu_client2" and "kdu_server2".  The older versions
  of these objects (those without the "2" suffix),
  supporting the original JPIK protocol, are still shipped
  but are being deprecated.  The new client and server
  implementations support our proposal to the JPIP standard,
  which is compatible with the current working draft produced
  at the ISO/IEC JTC1/SC29/WG1 meeting in Boston, July 2002.
  For more information, the reader is referred initially to
  the supplied document, "jpip-kakadu.pdf", and then to the
  object interface descriptions, and the "Usage_Examples.txt"
  file.
* A couple of bugs in the Java interface building tools
  supplied in v3.2 have been fixed, particularly relevant to
  multi-threaded Java applications.  The fixes were
  contributed by one of our clients who is working extensively
  with Java.
* The "kdu_message" object has been given a new virtual (callback)
  member function, "start_message", which is particularly useful for
  synchronizing message delivery in multi-threaded environments.  This
  is used by the "kdu_show" application to ensure reliable message
  delivery from both the image processing thread and a network
  management thread.
* Minor changes have been made to the JP2 file interfaces to allow
  cleaner support for distributing JP2 files over networks.
* The `jp2_colour::init' functions which generate ICC profiles for
  custom luminance and RGB spaces have been modified to include all
  mandatory ICC tags, rather than just those which are described in
  connection with the JP2 file format.
* Core system services have been included for including user-defined
  comments into the code-stream and recovering them, if necessary.  The
  "kdu_show" application's "File->Properties" menu item now displays
  all text comments it finds in the code-stream.
* The `kdu_codestream::flush' function now supports the inclusion of
  a special COM (comment) marker segment, embedding information about
  the rate-distortion slopes and sizes of the code-stream's quality
  layers, for use in other applications.  "kdu_compress" uses this
  option by default, unless it is explicitly disabled with the `-no_info'
  flag.  The information recorded in the marker segment, if present,
  is used by "kdu_server" to optimize the delivery of data to
  interactive clients using Rate-Distortion performance criteria.
  A brief description of this new feature may be found toward the
  end of the "jpip-kakadu.pdf" document.
* The "kdu_hyperdoc" utility now shortens the generated HTML file names
  to fit within the 33 character limit imposed by some browsers deployed
  in the Apple Mac environment.  A command-line argument may be given
  to restore the original fully expanded file naming convention.
* Some minor bugs have been fixed, relating to parameters with very
  large values, on the order of 2^31 or greater.  This includes a fix
  for problems sometimes encountered when compressing images larger
  than 2GB in size.
* A bug has also been fixed in the processing of encode-time ROI's
  (max-shift method) having large up-shifts (Rshift values).  The
  bug caused incorrect calculation of the distortion contributions
  associated with blocks whose up-shift required a dynamic range
  exceeding 32 bits to fully accommodate both foreground and
  background regions.

Changes from version 3.1 to version 3.2
---------------------------------------
* The most significant new feature is the introduction
  of Java native interface bindings to virtually all
  exposed Kakadu objects and public functions.  The
  Java bindings are automatically generated by the
  "kdu_hyperdoc" utility, which also includes details
  of the Java interfaces with the extensive HTML
  documentation it builds.  See the file,
  "java-interfaces.pdf", for more on building and
  using the Java interfaces.
* Introduced some additional interface functions to
  a number of the existing Kakadu classes, where the
  existing functions do not bind well to Java (or
  perhaps other languages).  This ensures that key
  functionality is available from Java (and maybe later
  other languages) through at least some method.
* Completely eliminated all C++ stream I/O dependencies
  from the core Kakadu system, rationalizing the
  error and warning message handling mechanisms in a way
  which is amenable to foreign language bindings.  This
  has additional advantages, since support for the C++
  I/O stream libraries is shaky or missing on some
  target platforms used by our customers.
  DEVELOPERS SHOULD CAREFULLY REVIEW the new declarations
  of `kdu_customize_errors' and `kdu_customize_warnings'.
* Added an overloaded version of the `kdr_region_decompressor::process'
  function which writes decompressed region data to a compact
  array of pixels, each having a 32-bit representation.  This is
  more efficient for Java applications and also for native
  language implementations which do not keep their entire viewport
  in a single contiguous buffer.
* Included a Java demonstration application,
  "KduRender.java", which uses the Java native interfaces
  to decompress and render an image incrementally to a
  display.  This should work across multiple platforms,
  although we have not yet tested it on anything other
  than the Windows platform.
* Corrected a couple of minor, yet potentially
  dangerous bugs, accidentally introduced in the
  upgrade from version 3.0 to version 3.1.

Changes from version 3.0.8 to version 3.1
-----------------------------------------
* Added a "restart" option to the "kdu_codestream"
  interface, which enables the internal code-stream
  management machinery to be efficiently restarted,
  with particular application to video compression
  and decompression.
* Incorporated MMX optimizations into the compressor
  as well as the decompressor, and fixed a bug in
  the previous MMX implementation which prevented
  the irreversible 5/3 transform (actually a JPEG2000
  Part 2 feature, not Part 1) from behaving correctly.
* Incorporated substantial support for the Motion
  JPEG2000 (MJ2) file format, which complements and
  leverages off the existing JP2 file format support.
  The new implementation should be easier to extend
  to other members of the JP2-family, e.g., JPM or JPX.
* Three new demonstration applications are included:
  -- "kdu_render" shows how the platform independent
     "kdr_region_decompressor" object can be used
     to build JPEG2000 decompression into a whole
     host of applications at a very high level,
     with a minimal amount of effort.
  -- "kdu_v_compress" and "kdu_v_expand are video
     compression and decompression applications.
     There is no interframe compression involved here.
     Instead, only frame-by-frame compression is
     supported, using JPEG2000 code-streams for each
     frame (or field, for interlaced video).  The
     applications use either the Motion JPEG2000
     file format or a much simpler video file format
     which has been created purely for demonstration
     purposes.
* The customizable error and warning message services
  have changed slightly with the addition of a
  user-definable argument to the callback functions.
* Fixed a number of minor bugs

Changes from version 3.0.7 to version 3.0.8
-------------------------------------------
* There is only one significant change here: all
  of the documentation which was distributed throughout
  the publically includable header files is now automatically
  built into a fully integrated documentation system.  You
  should find that the new "kdu_hyperdoc" utility compiles this
  documentation as soon as it is built (using the Makefiles
  or the Visual C++ build environment).  If not, you can always
  build the documentation system manually by running the
  command found in "documentation/hyperdoc.bat" -- you should
  change into the "documentation" directory first.  The
  integrated documentation system is accessed through the
  "index.html" file in the "documentation" directory.
* The names of three member functions have been changed -- the
  previous names were particularly unhelpful.  Specifically,
  "kdu_params::describe_string" and "kdu_params::describe_strings"
  are now called "kdu_params::describe_attribute" and
  "kdu_params::describe_attributes", while
  "jp2_palette::get_num_components" is now called
  "jp2_palette::get_num_luts".
* Some minor bug fixes.

Changes from version 3.0.6 to version 3.0.7
-------------------------------------------
* Corrected a mis-interpretation of ambiguous text
  in the description of the JP2 palette box.  The
  entries appear in entry-major order, as opposed to
  component-major order which previous versions implemented.
  This is an important correction for compliance.
* Fixed a minor bug in "kdu_compress" which made it difficult
  to specify component-specific subband weights.
* Added code to read and write profile identifiers from the
  Rsiz code in the SIZ marker segment.  More importantly,
  Kakadu carefully checks and reports and violations of the
  restrictions associated with individual profiles.  Warning
  messages are generated rather than error messages, allowing
  non-complying code-streams to be read and, optionally,
  transcoded into streams which do comply the a stated
  profile restriction.
* Added support for reading BMP files with incomplete palettes.

Changes from version 3.0.5 to version 3.0.6
-------------------------------------------
* Changed the code for the sYCC space in the JP2
  colour box to 18 from 22 to reflect a recent
  ammendment to the standard.
* Fixed a bug in which marker codes in the range
  FF30 to FF3F (not defined by JPEG2000) were not
  ignored as they are supposed to be.
* Minor enhancements to the "kdu_show" GUI including
  migration to BitBlt to support WinCE users.
* Fixed (I hope) a bug in the "kdu-server" application
  which occasionally caused the server to go into an
  active polling state (rather than blocking on a
  pending condition), thereby chewing up masses of
  CPU time.

Changes from version 3.0.4 to version 3.0.5
-------------------------------------------
* Minor bug fixes in the error resilience section of the
core system, plus some minor efficiency improvements.

Changes from version 3.0.3 to version 3.0.4
-------------------------------------------
* Significant improvements in efficiency of the "kdu_server"
application.  A single machine should now be able to serve
up to perhaps 100 clients simultaneously.  The flow control
protocol has also been substantially improved so as to
more nearly achieve the network capacity while retaining
(improving) interactive responsiveness.

* The "kdu_show" application can now write out compressed
images as JP2 or J2C files, which is particularly useful
when browsing images using JPIK.  Other controls have been
added to access all of the available image components (not
just the first 4) and to directly input JPIK URL's from
the menu.

* A "focus box" capability has been added to the "kdu_show"
application, which allows the user to specify a region of
interest, independently of the view port established by the
window dimensions, scroll and zoom settings.  When used,
the focus box provides the focus for zoom operations and
the region of interest for JPIK remote image browsing
sessions.  When connected to a JPIK server, the size of
the focus box may be limited to maximum dimensions specified
by the server, enabling it to bound the memory and disk
access resources required to serve large regions of interest
to clients.

* A number of minor bugs have been fixed: one which prevented
the "kdu_server" application from behaving properly with
images having certain coding parameters; and one very subtle
bug in the core system which manifested itself on very rare
occasions.

Changes from version 3.0.2 to version 3.0.3
-------------------------------------------
* Minor changes in the "kdu_server" and "kdu_show" applications
  to fix bugs and simplify the description of the JPIK protocol.
* Fixed a bug in the core system which appeared when decompressing
  images containing PLT marker segments and certain tile-part
  organizations.
* Implemented the second form of the "jp2_target::open" function,
  which was accidentally omitted.
* Fixed a minor memory error reported by one client using purify.

Changes from version 3.0.1 to version 3.0.2
-------------------------------------------
* Minor changes in the "kdu_server" and "kdu_show" applications to
  fix a problem which was causing a lot of firewalls to reject the
  traffic.
* Delegation and remote administration added to the capabilities of
  the "kdu_server" application.  It is now possible to review status,
  upload files to and/or shutdown the "kdu_server" application
  remotely under password protection.  It is also possible to serve
  files from a cluster of networked machines with a single public
  IP address.

Changes from version 3.0 to version 3.0.1
-----------------------------------------
* Minor changes in "kdu_server" and "kdu_show" to support a variant
  of the JPIK protocol which replaces the mixed TCP/UDP transport with
  a TCP-only transport.  Opening a URL with the suffix, ":TCP", using
  "kdu_show" will request the TCP-only transport for ongoing
  communications with the server.  The server can support multiple
  clients, where some might use UDP and others not.

Changes from version 2.2.3 to version 3.0
-----------------------------------------
Summary:
        This is a MAJOR upgrade, introducing many new features and
     achieving most of what the Kakadu architecture was originally
     designed for.  The most major new features may be summarized as:
     1) Lots of support for random access into very large compressed files;
     2) The introduction of "interchange" code-streams, with direct access
        to JPEG2000 packet data.
     3) A comprehensive demonstration of Kakadu's support for interactive
        client-server applications.

     As in previous releases, the principle form of documentation is the
     descriptions of public object member functions (interface functions)
     appearing in the relevant public header files.  These have been
     carefully organized for you.  The "kakadu.pdf" document has only
     limited information on the new advanced features introduced with
     version 3.0.  Additional typeset documentation for advanced
     capabilities will be forthcoming in the not too distant future.

Assorted Details:
* Kakadu now employs a customizable variable length data type, "kdu_long"
  (64 bits on Win32 and 64-bit Unix architectures at least) for all
  quantities which may be proportional to image area (as opposed to
  image dimensions), including compressed data size, file seek addresses
  and so forth.
* The core "kdu_codestream" object is able to exploit various pointer
  marker segments which may (optionally) be present to gain random access
  into a JPEG2000 code-stream.  In particular, TLM (Tile-Part Length Main)
  and PLT (Packet Length Tile-Part) marker segments are no longer discarded
  by the code-stream parser.  The compressor can generate PLT marker
  segments (or include them during transcoding with "kdu_transcode");
  however, generation of TLM marker segments cannot be accomplished in a
  single pass operation.  For this reason, a separate utility, "kdu_maketlm",
  is provided to introduce TLM marker segments (if you do not intend to tile
  the image, PLT marker segments are much more useful for random access than
  TLM marker segments, although both may be included).
     The conditions under which the information in PLT marker segments can
  be efficiently utilized are somewhat complex.  The compressed data source
  must advertise the KDU_SOURCE_CAP_SEEKABLE capability (as described in
  "kdu_compressed.h") and all packets of each precinct must appear
  contiguously within the code-stream (the default LRCP packet sequence
  generated by the "kdu_compress" application does not have this property;
  use RPCL, PCRL or CPRL).  The system will generally warn you when PLT
  information is present and ought to be usable apart from the selection of
  an adverse organization for the code-stream.
     When PLT marker segments are available and are able to be utilized,
  a "kdu_codestream" object with the persistent mode set (see
  "kdu_codestream::set_persistent") will unload compressed data from
  memory as soon as possible, since the PLT information may be used to
  reload it on demand.  This permits access to massive files with relatively
  little memory consumption.  Moreover, the system supports internal caching
  to minimize the frequency with which data must be reloaded from disk or
  reparsed from raw code-stream packets. The cache size can be directly
  controlled using "kdu_codestream::augment_cache".
* The "kdu_codestream" object can interact directly with a caching
  compressed data source (one which advertises the KDU_SOURCE_CAP_CACHED
  capability), recovering JPEG2000 packet data directly from such a source
  in any order (partially or in full) as needed.
* A "kdu_cache" object is provided, derived from
  "kdu_compressed_source" which is able to efficiently cache compressed
  data which arrives in any order whatsoever from an external source.
* A "kdu_client" object is provided, derived from "kdu_cache", which
  implements all the functionality required for the client side of an
  interactive client-server application.  Unlike the platform independent
  "kdu_cache" implementation, the "kdu_client" layer is multi-threaded
  and hence platform specific (it is currently implemented only for the
  WIN32 platform, although ports to POSIX threads would not be difficult).
* The "kdu_show" application has been augmented with a richer user
  interface and is able to utilize the "kdu_client" object as one of
  its various "kdu_compressed_source" derived input sources.  In this mode
  the "kdu_show" application becomes a fully fledged JPEG2000 image browsing
  application.  See the "Usage_Examples.txt" file for examples of this.
* The "kdu_codestream::create" function now comes in three different
  forms, rather than just two.  You can create the object for "input"
  (i.e., decompression), for "output" (i.e., compression or transcoding), or
  for "interchange" (this is the new one).  A "kdu_codestream" object opened
  for "interchange" has neither a compressed data source nor a compressed
  data target.  You write code-block data to such an object just like an
  output object, but rather than generating a compressed data stream, you
  can assemble and retrieve JPEG2000 compressed packets directly from the
  object, in any order whatsoever.  Various tools are provided to support
  rate control funcionalities which you might find useful.  Interchange
  objects are provided primarily for use with server applications which
  can then ship custom-built packets to a remote client, which uses them to
  augment a a caching compressed data source.
* A "kdu_serve" object is provided which implements the base functionality
  required to serve compressed data packets to a remote "kdu_cache" object.
  Both "kdu_serve" and "kdu_cache" have platform independent implementations.
* A "kdu_server" application is provided to demonstrate client-server
  capabilities.  Building upon the functionality offered by "kdu_serve",
  it talks directly with the "kdu_client" object activated when the
  "kdu_show" application is started with a network address having
  the protocol prefix, "jpik:".  The server is multi-threaded and hence
  not platform independent (the current WIN32 implementation could be
  ported to POSIX threads without too much difficulty).  The server can
  support multiple clients and multiple sources simultaneously.  It
  can share a single compressed data source across multiple clients
  and is able to take full advantage of the presence of PLT and PLM
  marker segments in the source files to avoid loading any more of the
  source file(s) than is necessary.  The server interacts with the
  client to deliver only the information which is relevant to the
  client's current region, resolution and components of interest.
  Moreover, the server automatically performs in-place transcoding of
  source files to allow the data to be transported using the smallest
  spatial precincts (finest spatial granularity) compatible with the
  code-block dimensions used during compression.  This enables any
  JPEG2000 source file whatsoever to be served up to a client in a
  spatially sensitive manner.
* The "jp2_source" and "jp2_target" objects have been enhanced to
  enable JP2 data to be sourced from or delivered to any
  "kdu_compressed_source" or "kdu_compressed_target" derived objects.
  This immediately allows JP2 functionality to be incorporated into
  memory based compressed data streams, or even caching compressed
  data sources such as "kdu_cache".  This, in turn, allows the
  client-server capabilities described above to work with JP2
  files, as well as raw JPEG2000 code-streams.  For more info,
  you might like to initially consult the comments appearing
  in "simple_example_c.cpp" and "simple_example_d.cpp" and then
  look at "jp2_source::open" and "jp2_target::open".
* The compressor now supports much richer tile-part generation
  capabilities.  Even if the image is not tiled, the single image
  tile can still be split into multiple tile-parts based upon
  resolution, component or layer indices, as controlled by the
  new "ORGtparts" compression attribute (see usage statement for
  "kdu_compress").
* A number of minor bugs and irregularities have been fixed.

Changes from version 2.2.2 to version 2.2.3
-------------------------------------------
* Extremely minor changes to avoid mixed use of formatted and
  unformatted calls to "ostream" objects.  These appear to
  excite a bug in GCC version 3.0.  The only file affected is
  "params.cpp" in "coresys/parameters".

Changes from version 2.2.1 to version 2.2.2
-------------------------------------------
  Note: none of these have any impact whatsoever on executable code
  or DLL's.
* Renamed the "core" directory as "coresys".  A trivial change
  and my appologies for those whom this might adversely affect.
  However, the use of the name "core" was causing some people
  difficulties, since it is identical to the name of a Unix core
  dump file.
* Made the Linux MMX-optimized functions "extern C" instead of
  "extern", so as to avoid problems caused by different name
  mangling conventions between various versions of gcc.
* Eliminated multi-line comments from assembler files so as to
  avoid problems created by earlier versions of the gnu assembler.

Changes from version 2.2 to version 2.2.1
-----------------------------------------
* Replaced the C++ I/O routines used for image and compressed data
  transfers with ANSI C I/O functions.  This was motivated by the
  fact that the new-style ANSI C++ I/O package is unbelievably slow,
  at least as implemented in Microsoft's Visual C++ compiler.
  The change has no impact whatsoever on the Kakadu core system;
  it affects only the implementation of a few application level
  objects -- the core system abstracts all such I/O considerations
  through interface classes which are implemented by applications.
  Everything now runs a little faster than it did in version 2.1 and
  quite a bit faster than it did in the first release of version 2.2.
* Made provision for compiling under versions of GCC prior to version 3.0.
  To use this, you should define GCC_VERSION_LESS_THAN_3.

Changes from version 2.1 to version 2.2
---------------------------------------
* Extensive support for ROI (Region of Interest) specification
  at encode time (see "kakadu.pdf" for more on this).
* Migrated from the old-style C++ iostream package to the new standard
  iostream package -- minimal use of "using namespace std" and never used
  in common header files, so this should enhance namespace protection
  and portability.
* Added AT&T style versions of the small amount of  Pentium assembly code
  to enable compilation of a high speed version under GCC as well as
  MSVC.
* Some minor bug fixes.

Changes from version 2.0.2 to version 2.1
-----------------------------------------
* Extensive support for working with the JP2 file format.  The "kdu_show"
  application demonstrates the capabilities required of a conformant
  JP2 reader: palette mapping; interpolation of components to a common
  resolution; application of registration offsets in the CRG marker
  segment; and colour conversion to an appropriate rendering space (sRGB
  here).  The "kdu_region_decompressor" object provides extensive support
  for general purpose interactive rendering applications, performing all
  of the above tasks in a platform independent manner.
* It is now possible to directly control rate-distortion slope thresholds
  used in the construction of quality layers.  This capability may also
  be used to significantly increase compression speed, if a suitable
  threshold is known, since the encoder then incrementally predicts
  the point at which there is no point in coding further coding passes.
* A number of improvements to the "kdu_show" application, including
  the ability to arbitrarily zoom into images.
* A number of minor bug fixes, including one important bug reported by
  Aaron Deever of Kodak, and a bug which occasionally manifested itself
  in the incremental rate prediction heuristic (reported by Jim Andrew
  of CISRA).
* Improved documentation.

Changes from version 2.0.1 to version 2.02
------------------------------------------
* A PDF document (documentation.pdf) has been prepared to guide the new
  user into some of the more important aspects of the Kakadu system.  The
  first draft is included here.
* A very simple compression example and a very simple decompression example
  have been added to assist developers in familiarizing themselves with
  the Kakadu system -- the existing demo apps provide perhaps too much
  functionality to be quickly understood.
* A full BIBO (Bounded Input Bounded Output) numerical analysis of
  the DWT and other processing steps is used to establish the best
  utilization of limited precision sample data representations.
  The new version should not be able to fall prey to numerical
  overflow or underflow difficulties under any circumstances (this
  could just have been possible with the last version).  It also
  provides slightly higher accuracy.
* The automatic heuristic for generating quality layer rate
  targets has been substantially improved.
* A number of minor bugs/limitations were corrected, although these
  had not manifested themselves in any real examples.


Changes from version 2.0 to version 2.01
----------------------------------------
* One line change in each of "kdu_expand.cpp" and "kdu_compress.cpp" to
  correct a minor rare bug in these demo applications.
* Minor changes in "kdu_show.cpp" to correct a rare bug and improve the
  user interface in regard to image rotation.
* Four lines added to each of "encoder.cpp" and "decoder.cpp" to fix
  a minor memory leak.
